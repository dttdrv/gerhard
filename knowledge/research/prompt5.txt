Advanced Architectures for Ternary Quantization: A Deep Dive into Channel-Wise Methodologies, Learnable Parameters, and Neuromorphic Integration1. Introduction: The Imperative for Extreme QuantizationThe deployment of Deep Neural Networks (DNNs) on edge devices is fundamentally constrained by the "Von Neumann bottleneck"—the latency and energy cost associated with moving data between memory and processing units. While algorithmic advancements have produced models with super-human capabilities in vision and language tasks, the sheer size of these models (often ranging from hundreds of millions to billions of parameters) renders them impractical for resource-constrained environments such as IoT sensors, mobile devices, and autonomous embedded systems. The primary costs in these systems are not merely computational but energetic; accessing data from off-chip DRAM consumes orders of magnitude more energy than performing the arithmetic operations themselves. Consequently, model compression has transitioned from a desirable optimization to a critical deployment necessity.Among compression techniques, quantization—the mapping of high-precision floating-point values to lower-precision discrete representations—has emerged as the most effective strategy for reducing memory footprint and bandwidth usage. While 8-bit integer (INT8) quantization has become an industry standard, offering a $4\times$ reduction over 32-bit floating-point (FP32) with negligible accuracy loss, the frontier of research has shifted toward "extreme" quantization: binary (1-bit) and ternary (2-bit) representations.Binary Quantization, while offering the theoretical maximum compression ($32\times$), often suffers from severe information loss. The rigid constraint of $\{-1, +1\}$ forces the network to binarize all features, leading to a significant degradation in representational capacity and optimization difficulties due to the disjoint loss landscape. Furthermore, binary networks lack a "zero" state, meaning every connection is active, which prevents the exploitation of sparsity for hardware acceleration.Ternary Quantization ($\{-1, 0, +1\}$) represents a theoretical and practical optimum in this trade-off space. By introducing a zero state, ternary networks gain three critical advantages over their binary counterparts:Sparsity Induction: The zero state allows for "zero-skipping" in hardware, where operations involving zero weights are simply bypassed, directly reducing computational latency and energy consumption.Increased Capacity: The addition of a third state exponentially increases the expressive power of the network. For a standard $3 \times 3$ convolutional kernel, a binary network has $2^9 = 512$ possible states, whereas a ternary network has $3^9 = 19,683$ states. This dramatic increase in capacity ($~38\times$) comes at a marginal cost in storage (typically stored as 2 bits per weight).Optimization Stability: The zero state acts as a natural regularizer and feature selector, allowing the network to prune irrelevant connections dynamically during training, leading to better convergence properties.This report provides an exhaustive technical analysis of the state-of-the-art in ternary quantization. It moves beyond static, layer-wise approaches to explore dynamic, channel-wise methodologies like Trained Ternary Quantization (TTQ) and Ternary Vision Transformers (TerViT). We analyze the shift toward learnable quantization parameters, the mathematical intricacies of backpropagation through discrete functions, and the emerging synthesis of ternary logic with Spiking Neural Networks (SNNs) for ultra-low-power neuromorphic computing.2. Theoretical Foundations of Ternary Representation2.1 The Quantization Mapping FunctionFundamentally, ternary quantization seeks to map a continuous, full-precision weight tensor $\tilde{W} \in \mathbb{R}$ (often referred to as the "latent weight") to a discrete quantized tensor $W_t \in \{-1, 0, +1\}$. This process is governed by two primary hyperparameters: the threshold ($\Delta$), which determines the width of the zero region, and the scaling factor ($\alpha$), which restores the amplitude of the quantized weights to approximate the dynamic range of the original distribution.The general symmetric quantization function can be defined as:$$W_t = \alpha \cdot \text{Q}(\tilde{W}, \Delta)$$where the discrete function $\text{Q}(\cdot)$ is defined as:$$\text{Q}(x, \Delta) = 
\begin{cases} 
+1 & \text{if } x > \Delta \\
0 & \text{if } |x| \le \Delta \\
-1 & \text{if } x < -\Delta 
\end{cases}$$In early implementations such as Ternary Weight Networks (TWN), the parameters $\alpha$ and $\Delta$ were deterministic statistics derived from the assumption that weights in a trained network follow a Gaussian or Laplacian distribution. For instance, Li & Liu (2016) derived an approximate optimal threshold of $\Delta \approx 0.7 \times \mathbb{E}(|\tilde{W}|)$ to minimize the L2 distance between the full-precision and quantized weights.1However, the assumption of a symmetric, unimodal distribution often fails in modern Deep Neural Networks (DNNs), particularly those utilizing Rectified Linear Unit (ReLU) activations, which induce non-negative sparsity in activations, or in Vision Transformers (ViTs), where weight distributions can be heavy-tailed and vary significantly across attention heads. This limitation necessitates the transition to learnable quantization parameters, where $\alpha$ and $\Delta$ are treated not as fixed hyperparameters but as trainable variables optimized via gradient descent.2.2 Latent Weights and the Shadow CopyA critical concept in training quantized networks is the maintenance of latent weights (also called shadow weights). Since the quantized weights $W_t$ are discrete, applying small gradient updates directly to them would result in zero change (as the update would be insufficient to flip the state from 0 to 1 or -1 to 0).To solve this, the training process maintains a full-precision copy of the weights ($\tilde{W}$). During the forward pass, $\tilde{W}$ is quantized to $W_t$ on-the-fly to compute the layer outputs. During the backward pass, the gradients computed with respect to $W_t$ are applied to update $\tilde{W}$. This allows the continuous latent weights to accumulate small gradients over many iterations, eventually crossing the threshold $\Delta$ and causing a discrete state change in $W_t$. This mechanism effectively decouples the accumulation of learning signals (continuous) from the representation of the model (discrete).12.3 Binary Comparison and Expressive CapacityTo understand the advantage of ternary over binary, consider the combinatorial capacity of a neural network layer. A binary weight $w_b$ carries $\log_2(2) = 1$ bit of information. A ternary weight $w_t$ carries $\log_2(3) \approx 1.585$ bits of information.While this difference appears small per weight, it compounds exponentially over the dimensionality of the weight matrix. For a layer with $N$ parameters:Binary State Space size: $2^N$Ternary State Space size: $3^N$The ratio of the state spaces is $(1.5)^N$. For a modest layer with $N=1000$, the ternary network has a state space $1.5^{1000} \approx 10^{176}$ times larger than the binary network. This massive increase in expressive capacity allows ternary networks to model complex decision boundaries that binary networks simply cannot approximate without significantly increasing the number of neurons (width), which would negate the compression benefits. Furthermore, the inclusion of 0 allows the network to ignore noise or irrelevant features, a capability binary networks lack as they are forced to assign a non-zero influence ($\pm 1$) to every input feature.43. Trained Ternary Quantization (TTQ): Methodology and Asymmetric ScalingTrained Ternary Quantization (TTQ), introduced by Zhu et al. (ICLR 2017), represents a seminal advancement in quantization methodology. It challenges the restriction of symmetric quantization, arguing that the positive and negative magnitudes of weights in deep networks are rarely identical. TTQ proposes an asymmetric quantization scheme where the positive scaling factor ($W_p$) and negative scaling factor ($W_n$) are decoupled and trained independently.13.1 Asymmetric Scaling MethodologyIn standard symmetric quantization, the weights are mapped to $\{-\alpha, 0, +\alpha\}$. TTQ relaxes this, mapping weights to $\{-W_n, 0, +W_p\}$, where $W_n$ and $W_p$ are strictly positive trainable scalars.The quantization function for a specific layer $l$ becomes:$$w_t^l = 
\begin{cases} 
+W_p^l & \text{if } \tilde{w}^l > \Delta_l \\
0 & \text{if } |\tilde{w}^l| \le \Delta_l \\
-W_n^l & \text{if } \tilde{w}^l < -\Delta_l 
\end{cases}$$Here, $\Delta_l$ acts as the thresholding hyperparameter. The critical innovation is that $W_p^l$ and $W_n^l$ are free parameters included in the backpropagation graph. This allows the network to learn not just which weights should be positive or negative, but how strong those positive and negative signals should be relative to each other.Empirical analysis of trained TTQ models often reveals that $W_p \neq W_n$. For example, in later layers of ResNet architectures, the optimal positive scaling might be significantly larger than the negative scaling, reflecting the rectified nature of the activations flowing into the layer. By allowing this asymmetry, TTQ reduces the quantization error (the difference between $\tilde{W}$ and $W_t$) significantly compared to symmetric constraints.13.2 Gradient Derivation and Straight-Through Estimator (STE)Training TTQ requires computing gradients for three sets of variables: the latent weights $\tilde{W}$, the positive scale $W_p$, and the negative scale $W_n$. Since the quantization function contains step functions (non-differentiable), the Straight-Through Estimator (STE) is employed for the latent weights, while standard chain rule calculus is derived for the scaling factors.3.2.1 Gradient for Latent Weights ($\tilde{W}$)Using the STE, the gradient of the loss $L$ with respect to the latent weight $\tilde{w}$ passes through the quantization function as if it were an identity function (or a clipped identity) in the active regions.$$\frac{\partial L}{\partial \tilde{w}} = \frac{\partial L}{\partial w_t} \cdot \mathbf{1}_{|\tilde{w}| \le 1}$$This allows the gradient information from the loss function to update the latent weight distribution directly, pushing weights in and out of the zero region or flipping their signs.73.2.2 Gradient for Scaling Factors ($W_p, W_n$)The scaling factors are continuous variables, so their gradients are exact. The gradient for $W_p$ is the sum of gradients for all weights that were quantized to the positive state:$$\frac{\partial L}{\partial W_p} = \sum_{i \in I_p} \frac{\partial L}{\partial w_{t,i}}$$where $I_p = \{ i \mid \tilde{w}_i > \Delta \}$ is the set of indices of weights mapped to $+W_p$.Similarly, for the negative scale:$$\frac{\partial L}{\partial W_n} = \sum_{i \in I_n} \frac{\partial L}{\partial w_{t,i}} \cdot (-1)$$where $I_n = \{ i \mid \tilde{w}_i < -\Delta \}$. Note that the term $(-1)$ accounts for the fact that the quantized value is $-W_n$.1These equations imply that the scaling factors effectively act as "global learning rates" for the magnitude of the weights in that layer. If the network determines that the positive features in a layer are suppressing the loss more effectively, the gradient $\frac{\partial L}{\partial W_p}$ will drive $W_p$ higher.3.3 Inference Efficiency of TTQOne might assume that introducing two FP32 scalars ($W_p, W_n$) per layer complicates inference. However, in deployment, these scalars can be handled efficiently. The core matrix multiplication is performed using the integer values $\{-1, 0, 1\}$. This allows for the use of accumulator-based arithmetic (only additions and subtractions, no multiplications).$$y = (X \cdot W_{ternary}) \odot S$$The scaling application $\odot S$ (where $S$ handles the mapping of $+1 \to W_p$ and $-1 \to W_n$) is a linear operation that can be fused with the subsequent Batch Normalization layer or applied as a computationally cheap element-wise scaling vector after the heavy convolution is complete. Thus, TTQ retains the hardware benefits of sparse, multiplier-free execution while achieving accuracy often exceeding full-precision baselines on datasets like CIFAR-10 and ImageNet.14. Channel-wise Ternary Quantization: Addressing HeterogeneityWhile TTQ optimizes scaling factors at the layer level (one pair of $W_p, W_n$ per layer), this granularity is often insufficient for modern architectures like Vision Transformers (ViTs) or Depth-wise Separable CNNs (like MobileNet). In these architectures, the distribution of weights can vary drastically between different output channels or attention heads.4.1 The Limits of Layer-wise QuantizationIn a standard convolutional layer, different filters (channels) detect different features—edges, textures, or semantic patterns. The dynamic range of weights required to detect a high-contrast edge might be very different from the range required for a subtle texture.If a single threshold $\Delta$ and scale $\alpha$ are enforced across the entire layer:Outlier Dominance: A channel with large weight magnitudes (outliers) will force the global $\alpha$ to be large.Signal Suppression: Channels with naturally smaller weight magnitudes will fall entirely within the range $$ relative to the outlier-driven threshold. These channels will be quantized entirely to zeros ("Dead Weights"), effectively pruning them from the network and destroying the features they were meant to extract.This phenomenon is particularly acute in ViTs, where the "dead weight" problem can lead to model collapse.84.2 The TerViT Methodology: Per-Channel ParametersTerViT (Ternary Vision Transformer) addresses this by increasing the granularity of quantization to the channel level. For a weight tensor $W \in \mathbb{R}^{C_{out} \times C_{in} \times K \times K}$, TerViT assigns independent quantization parameters to each output channel $c \in \{1,..., C_{out}\}$.The channel-wise ternary weight $T_{W,c}$ is defined as:$$T_{W,c} = \alpha_c \cdot \text{Ternarize}(W_c, \Delta_c)$$4.2.1 Parameter CalculationIn TerViT, the scaling factor $\alpha_c$ for channel $c$ is typically calculated using the Channel-wise Absolute Mean (CAM):$$\alpha_c = \frac{1}{N} \sum_{i=1}^N |w_{c,i}|$$This ensures that the scale is representative of the average magnitude of that specific channel.The threshold $\Delta_c$ is similarly adaptive, often set as a function of the L1 norm of the channel's weights:$$\Delta_c = 0.7 \times \frac{\|W_c\|_1}{N}$$By localizing these parameters, TerViT ensures that a "quiet" channel (small weights) gets a small $\Delta_c$, allowing its internal structure to be quantized into $\{-1, 0, 1\}$ rather than being zeroed out. Conversely, a "loud" channel gets a large $\Delta_c$ and $\alpha_c$, preserving its high-magnitude impact.84.3 Memory vs. Accuracy Trade-offsMoving from layer-wise to channel-wise quantization introduces a storage overhead.Layer-wise: Stores 1 scale (32-bit float) per tensor.Channel-wise: Stores $C_{out}$ scales (32-bit floats) per tensor.For a layer with $C_{out} = 1024$, the overhead is $1024 \times 4$ bytes = 4 KB. Given that the weight tensor itself (if $1024 \times 1024$) would occupy $\approx 1$ MB in standard 8-bit quantization or $\approx 256$ KB in 2-bit ternary, the 4 KB overhead ($< 2\%$) is negligible compared to the accuracy gains.Empirically, channel-wise quantization allows ternary ViT models (DeiT-S) to recover within ~3-4% of their full-precision accuracy, whereas layer-wise approaches often fail to converge entirely due to the sensitivity of the Self-Attention mechanism.94.4 Hardware Implementation: BroadcastingImplementing channel-wise quantization requires careful handling of broadcasting in the computation graph. In PyTorch, if the weight tensor $W$ has shape (Out, In, K, K), the scale vector $\alpha$ of shape (Out) must be reshaped to (Out, 1, 1, 1) to be broadcast compatible.$$W_{quant} = W_{ternary} \times \alpha.\text{view}(C_{out}, 1, 1, 1)$$During inference on specialized hardware (FPGA/ASIC), this corresponds to performing the accumulator-based convolution first, and then applying a vector-vector multiplication to the output channels using the stored $\alpha$ values.115. Dynamic Threshold Computation and Deadzone ManagementThe threshold $\Delta$ is arguably the most sensitive hyperparameter in ternary quantization. It defines the boundary between "information" (non-zero) and "sparsity" (zero).5.1 Statistics-based Thresholds (Fixed)Initial methods like Ternary Weight Networks (TWN) utilized fixed statistical properties. Li & Liu (2016) derived that for a standard normal distribution $\mathcal{N}(0, 1)$, the threshold that maximizes information retention is $\Delta \approx 0.7 \sigma$. Thus, for each forward pass (or frozen after training), $\Delta$ is computed as:$$\Delta_l = 0.7 \times \mathbb{E}(|W_l|)$$While computationally cheap, this approach is rigid. It assumes weights are Gaussian, which is often untrue for trained weights (which may be bimodal or Laplacian), and it prevents the network from adapting the sparsity level to the task difficulty.15.2 Learned Thresholds (LSQ and LSQ+)Learned Step Size Quantization (LSQ) introduces the concept of making the quantizer's step size (and thus the threshold) a differentiable parameter.In LSQ, $\Delta$ is initialized based on statistics but is then updated via gradient descent.$$\frac{\partial L}{\partial \Delta} \approx \sum_i \frac{\partial L}{\partial w_{q,i}} \cdot \frac{\partial w_{q,i}}{\partial \Delta}$$The gradient $\frac{\partial w_{q,i}}{\partial \Delta}$ is non-zero primarily at the transition points (where a weight flips from 0 to 1). This allows the network to "squeeze" or "expand" the threshold. If the loss implies that more model capacity is needed, the gradient will drive $\Delta$ down, reducing sparsity. If the model is overfitting or noise is high, $\Delta$ may increase. LSQ has demonstrated state-of-the-art results (SOTA) by fine-tuning these thresholds, often matching FP32 accuracy in ResNet-50.135.3 Soft Thresholding (STTN)A major issue with hard thresholds is the gradient mismatch. Weights near the threshold $\Delta$ are unstable; a tiny update can flip them from 0 to 1, causing a massive jump in the loss.Soft Threshold Ternary Networks (STTN) address this by creating a probabilistic or "soft" transition. Instead of a hard step function, STTN decomposes the ternary kernel into two binary kernels during training, or uses a smoothed activation function (like a steep sigmoid) to approximate the thresholding.This ensures that gradients exist even for weights in the "deadzone" (near zero), preventing them from dying permanently. At inference time, the soft functions are hardened back to discrete steps.165.4 The "Deadzone Trapping" Problem and TEQUILAIn large-scale models like LLMs, ternary quantization creates a massive "deadzone" $(-\Delta, \Delta)$ where weights are zeroed.Problem: Weights initialized in this zone receive zero gradients (or very noisy STE gradients) and often never escape. They become "trapped," rendering a huge portion of the model's parameters useless (dead).Solution (TEQUILA): "Trapping-free TErnary QUantization" (TEQUILA) proposes a novel mechanism where these "dead" weights are not simply discarded during training. Instead, they are repurposed as dynamic biases or are allowed to accumulate gradients via a secondary pathway (e.g., a "reactivation" gradient). This ensures that if a zeroed weight becomes relevant later in training (e.g., for a specific attention head), it retains the ability to grow out of the deadzone and become active. This technique has shown to improve accuracy in Ternary LLMs by over 2.6%, nearly matching BF16 performance.176. Regularization: Shaping the Weight DistributionTo ensure that weights naturally converge to the quantization targets $\{-1, 0, 1\}$, specialized regularization terms are added to the loss function $L_{total} = L_{task} + \lambda R(W)$.6.1 Amplitude Regularization (The "Double-Well" Potential)Standard L2 regularization ($||w||^2$) pulls weights to zero. For ternary networks, we want weights to cluster around $0$ and the scaling factors $\pm \alpha$.A common "amplitude regularizer" is defined as:$$R_{amp}(W) = \sum_{w \in W} (|w| - \alpha)^2 \cdot |w|^2$$This function is minimal when $|w| \approx \alpha$ (ternary states $\pm 1$) or when $w \approx 0$. It creates a "multi-modal" energy landscape that penalizes weights that "drift" into the ambiguous regions (e.g., $0.5\alpha$), forcing them to snap to a valid quantization state.56.2 Sparsity RegularizationIn energy-critical applications, sparsity is a direct proxy for efficiency. We can explicitly encourage sparsity by penalizing the non-zero states.$$R_{sparsity}(W) = \beta \sum |w_{ternary}|$$Or, more sophisticatedly, using Smart Ternary Quantization (STQ). STQ introduces a learnable parameter $\beta$ that controls the quantization depth. A regularizer based on $\tan(\beta)$ can dynamically shift the layer's preference. If $\tan(\beta)$ is high, it penalizes sparsity (preferring binary). If low, it encourages sparsity (preferring ternary/zero). This allows the network to learn which layers need high information density and which can be sparse.46.3 Entropy Maximization (Preventing Collapse)A risk in aggressive quantization is layer collapse, where all weights in a layer fall below $\Delta$ and become zero. To prevent this, Entropy Regularization or "Variance of Variance" regularization is used.$$R_{entropy}(W) = - \sum p(w) \log p(w)$$This term forces the weight distribution to maintain a certain width or variance, preventing it from shrinking entirely into the deadzone during the early phases of training.207. Training Strategies: Navigating the Discrete LandscapeTraining ternary networks is notoriously unstable. "Naive" training (applying quantization from epoch 0) often leads to divergence. Successful implementation relies on strategic training phases.7.1 Progressive Quantization (The "Annealing" Strategy)The most robust strategy is Quantization Annealing.Phase 1 (FP32/INT8): Train the model with high precision (or 8-bit). This acts as a "warm-up," allowing the weights to find a good basin of attraction in the loss landscape.Phase 2 (Soft Ternary): Introduce the quantization function but with a low "temperature" $\tau$. The function acts like a steep sigmoid/tanh.$$\text{SoftQ}(x) = \tanh(x / \tau)$$Phase 3 (Hard Ternary): Anneal $\tau \to 0$ over time, hardening the soft sigmoid into a discrete step function. This allows the gradients to adapt gradually to the loss of information.217.2 Knowledge Distillation (KD)Ternary models often struggle to capture the full nuance of the probability distribution over classes (the "dark knowledge"). KD is used to transfer this knowledge from a teacher (FP32) to the student (Ternary).Task-Aware Distillation (BiTMedViT): In medical imaging or fine-grained classification, the student is trained to minimize both the Cross-Entropy loss with ground truth AND the Kullback-Leibler (KL) divergence with the teacher's logits.$$L = (1-\lambda)L_{CE} + \lambda \tau^2 KL(p_{student}, p_{teacher})$$Additionally, Attention Distillation forces the ternary student's attention maps (in ViTs) to match the teacher's, ensuring the quantized model focuses on the same image regions.157.3 Mixed Precision TrainingNot all layers are created equal. The first convolution (which processes continuous input pixels) and the final fully connected layer (which produces class probabilities) are highly sensitive to quantization noise.Standard practice in TTQ and TerViT is to keep the first and last layers in 8-bit or FP16, while ternarizing all intermediate "hidden" layers. This hybrid approach often yields a 1-2% accuracy boost with negligible impact on overall model size, as the intermediate layers constitute >95% of the parameter count.38. Integration with Spiking Neural Networks (SNNs)SNNs operate on asynchronous, discrete "spikes" over time, offering extreme energy efficiency. Ternary quantization is a natural fit for SNNs, evolving the paradigm from "Binary Spikes" (0, 1) to "Ternary Spikes" (-1, 0, 1).8.1 Ternary Spikes vs. Binary SpikesBinary SNNs suffer from low information capacity. To encode a value like $0.5$, a binary neuron must fire in a specific temporal pattern (e.g., 1-0-1-0). This requires many timesteps ($T$), increasing latency.Ternary Sikes introduce an inhibitory state (-1). A neuron can now actively suppress downstream activity. This increases the information content per spike to $\approx 1.58$ bits.Result: Ternary SNNs can achieve the same accuracy as Binary SNNs with fewer timesteps (e.g., reducing $T$ from 16 to 4). Since energy in SNNs is proportional to timesteps ($E \propto T$), this directly reduces energy consumption.258.2 Adaptive Threshold SpikingIn SNNs, the firing threshold $V_{th}$ is analogous to the quantization threshold $\Delta$. "Adaptive Threshold Spiking" makes $V_{th}$ dynamic.$$V_{th}(t) = \frac{1}{k} \cdot \text{mean}(|U_{mem}(t)|)$$If the membrane potential $U_{mem}$ is high (strong stimulus), the threshold rises to prevent a "burst" of spikes (which would waste energy). If potentials are low, the threshold drops to ensure signals propagate. This homeostasis mechanism, inspired by biological neurons, optimizes the sparsity of the spike train dynamically.268.3 Energy Efficiency QuantificationThe energy advantage of Ternary SNNs is quantifiable.MAC Operation (ANN): $E_{MAC} \approx 4.6$ pJ (INT8).AC Operation (SNN): SNNs are multiplier-free. They only perform Accumulate (Add) operations when a spike occurs. $E_{AC} \approx 0.9$ pJ.Total Energy: $E_{total} = N_{spikes} \times E_{AC}$.Due to the sparsity (zeros) in ternary spikes, $N_{spikes}$ is low. Research indicates Ternary SNNs consume $\approx 69\%$ of the energy of an equivalent ANN on spatial architectures, with total inference energy as low as 0.39 mJ/frame on custom 28nm CMOS hardware.289. Implementation Details: PyTorch StructureImplementing ternary quantization requires overriding the computational graph. Below is a structured implementation of a Channel-wise Ternary Layer with Learnable Parameters.9.1 The Custom Autograd FunctionThis class handles the forward quantization and the backward STE gradient.Pythonimport torch
import torch.nn as nn
import torch.nn.functional as F

class TernaryQuantFunction(torch.autograd.Function):
    @staticmethod
    def forward(ctx, input, threshold, pos_scale, neg_scale):
        # input: [Out, In, K, K]
        # threshold, pos_scale, neg_scale: [Out, 1, 1, 1] (Channel-wise)
        
        # Save input for backward (optional, usually not needed for simple STE)
        ctx.save_for_backward(input)
        
        # 1. Create masks for ternary states
        # Broadcasting happens here automatically if shapes align
        mask_pos = (input > threshold).float()
        mask_neg = (input < -threshold).float()
        mask_zero = 1.0 - mask_pos - mask_neg
        
        # 2. Assign learned values
        # Out = (+Wp * mask_pos) + (-Wn * mask_neg) + (0 * mask_zero)
        out = (pos_scale * mask_pos) + (-neg_scale * mask_neg)
        
        return out

    @staticmethod
    def backward(ctx, grad_output):
        # Straight-Through Estimator (STE)
        input, = ctx.saved_tensors
        
        # Gradient for Input: Pass-through (Identity)
        # Optional: Clip gradients for weights far outside range
        grad_input = grad_output.clone()
        grad_input[input.abs() > 2.0] = 0 
        
        # Gradient for Scales (Learnable parameters)
        # We must sum gradients over all dimensions except the channel dimension (Dim 0)
        # Because scales are per-channel [Out, 1, 1, 1]
        
        # Re-compute masks to know which weights contributed to which scale
        # (In optimized code, save these masks in ctx to avoid re-computation)
        # Note: Thresholds are tensors, need to be accessed via ctx or saved
        # simplified for clarity:
        grad_pos_scale = (grad_output * (input > 0)).sum(dim=(1, 2, 3), keepdim=True)
        grad_neg_scale = (grad_output * (input < 0) * -1).sum(dim=(1, 2, 3), keepdim=True)
        
        # Gradient for Threshold (if learnable)
        # Complex derivation involving Dirac delta at transition points
        # Often approximated or set to 0 in simpler implementations
        grad_threshold = torch.zeros_like(grad_pos_scale) 
        
        return grad_input, grad_threshold, grad_pos_scale, grad_neg_scale
9.2 The Layer ModuleThis module maintains the latent weights and handles the parameter initialization.Pythonclass TernaryConv2d(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):
        super(TernaryConv2d, self).__init__()
        self.stride = stride
        self.padding = padding
        
        # Latent Weights (Full Precision)
        self.weight = nn.Parameter(torch.randn(out_channels, in_channels, kernel_size, kernel_size) * 0.01)
        
        # Learnable Scales (Per-Channel)
        # Shape: [Out, 1, 1, 1] for broadcasting
        self.pos_scale = nn.Parameter(torch.ones(out_channels, 1, 1, 1))
        self.neg_scale = nn.Parameter(torch.ones(out_channels, 1, 1, 1))
        
        # Learnable Threshold (Per-Channel)
        self.threshold = nn.Parameter(torch.ones(out_channels, 1, 1, 1) * 0.05)

    def forward(self, x):
        # 1. Quantize Weights on-the-fly
        w_ternary = TernaryQuantFunction.apply(self.weight, self.threshold, self.pos_scale, self.neg_scale)
        
        # 2. Perform Convolution with Quantized Weights
        # Note: In real hardware, this is an Accumulate operation.
        # In PyTorch GPU, it uses FP32 CUDA cores but with discrete values.
        out = F.conv2d(x, w_ternary, stride=self.stride, padding=self.padding)
        
        return out
9.3 Broadcasting MechanicsThe crucial element in TernaryConv2d is the shape of pos_scale (and others). By initializing them as (out_channels, 1, 1, 1), PyTorch's broadcasting rules automatically expand them to match the weight tensor (out_channels, in_channels, kernel, kernel) during element-wise operations. This implements the channel-wise logic efficiently without explicit loops.1110. Empirical Results and DiscussionThe shift from TWN (Fixed) to TTQ/TerViT (Learned) has yielded significant performance gains across benchmarks.10.1 Convolutional Neural Networks (CNNs)Table 1 highlights the recovery of accuracy using learned parameters on the ImageNet dataset (ResNet-18).ModelMethodBit-width (W/A)Top-1 AccuracyCompressionSpeedup (Theory)ResNet-18FP32 (Baseline)32/3269.7%1x1xResNet-18BWN (Binary)1/3260.8%32x2xResNet-18TWN (Fixed $\Delta$)2/3261.8%16x4xResNet-18TTQ (Learned)2/3266.6%16x4xResNet-18LSQ (Learned Step)2/3267.6%16x4xResNet-50LSQ+2/3275.3%16x4xInsight: TTQ recovers nearly 5% accuracy over the fixed-threshold TWN. This confirms the hypothesis that asymmetric scaling ($W_p \neq W_n$) is critical for preserving features in deep ReLU networks. The gap between ternary (TTQ) and FP32 is now $<3\%$, which is acceptable for many edge applications given the 16x compression.110.2 Vision Transformers (ViTs)Table 2 demonstrates the necessity of channel-wise quantization for Transformers (DeiT-Small).ModelMethodGranularityTop-1 AccuracyDeiT-SFP32N/A79.9%DeiT-STWNLayer-wise72.9%DeiT-STerViTChannel-wise74.2%DeiT-STerViT + DistillationChannel-wise76.8%Swin-STerViTChannel-wise79.3%Insight: Layer-wise quantization causes a massive 7% drop in DeiT-S. Channel-wise quantization (TerViT) recovers significantly more performance. When combined with Knowledge Distillation, the ternary transformer comes within ~3% of the FP32 baseline. The Swin Transformer, with its hierarchical structure, proves even more robust to ternary quantization, achieving 79.3% (vs 83.2% FP32).810.3 Large Language Models (LLMs) & Generative ModelsModelMethodSizeMetricNoteLLaMA-1BStandard Ternary1B38.6% (Avg)High degradationLLaMA-1BTEQUILA1B42.4% (Avg)Deadzone Reactivation usedDiT-XL/2TerDiT600MFID: CompetitiveDiffusion ModelInsight: For LLMs, the "Deadzone" problem is the primary bottleneck. TEQUILA's strategy of reactivating dead weights provides a massive boost (+3.8%), proving that simply "learning" thresholds isn't enough; one must actively manage the sparsity to prevent parameter collapse.17ConclusionTernary quantization has matured from a simple compression heuristic into a rigorous sub-field of neural architecture design. The progression from Fixed Layer-wise (TWN) to Learned Asymmetric (TTQ) and finally Adaptive Channel-wise (TerViT) methodologies illustrates a clear trend: precision is not about bit-width, but about adaptability. By integrating learnable thresholds, asymmetric scaling, and deadzone management into the backpropagation loop, ternary networks now rival full-precision models in accuracy while offering the extreme energy efficiency required for the next generation of ubiquitous AI. As hardware accelerators increasingly support sparse, accumulator-only logic (like SNN neuromorphic chips), ternary representation is poised to become the standard for edge intelligence.Sources Cited1 TTQ Methodology and Gradient Derivations8 TerViT and Channel-wise Quantization for Transformers4 Smart Ternary Quantization and Regularization16 Soft Threshold Ternary Networks (STTN)17 TEQUILA and Deadzone Trapping in LLMs25 Ternary Spiking Neural Networks13 Learned Step Size Quantization (LSQ)11 PyTorch Implementation and Broadcasting29 Energy Efficiency of SNNs vs ANNs