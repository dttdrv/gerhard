{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# cell 1: environment setup (v14.1 - Hyperparameter Tuning)\n",
    "# =============================================================================\n",
    "import os\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import json\n",
    "import base64\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from dataclasses import dataclass, asdict, field\n",
    "from typing import Dict, List, Optional, Tuple, Any\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# v15 full validation run: disable torch.compile for stability with rich spike instrumentation outputs.\n",
    "USE_TORCH_COMPILE = False\n",
    "USE_GRADIENT_CHECKPOINTING = True\n",
    "\n",
    "# generate timestamp for this run\n",
    "RUN_TIMESTAMP = datetime.now().strftime('%Y-%m-%d_%H%M%S')\n",
    "print(f\"run timestamp: {RUN_TIMESTAMP}\")\n",
    "\n",
    "# detect platform\n",
    "IS_KAGGLE = 'KAGGLE_KERNEL_RUN_TYPE' in os.environ\n",
    "IS_COLAB = 'COLAB_GPU' in os.environ or 'google.colab' in sys.modules\n",
    "PLATFORM = 'kaggle' if IS_KAGGLE else 'colab' if IS_COLAB else 'local'\n",
    "OUTPUT_DIR = '/kaggle/working/outputs' if IS_KAGGLE else 'outputs'\n",
    "\n",
    "for subdir in ['figures', 'checkpoints', 'logs', 'results']:\n",
    "    os.makedirs(f'{OUTPUT_DIR}/{subdir}', exist_ok=True)\n",
    "\n",
    "print(f\"platform: {PLATFORM}\")\n",
    "print(f\"output directory: {OUTPUT_DIR}\")\n",
    "print(f\"torch.compile: {'enabled' if USE_TORCH_COMPILE else 'disabled'}\")\n",
    "print(f\"gradient checkpointing: {'enabled' if USE_GRADIENT_CHECKPOINTING else 'disabled'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# cell 2: pytorch and hardware setup (v14.1)\n",
    "# =============================================================================\n",
    "# Dependency bootstrap: fail early with clear errors, auto-install when allowed.\n",
    "import importlib\n",
    "import subprocess\n",
    "\n",
    "AUTO_INSTALL_MISSING_DEPS = os.environ.get('GERHARD_AUTO_INSTALL_DEPS', '1') == '1'\n",
    "\n",
    "def ensure_dependency(import_name: str, pip_name: str = None, required: bool = True) -> bool:\n",
    "    pip_target = pip_name or import_name\n",
    "    try:\n",
    "        importlib.import_module(import_name)\n",
    "        return True\n",
    "    except ModuleNotFoundError as exc:\n",
    "        if AUTO_INSTALL_MISSING_DEPS:\n",
    "            print(f\"missing dependency '{import_name}', attempting pip install: {pip_target}\")\n",
    "            try:\n",
    "                subprocess.check_call([sys.executable, '-m', 'pip', 'install', '--quiet', pip_target])\n",
    "                importlib.import_module(import_name)\n",
    "                print(f\"installed dependency '{import_name}'\")\n",
    "                return True\n",
    "            except Exception as install_exc:\n",
    "                message = (\n",
    "                    f\"failed to install dependency '{import_name}' via pip target '{pip_target}'. \"\n",
    "                    f\"set GERHARD_AUTO_INSTALL_DEPS=0 to disable auto-install attempts.\"\n",
    "                )\n",
    "                if required:\n",
    "                    raise ModuleNotFoundError(message) from install_exc\n",
    "                print(f\"warning: {message}\")\n",
    "                return False\n",
    "        message = (\n",
    "            f\"missing dependency '{import_name}'. \"\n",
    "            f\"install '{pip_target}' or set GERHARD_AUTO_INSTALL_DEPS=1 for automatic install.\"\n",
    "        )\n",
    "        if required:\n",
    "            raise ModuleNotFoundError(message) from exc\n",
    "        print(f\"warning: {message}\")\n",
    "        return False\n",
    "\n",
    "# Required dependencies used later in the notebook.\n",
    "ensure_dependency('tqdm', 'tqdm', required=True)\n",
    "ensure_dependency('transformers', 'transformers', required=True)\n",
    "ensure_dependency('datasets', 'datasets', required=True)\n",
    "\n",
    "# Optional plotting dependency. Training/evaluation can proceed without it.\n",
    "MATPLOTLIB_AVAILABLE = ensure_dependency('matplotlib', 'matplotlib', required=False)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "import numpy as np\n",
    "\n",
    "if MATPLOTLIB_AVAILABLE:\n",
    "    import matplotlib\n",
    "    matplotlib.use('Agg')\n",
    "    import matplotlib.pyplot as plt\n",
    "else:\n",
    "    plt = None\n",
    "    print(\"warning: matplotlib unavailable; plot generation will be skipped.\")\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "SEED = 42\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"gpu: {gpu_name}\")\n",
    "    print(f\"memory: {gpu_memory:.1f} gb\")\n",
    "\n",
    "# v14: set float32 matmul precision for torch.compile\n",
    "if USE_TORCH_COMPILE and hasattr(torch, 'set_float32_matmul_precision'):\n",
    "    torch.set_float32_matmul_precision('high')\n",
    "    print(\"float32 matmul precision: high (for torch.compile)\")\n",
    "\n",
    "print(f\"device: {DEVICE}\")\n",
    "print(f\"pytorch: {torch.__version__}\")\n",
    "\n",
    "# check torch.compile availability\n",
    "TORCH_COMPILE_AVAILABLE = hasattr(torch, 'compile') and torch.__version__ >= '2.0'\n",
    "print(f\"torch.compile available: {TORCH_COMPILE_AVAILABLE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# cell 4: configuration (v14.1 - Hyperparameter Tuning per External LLM)\n",
    "# =============================================================================\n",
    "@dataclass\n",
    "class Config:\n",
    "    # Version for dynamic labeling (NEVER hardcode versions elsewhere!)\n",
    "    VERSION: str = 'v15'\n",
    "    VERSION_DESC: str = 'SpikingBrain information encoding validation'\n",
    "    \n",
    "    # gpt-2 teacher (frozen, pre-trained)\n",
    "    teacher_name: str = \"gpt2\"\n",
    "\n",
    "    # student model architecture - v14.1: capacity increase (512d, 5L, ~56M)\n",
    "    d_model: int = 768      # v14.3: safer scaling (512->768 instead of 512->1024)\n",
    "    n_layers: int = 5       # v10 value (DO NOT reduce)\n",
    "    vocab_size: int = 50257\n",
    "    max_seq_len: int = 256\n",
    "\n",
    "    # ==========================================================================\n",
    "    # v14.1: Feature Dynamics Distillation (FDD) with CKA Loss\n",
    "    # ==========================================================================\n",
    "    # FDD aligns layer-wise dynamics (Δh) between student and teacher\n",
    "    # Uses CKA (Centered Kernel Alignment) - projector-free, dimension-agnostic\n",
    "    use_fdd: bool = True\n",
    "    fdd_weight: float = 0.1           # v14.1: 100x increase (CKA bounded [0,1], safe)\n",
    "    fdd_warmup_steps: int = 500       # Don't enable until step 500\n",
    "    fdd_loss_type: str = \"cka\"        # Options: \"cka\" (recommended), \"mse\"\n",
    "    fdd_kill_threshold: float = 0.10  # Disable if PPL increases >10%\n",
    "\n",
    "    # ==========================================================================\n",
    "    # v14.1: Hard Distillation (CE with ground truth)\n",
    "    # ==========================================================================\n",
    "    # Anchors student to correct tokens, not just teacher's soft distribution\n",
    "    ce_hard_weight: float = 0.5       # Ground truth CE loss weight\n",
    "    \n",
    "    # Layer mapping: student_layer -> teacher_layer\n",
    "    # With 5 student layers and 12 teacher layers:\n",
    "    # We align early/middle/late semantic representations\n",
    "    # Default: {0: 2, 2: 6, 4: 10}\n",
    "    fdd_n_align_layers: int = 3       # Number of layer pairs to align\n",
    "\n",
    "    # ==========================================================================\n",
    "    # v14.1: Extended Training (same as v13.1)\n",
    "    # ==========================================================================\n",
    "    distill_steps: int = 7000       # v14.3: more steps for larger model\n",
    "    distill_lr: float = 2e-4       # v14.3: reduced for larger model\n",
    "    warmup_steps: int = 100\n",
    "    min_lr: float = 1e-6\n",
    "\n",
    "    # v14.1: gradient accumulation\n",
    "    accumulation_steps: int = 2       # effective batch = 8 * 2 = 16\n",
    "\n",
    "    # ==========================================================================\n",
    "    # v14.1: Early Stopping (same as v13.1)\n",
    "    # ==========================================================================\n",
    "    use_early_stopping: bool = True\n",
    "    early_stopping_patience: int = 800  # v14.3: more patience for larger model\n",
    "    min_ppl_delta: float = 1.0\n",
    "\n",
    "    # ==========================================================================\n",
    "    # v14.1: POCL DISABLED (failed in v13)\n",
    "    # ==========================================================================\n",
    "    use_pocl: bool = False\n",
    "    pocl_stages: int = 3\n",
    "    pocl_temp_schedule: tuple = (1.0, 1.5, 2.0)\n",
    "    pocl_pretrain_steps: int = 100\n",
    "\n",
    "    # ==========================================================================\n",
    "    # v14.1: CTKD ENABLED (proven in v12.1, v13.1)\n",
    "    # ==========================================================================\n",
    "    use_ctkd: bool = True\n",
    "    tau_min: float = 1.0\n",
    "    tau_max: float = 5.0\n",
    "    tau_init: float = 2.0\n",
    "    lambda_max: float = 1.0\n",
    "    lambda_warmup_ratio: float = 0.25  # v14.3: slower CTKD ramp-up\n",
    "\n",
    "    # Legacy flags (all disabled for v14.1)\n",
    "    use_learnable_temperature: bool = False\n",
    "    use_channel_wise_spikes: bool = False\n",
    "    use_progressive_stages: bool = False\n",
    "    temperature: float = 2.0\n",
    "\n",
    "    # Hidden alignment DISABLED (using FDD instead)\n",
    "    hidden_align_weight: float = 0.0\n",
    "    teacher_d_model: int = 768\n",
    "    teacher_n_layers: int = 12\n",
    "    temperature_lr: float = 0.001\n",
    "\n",
    "    # lora for ttt\n",
    "    lora_rank: int = 8\n",
    "    lora_alpha: float = 16.0\n",
    "    ttt_lr: float = 1e-4\n",
    "    ttt_steps: int = 100\n",
    "\n",
    "    # spiking parameters\n",
    "    spike_alpha: float = 1.0\n",
    "    spike_threshold_mix: float = 0.35\n",
    "    spike_surrogate_temp: float = 0.10\n",
    "\n",
    "    # v15 spike semantic/health shaping\n",
    "    use_spike_semantic_loss: bool = True\n",
    "    spike_semantic_weight: float = 0.08\n",
    "    spike_semantic_warmup_steps: int = 400\n",
    "    spike_target_threshold_scale: float = 0.75\n",
    "\n",
    "    # general training\n",
    "    batch_size: int = 8\n",
    "    max_grad_norm: float = 1.0\n",
    "    eval_interval: int = 300\n",
    "\n",
    "config = Config()\n",
    "\n",
    "print(f\"configuration (v14.1 - Hyperparameter Tuning per External LLM):\")\n",
    "print(f\"  teacher: {config.teacher_name} (124m params)\")\n",
    "print(f\"  student: d={config.d_model}, layers={config.n_layers} (~56M params)\")\n",
    "print(f\"\")\n",
    "print(f\"{config.VERSION} CHANGES (based on external LLM diagnosis):\")\n",
    "print(f\"  d_model: 320 -> 512 (capacity increase for ternary compensation)\")\n",
    "print(f\"  fdd_weight: 0.001 -> 0.1 (enable alignment gradient signal)\")\n",
    "print(f\"  ce_hard_weight: {config.ce_hard_weight} (NEW - ground truth anchoring)\")\n",
    "print(f\"\")\n",
    "print(f\"{config.VERSION} INNOVATION - Feature Dynamics Distillation (FDD):\")\n",
    "print(f\"  use_fdd: {config.use_fdd}\")\n",
    "print(f\"  fdd_weight: {config.fdd_weight} (100x increase, CKA bounded [0,1], safe)\")\n",
    "print(f\"  fdd_warmup_steps: {config.fdd_warmup_steps}\")\n",
    "print(f\"  fdd_loss_type: {config.fdd_loss_type}\")\n",
    "print(f\"  fdd_n_align_layers: {config.fdd_n_align_layers}\")\n",
    "print(f\"  fdd_kill_threshold: {config.fdd_kill_threshold} (10% PPL increase triggers disable)\")\n",
    "print(f\"\")\n",
    "print(f\"  FDD Strategy:\")\n",
    "print(f\"    - Align layer DYNAMICS (Δh), not just hidden states\")\n",
    "print(f\"    - Use CKA loss (projector-free, dimension-agnostic)\")\n",
    "print(f\"    - 100x weight increase (was too weak before)\")\n",
    "print(f\"    - Safety kill-switch if PPL regresses\")\n",
    "print(f\"\")\n",
    "print(f\"{config.VERSION}: Hard Distillation:\")\n",
    "print(f\"  ce_hard_weight: {config.ce_hard_weight}\")\n",
    "print(f\"  Formula: L = KL + 0.5*CE + 0.1*FDD\")\n",
    "print(f\"\")\n",
    "print(f\"{config.VERSION}: CTKD (proven technique):\")\n",
    "print(f\"  use_ctkd: {config.use_ctkd}\")\n",
    "print(f\"  Temperature bounds: [{config.tau_min}, {config.tau_max}]\")\n",
    "print(f\"  Lambda warmup: {config.lambda_warmup_ratio*100:.0f}%\")\n",
    "print(f\"\")\n",
    "print(f\"{config.VERSION}: Extended Training:\")\n",
    "print(f\"  distill_steps: {config.distill_steps}\")\n",
    "print(f\"  warmup_steps: {config.warmup_steps}\")\n",
    "print(f\"  min_lr: {config.min_lr}\")\n",
    "print(f\"\")\n",
    "print(f\"{config.VERSION}: Early Stopping:\")\n",
    "print(f\"  use_early_stopping: {config.use_early_stopping}\")\n",
    "print(f\"  patience: {config.early_stopping_patience} steps\")\n",
    "print(f\"  min_delta: {config.min_ppl_delta} PPL\")\n",
    "print(f\"\")\n",
    "print(f\"disabled features:\")\n",
    "print(f\"  POCL: {config.use_pocl} (failed in v13)\")\n",
    "print(f\"  channel-wise spikes: {config.use_channel_wise_spikes}\")\n",
    "print(f\"  old hidden alignment: {config.hidden_align_weight}\")\n",
    "print(f\"\")\n",
    "print(f\"training:\")\n",
    "print(f\"  accumulation: {config.accumulation_steps} (effective batch = {config.batch_size * config.accumulation_steps})\")\n",
    "print(f\"  spike_threshold_mix: {config.spike_threshold_mix}\")\n",
    "print(f\"  spike_surrogate_temp: {config.spike_surrogate_temp}\")\n",
    "print(f\"  use_spike_semantic_loss: {config.use_spike_semantic_loss}\")\n",
    "if config.use_spike_semantic_loss:\n",
    "    print(f\"    spike_semantic_weight: {config.spike_semantic_weight}\")\n",
    "    print(f\"    spike_semantic_warmup_steps: {config.spike_semantic_warmup_steps}\")\n",
    "    print(f\"    spike_target_threshold_scale: {config.spike_target_threshold_scale}\")\n",
    "print(f\"\")\n",
    "print(f\"targets:\")\n",
    "print(f\"  PPL: validate v14.3 (306.89) spike encoding quality\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# cell 3: PRE-TRAINING VALIDATION (run before training to catch issues)\n",
    "# =============================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"PRE-TRAINING VALIDATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "validation_errors = []\n",
    "validation_warnings = []\n",
    "\n",
    "# 1. Config Sanity Checks\n",
    "print(\"\")\n",
    "print(\"[1] CONFIG SANITY CHECKS\")\n",
    "\n",
    "if config.d_model < 256:\n",
    "    validation_errors.append(f\"d_model={config.d_model} too small (min 256)\")\n",
    "elif config.d_model > 2048:\n",
    "    validation_warnings.append(f\"d_model={config.d_model} very large - check VRAM\")\n",
    "print(f\"  d_model: {config.d_model}\")\n",
    "\n",
    "if config.n_layers < 3:\n",
    "    validation_errors.append(f\"n_layers={config.n_layers} too few\")\n",
    "print(f\"  n_layers: {config.n_layers}\")\n",
    "\n",
    "print(f\"  fdd_weight: {config.fdd_weight}\")\n",
    "print(f\"  ce_hard_weight: {config.ce_hard_weight}\")\n",
    "print(f\"  spike_threshold_mix: {config.spike_threshold_mix}\")\n",
    "if not (0.0 <= config.spike_threshold_mix <= 1.0):\n",
    "    validation_errors.append(f\"spike_threshold_mix={config.spike_threshold_mix} must be in [0, 1]\")\n",
    "if config.spike_surrogate_temp <= 0:\n",
    "    validation_errors.append(f\"spike_surrogate_temp={config.spike_surrogate_temp} must be > 0\")\n",
    "if config.use_spike_semantic_loss:\n",
    "    print(f\"  spike_semantic_weight: {config.spike_semantic_weight}\")\n",
    "    print(f\"  spike_semantic_warmup_steps: {config.spike_semantic_warmup_steps}\")\n",
    "    if config.spike_semantic_weight < 0:\n",
    "        validation_errors.append(f\"spike_semantic_weight={config.spike_semantic_weight} must be >= 0\")\n",
    "print(f\"  VERSION: {config.VERSION}\")\n",
    "print(f\"  VERSION_DESC: {config.VERSION_DESC}\")\n",
    "\n",
    "# 2. Memory Estimation\n",
    "print(\"\")\n",
    "print(\"[2] MEMORY ESTIMATION\")\n",
    "embed_params = config.vocab_size * config.d_model * 2\n",
    "layer_params = config.n_layers * config.d_model * config.d_model * 8\n",
    "total_params_est = embed_params + layer_params\n",
    "print(f\"  Estimated params: ~{total_params_est/1e6:.1f}M\")\n",
    "\n",
    "vram_est_gb = (total_params_est * 4 * 3) / 1e9\n",
    "print(f\"  Estimated VRAM: ~{vram_est_gb:.1f}GB\")\n",
    "\n",
    "if vram_est_gb > 14:\n",
    "    validation_warnings.append(f\"VRAM estimate {vram_est_gb:.1f}GB may exceed 16GB limit\")\n",
    "    print(f\"  WARNING: May exceed 16GB VRAM limit!\")\n",
    "\n",
    "# 3. Training Config\n",
    "print(\"\")\n",
    "print(\"[3] TRAINING CONFIG\")\n",
    "print(f\"  distill_steps: {config.distill_steps}\")\n",
    "print(f\"  distill_lr: {config.distill_lr}\")\n",
    "print(f\"  batch_size: {config.batch_size}\")\n",
    "if hasattr(config, 'accumulation_steps'):\n",
    "    eff_batch = config.batch_size * config.accumulation_steps\n",
    "    print(f\"  effective_batch: {eff_batch}\")\n",
    "print(f\"  early_stopping: patience={config.early_stopping_patience}\")\n",
    "\n",
    "# 4. Feature Flags\n",
    "print(\"\")\n",
    "print(\"[4] FEATURE FLAGS\")\n",
    "print(f\"  use_fdd: {config.use_fdd}\")\n",
    "print(f\"  use_ctkd: {config.use_ctkd}\")\n",
    "print(f\"  use_pocl: {config.use_pocl}\")\n",
    "\n",
    "# Summary\n",
    "print(\"\")\n",
    "print(\"=\" * 70)\n",
    "if validation_errors:\n",
    "    print(f\"VALIDATION FAILED - {len(validation_errors)} errors:\")\n",
    "    for e in validation_errors:\n",
    "        print(f\"   - {e}\")\n",
    "    raise RuntimeError(\"Fix validation errors before training!\")\n",
    "elif validation_warnings:\n",
    "    print(f\"VALIDATION PASSED WITH {len(validation_warnings)} WARNINGS:\")\n",
    "    for w in validation_warnings:\n",
    "        print(f\"   - {w}\")\n",
    "else:\n",
    "    print(\"ALL VALIDATIONS PASSED\")\n",
    "print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# cell 6: v13 PROPER CTKD Implementation\n",
    "# =============================================================================\n",
    "# References:\n",
    "# - CTKD Paper: https://arxiv.org/abs/2211.16231\n",
    "# - GRL Origin: Ganin & Lempitsky (2015) https://arxiv.org/abs/1409.7495\n",
    "# - torch-gradient-reversal: https://pypi.org/project/torch-gradient-reversal/\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# GradientReversalFunction (Custom Autograd)\n",
    "# -----------------------------------------------------------------------------\n",
    "class GradientReversalFunction(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    Gradient Reversal Layer for adversarial training.\n",
    "    \n",
    "    Forward: Identity mapping f(x) = x\n",
    "    Backward: Negates gradient ∂f/∂x = -λ * grad\n",
    "    \n",
    "    This enables min-max optimization in a single backward pass:\n",
    "    - Student minimizes loss (normal gradients)\n",
    "    - Temperature maximizes loss (reversed gradients via GRL)\n",
    "    \n",
    "    Reference: Ganin & Lempitsky, \"Unsupervised Domain Adaptation by Backpropagation\"\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, x, lambda_):\n",
    "        # Save lambda for backward pass\n",
    "        ctx.lambda_ = lambda_\n",
    "        # Forward is identity (must clone to avoid in-place issues)\n",
    "        return x.clone()\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # Backward negates and scales gradient\n",
    "        # Returns: (grad for x, grad for lambda_)\n",
    "        # lambda_ is a hyperparameter, doesn't need gradient\n",
    "        return -ctx.lambda_ * grad_output, None\n",
    "\n",
    "\n",
    "class GradientReversalLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Module wrapper for GradientReversalFunction.\n",
    "    \n",
    "    Usage:\n",
    "        grl = GradientReversalLayer()\n",
    "        grl.set_lambda(0.5)  # Set adversarial strength\n",
    "        y = grl(x)  # Forward: y = x, Backward: grad_x = -0.5 * grad_y\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lambda_ = 1.0\n",
    "    \n",
    "    def set_lambda(self, lambda_: float):\n",
    "        \"\"\"Set the adversarial strength (0 = no reversal, 1 = full reversal).\"\"\"\n",
    "        self.lambda_ = lambda_\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return GradientReversalFunction.apply(x, self.lambda_)\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Lambda Scheduler (Cosine with Warmup)\n",
    "# -----------------------------------------------------------------------------\n",
    "def get_lambda(step: int, total_steps: int, lambda_max: float = 1.0, \n",
    "               warmup_ratio: float = 0.2) -> float:\n",
    "    \"\"\"\n",
    "    Cosine schedule for adversarial strength λ.\n",
    "    \n",
    "    - During warmup (first warmup_ratio of training): λ = 0\n",
    "      Temperature learns freely to find reasonable range\n",
    "    - After warmup: λ increases from 0 to lambda_max via cosine\n",
    "      Gradually increases adversarial pressure\n",
    "    \n",
    "    Args:\n",
    "        step: Current training step\n",
    "        total_steps: Total number of training steps\n",
    "        lambda_max: Maximum λ value (default 1.0 = full reversal)\n",
    "        warmup_ratio: Fraction of training for warmup (default 0.2 = 20%)\n",
    "    \n",
    "    Returns:\n",
    "        Current λ value in [0, lambda_max]\n",
    "    \"\"\"\n",
    "    warmup_steps = int(total_steps * warmup_ratio)\n",
    "    \n",
    "    if step < warmup_steps:\n",
    "        return 0.0\n",
    "    \n",
    "    # Progress after warmup [0, 1]\n",
    "    progress = (step - warmup_steps) / max(total_steps - warmup_steps, 1)\n",
    "    # Cosine increase from 0 to lambda_max\n",
    "    lambda_ = lambda_max * (1 - math.cos(math.pi * progress)) / 2\n",
    "    return lambda_\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# CTKDTemperature (Proper Implementation with GRL)\n",
    "# -----------------------------------------------------------------------------\n",
    "class CTKDTemperature(nn.Module):\n",
    "    \"\"\"\n",
    "    Curriculum Temperature for Knowledge Distillation (CTKD).\n",
    "    \n",
    "    Key features:\n",
    "    1. Adversarial learning via Gradient Reversal Layer\n",
    "    2. Sigmoid bounding for smooth gradients at boundaries\n",
    "    3. Proper initialization via logit transform\n",
    "    \n",
    "    The temperature module tries to MAXIMIZE the KL loss (via GRL),\n",
    "    finding the \"hardest\" temperature for the student.\n",
    "    The student tries to MINIMIZE the KL loss.\n",
    "    This adversarial game leads to optimal curriculum difficulty.\n",
    "    \n",
    "    Reference: Li et al., \"Curriculum Temperature for Knowledge Distillation\", AAAI 2023\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, tau_min: float = 1.0, tau_max: float = 5.0, init: float = 2.0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tau_min: Minimum temperature (default 1.0)\n",
    "            tau_max: Maximum temperature (default 5.0, conservative for LLMs)\n",
    "            init: Initial temperature (default 2.0)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.tau_min = tau_min\n",
    "        self.tau_range = tau_max - tau_min\n",
    "        \n",
    "        # Initialize raw parameter so sigmoid outputs init value\n",
    "        # sigmoid(raw) = (init - tau_min) / tau_range\n",
    "        # raw = logit((init - tau_min) / tau_range)\n",
    "        init_normalized = (init - tau_min) / self.tau_range\n",
    "        init_normalized = max(0.01, min(0.99, init_normalized))  # Clamp for numerical stability\n",
    "        init_raw = math.log(init_normalized / (1 - init_normalized))  # logit function\n",
    "        \n",
    "        self.raw_temp = nn.Parameter(torch.tensor(init_raw, dtype=torch.float32))\n",
    "        self.grl = GradientReversalLayer()\n",
    "        \n",
    "        # Store config for logging\n",
    "        self.tau_min_val = tau_min\n",
    "        self.tau_max_val = tau_max\n",
    "        self.init_val = init\n",
    "    \n",
    "    def forward(self, lambda_: float) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute temperature with GRL applied.\n",
    "        \n",
    "        Args:\n",
    "            lambda_: Current adversarial strength from scheduler\n",
    "        \n",
    "        Returns:\n",
    "            Temperature τ ∈ [tau_min, tau_max]\n",
    "        \"\"\"\n",
    "        # Set GRL strength\n",
    "        self.grl.set_lambda(lambda_)\n",
    "        \n",
    "        # Apply GRL to raw parameter (this is where gradient reversal happens!)\n",
    "        raw_reversed = self.grl(self.raw_temp)\n",
    "        \n",
    "        # Sigmoid bounding (smooth, differentiable at boundaries)\n",
    "        tau = self.tau_min + self.tau_range * torch.sigmoid(raw_reversed)\n",
    "        \n",
    "        return tau\n",
    "    \n",
    "    def get_temperature(self) -> float:\n",
    "        \"\"\"Get current temperature without GRL (for logging/display).\"\"\"\n",
    "        with torch.no_grad():\n",
    "            tau = self.tau_min + self.tau_range * torch.sigmoid(self.raw_temp)\n",
    "            return tau.item()\n",
    "    \n",
    "    def get_raw_value(self) -> float:\n",
    "        \"\"\"Get raw (unbounded) parameter value (for debugging).\"\"\"\n",
    "        return self.raw_temp.item()\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Legacy Classes (kept for backward compatibility)\n",
    "# -----------------------------------------------------------------------------\n",
    "class LearnableTemperature(nn.Module):\n",
    "    \"\"\"\n",
    "    DEPRECATED: Simple learnable temperature WITHOUT GRL.\n",
    "    Kept for backward compatibility. Use CTKDTemperature instead.\n",
    "    \n",
    "    WARNING: This class caused temperature runaway in v12!\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, init: float = 2.0):\n",
    "        super().__init__()\n",
    "        self.log_temp = nn.Parameter(torch.log(torch.tensor(init)))\n",
    "    \n",
    "    def forward(self) -> torch.Tensor:\n",
    "        return torch.exp(self.log_temp).clamp(1.0, 10.0)\n",
    "    \n",
    "    def get_temperature(self) -> float:\n",
    "        return self.forward().item()\n",
    "\n",
    "\n",
    "class ChannelWiseTernarySpike(nn.Module):\n",
    "    \"\"\"\n",
    "    Per-channel learnable alpha and amplitude for ternary spikes.\n",
    "    DISABLED in v13 due to structural symmetry issue with RWKV.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, alpha_init: float = 1.0):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.alpha = nn.Parameter(torch.ones(d_model) * alpha_init)\n",
    "        self.amplitude = nn.Parameter(torch.ones(d_model))\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x_abs_mean = x.abs().mean(dim=(0, 1), keepdim=True)\n",
    "        threshold = self.alpha * x_abs_mean\n",
    "        threshold = threshold.clamp(min=0.01, max=10.0)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            pos_mask = (x > threshold).float()\n",
    "            neg_mask = (x < -threshold).float()\n",
    "            spike_signs = pos_mask - neg_mask\n",
    "        \n",
    "        spikes = self.amplitude * spike_signs\n",
    "        return spikes + (x - x.detach())\n",
    "    \n",
    "    def get_amplitude(self) -> float:\n",
    "        return self.amplitude.mean().item()\n",
    "    \n",
    "    def get_stats(self) -> dict:\n",
    "        return {\n",
    "            'alpha_mean': self.alpha.mean().item(),\n",
    "            'alpha_std': self.alpha.std().item(),\n",
    "            'amplitude_mean': self.amplitude.mean().item(),\n",
    "            'amplitude_std': self.amplitude.std().item(),\n",
    "        }\n",
    "\n",
    "    def get_amplitude_stats(self) -> dict:\n",
    "        return {\n",
    "            'mean': self.amplitude.mean().item(),\n",
    "            'std': self.amplitude.std().item(),\n",
    "            'min': self.amplitude.min().item(),\n",
    "            'max': self.amplitude.max().item(),\n",
    "        }\n",
    "\n",
    "\n",
    "class TrainableTernarySpike(nn.Module):\n",
    "    \"\"\"Original trainable ternary spike with scalar amplitude (from v8).\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        alpha: float = 1.0,\n",
    "        threshold_mix: float = 0.35,\n",
    "        surrogate_temp: float = 0.10,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.threshold_mix = threshold_mix\n",
    "        self.surrogate_temp = surrogate_temp\n",
    "        self.amplitude = nn.Parameter(torch.ones(1))\n",
    "\n",
    "    def forward(self, x: torch.Tensor, return_aux: bool = False):\n",
    "        token_scale = x.abs().mean(dim=-1, keepdim=True)\n",
    "        channel_scale = x.abs().mean(dim=(0, 1), keepdim=True)\n",
    "        threshold = self.alpha * (\n",
    "            (1.0 - self.threshold_mix) * token_scale + self.threshold_mix * channel_scale\n",
    "        )\n",
    "        threshold = threshold.clamp(min=0.01, max=10.0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pos_mask = (x > threshold).float()\n",
    "            neg_mask = (x < -threshold).float()\n",
    "            spike_signs = pos_mask - neg_mask\n",
    "\n",
    "        amplitude = self.amplitude.clamp(min=0.25, max=4.0)\n",
    "        spikes = amplitude * spike_signs\n",
    "        spikes = spikes + (x - x.detach())\n",
    "\n",
    "        if return_aux:\n",
    "            soft_activity = torch.sigmoid((x.abs() - threshold) / self.surrogate_temp)\n",
    "            return spikes, {\n",
    "                'threshold': threshold.detach(),\n",
    "                'soft_activity': soft_activity,\n",
    "            }\n",
    "        return spikes\n",
    "\n",
    "    def get_amplitude(self) -> float:\n",
    "        return self.amplitude.item()\n",
    "\n",
    "\n",
    "def get_stage_params(step: int, total_steps: int = 3000) -> dict:\n",
    "    \"\"\"Progressive training stages (POCL) - kept for infrastructure.\"\"\"\n",
    "    if step < total_steps * 0.4:\n",
    "        return {'stage': 1, 'temp_target': 1.0, 'align_mult': 0.0, 'alpha': 0.9}\n",
    "    elif step < total_steps * 0.7:\n",
    "        return {'stage': 2, 'temp_target': 1.5, 'align_mult': 0.5, 'alpha': 0.7}\n",
    "    else:\n",
    "        return {'stage': 3, 'temp_target': 2.0, 'align_mult': 1.0, 'alpha': 0.5}\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Unit Tests for CTKD Components\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"=\"*60)\n",
    "print(\"v13 CTKD Component Tests\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Test 1: GRL Gradient Reversal\n",
    "print(\"\\n[1] GRL Gradient Reversal Test\")\n",
    "grl = GradientReversalLayer()\n",
    "grl.set_lambda(1.0)\n",
    "x_test = torch.tensor([2.0], requires_grad=True)\n",
    "y_test = grl(x_test)\n",
    "loss_test = y_test.sum()\n",
    "loss_test.backward()\n",
    "expected_grad = -1.0  # GRL should negate: 1 * -1.0 = -1.0\n",
    "actual_grad = x_test.grad.item()\n",
    "grl_pass = abs(actual_grad - expected_grad) < 1e-6\n",
    "print(f\"  Input grad without GRL would be: +1.0\")\n",
    "print(f\"  Input grad with GRL (λ=1.0): {actual_grad:.4f}\")\n",
    "print(f\"  Expected: {expected_grad:.4f}\")\n",
    "print(f\"  {'PASS' if grl_pass else 'FAIL'}\")\n",
    "del x_test, y_test, loss_test\n",
    "\n",
    "# Test 2: Lambda Schedule\n",
    "print(\"\\n[2] Lambda Schedule Test\")\n",
    "total = 3000\n",
    "warmup = 0.2\n",
    "# During warmup\n",
    "lambda_0 = get_lambda(0, total, warmup_ratio=warmup)\n",
    "lambda_500 = get_lambda(500, total, warmup_ratio=warmup)\n",
    "# After warmup\n",
    "lambda_1500 = get_lambda(1500, total, warmup_ratio=warmup)\n",
    "lambda_2999 = get_lambda(2999, total, warmup_ratio=warmup)\n",
    "\n",
    "warmup_pass = lambda_0 == 0.0 and lambda_500 == 0.0\n",
    "increase_pass = 0 < lambda_1500 < lambda_2999 <= 1.0\n",
    "lambda_pass = warmup_pass and increase_pass\n",
    "print(f\"  λ(0) = {lambda_0:.4f} (should be 0.0)\")\n",
    "print(f\"  λ(500) = {lambda_500:.4f} (should be 0.0, still in warmup)\")\n",
    "print(f\"  λ(1500) = {lambda_1500:.4f} (should be > 0)\")\n",
    "print(f\"  λ(2999) = {lambda_2999:.4f} (should be ≈ 1.0)\")\n",
    "print(f\"  {'PASS' if lambda_pass else 'FAIL'}\")\n",
    "\n",
    "# Test 3: Temperature Bounds\n",
    "print(\"\\n[3] Temperature Bounds Test\")\n",
    "temp_module = CTKDTemperature(tau_min=1.0, tau_max=5.0, init=2.0).to(DEVICE)\n",
    "init_temp = temp_module.get_temperature()\n",
    "\n",
    "# Force extreme raw values\n",
    "with torch.no_grad():\n",
    "    temp_module.raw_temp.fill_(-100)\n",
    "    tau_low = temp_module.get_temperature()\n",
    "    \n",
    "    temp_module.raw_temp.fill_(100)\n",
    "    tau_high = temp_module.get_temperature()\n",
    "    \n",
    "    # Reset to init\n",
    "    init_normalized = (2.0 - 1.0) / 4.0\n",
    "    init_raw = math.log(init_normalized / (1 - init_normalized))\n",
    "    temp_module.raw_temp.fill_(init_raw)\n",
    "\n",
    "bounds_pass = (1.0 <= tau_low <= 1.01) and (4.99 <= tau_high <= 5.0) and (1.9 <= init_temp <= 2.1)\n",
    "print(f\"  Initial temp: {init_temp:.4f} (should be ≈ 2.0)\")\n",
    "print(f\"  Min bound test: {tau_low:.4f} (should be ≈ 1.0)\")\n",
    "print(f\"  Max bound test: {tau_high:.4f} (should be ≈ 5.0)\")\n",
    "print(f\"  {'PASS' if bounds_pass else 'FAIL'}\")\n",
    "\n",
    "# Test 4: End-to-End Gradient Flow\n",
    "print(\"\\n[4] End-to-End Gradient Flow Test\")\n",
    "temp_module_test = CTKDTemperature(tau_min=1.0, tau_max=5.0, init=2.0).to(DEVICE)\n",
    "lambda_test = 0.5\n",
    "\n",
    "# Simulate forward pass\n",
    "T = temp_module_test(lambda_test)\n",
    "fake_kl_loss = T * 2.0  # Gradient ∂L/∂T = 2.0\n",
    "\n",
    "# Without GRL: optimizer would DECREASE T to minimize loss\n",
    "# With GRL: optimizer should INCREASE T (because grad is reversed)\n",
    "fake_kl_loss.backward()\n",
    "\n",
    "raw_grad = temp_module_test.raw_temp.grad.item()\n",
    "# The gradient through sigmoid and GRL should be negative (reversed)\n",
    "# Original: ∂L/∂raw > 0 would decrease raw\n",
    "# With GRL: ∂L/∂raw < 0 (negated), so optimizer increases raw\n",
    "grad_flow_pass = raw_grad < 0  # Should be negative due to GRL\n",
    "print(f\"  Loss = T * 2.0, so ∂L/∂T = 2.0 (positive)\")\n",
    "print(f\"  Without GRL: raw_grad would be positive (decrease T)\")\n",
    "print(f\"  With GRL (λ=0.5): raw_grad = {raw_grad:.4f} (should be negative)\")\n",
    "print(f\"  {'PASS' if grad_flow_pass else 'FAIL'}\")\n",
    "del temp_module_test\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "all_pass = grl_pass and lambda_pass and bounds_pass and grad_flow_pass\n",
    "print(f\"CTKD Component Tests: {'ALL PASS' if all_pass else 'SOME FAILED'}\")\n",
    "if not all_pass:\n",
    "    print(\"WARNING: Fix failing tests before running training!\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# cell 6.5: v14.1 FDD (Feature Dynamics Distillation) with CKA Loss\n",
    "# =============================================================================\n",
    "# References:\n",
    "# - CKA: Kornblith et al., \"Similarity of Neural Network Representations Revisited\"\n",
    "# - FDD: Feature Dynamics Distillation (view transformer as ODE)\n",
    "# - v7 lesson: Hidden alignment with weight=1.0 caused PPL regression to 1655!\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Centered Kernel Alignment (CKA) Loss\n",
    "# -----------------------------------------------------------------------------\n",
    "def cka_loss(X: torch.Tensor, Y: torch.Tensor, eps: float = 1e-8) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Centered Kernel Alignment (CKA) loss for representation alignment.\n",
    "\n",
    "    CKA is a similarity measure between feature representations that is:\n",
    "    - Invariant to orthogonal transformations\n",
    "    - Invariant to isotropic scaling\n",
    "    - Does NOT require dimension matching (projector-free!)\n",
    "\n",
    "    Args:\n",
    "        X: Student features [n_samples, dim_x]\n",
    "        Y: Teacher features [n_samples, dim_y]\n",
    "        eps: Small constant for numerical stability\n",
    "\n",
    "    Returns:\n",
    "        Loss = 1 - CKA (minimize to maximize alignment)\n",
    "        CKA = 1 means perfect alignment, CKA = 0 means no alignment\n",
    "\n",
    "    Note: n_samples must match, but dim_x and dim_y can differ!\n",
    "\n",
    "    CRITICAL: Uses float32 to prevent overflow in mixed precision training.\n",
    "    With n=2048 samples, Gram matrix sums can exceed float16 max (~65504).\n",
    "    This is training-time only - does NOT affect student's ternary activations.\n",
    "    \"\"\"\n",
    "    # CRITICAL: Force float32 to prevent overflow in mixed precision\n",
    "    # Under torch.cuda.amp.autocast(), tensors are float16 by default\n",
    "    # Gram matrix sums: 4M elements squared and summed -> can exceed 65504\n",
    "    with torch.cuda.amp.autocast(enabled=False):\n",
    "        X = X.float()\n",
    "        Y = Y.float()\n",
    "\n",
    "        # Validate input shapes\n",
    "        assert X.dim() == 2 and Y.dim() == 2, f\"Expected 2D tensors, got X:{X.dim()}D, Y:{Y.dim()}D\"\n",
    "        assert X.size(0) == Y.size(0), f\"Sample count mismatch: X={X.size(0)}, Y={Y.size(0)}\"\n",
    "\n",
    "        # Center the features (critical for CKA)\n",
    "        X_centered = X - X.mean(dim=0, keepdim=True)\n",
    "        Y_centered = Y - Y.mean(dim=0, keepdim=True)\n",
    "\n",
    "        # Row-normalize for additional numerical stability\n",
    "        # This bounds Gram matrix elements to [-1, 1] range\n",
    "        X_norm = X_centered / (X_centered.norm(dim=1, keepdim=True) + eps)\n",
    "        Y_norm = Y_centered / (Y_centered.norm(dim=1, keepdim=True) + eps)\n",
    "\n",
    "        # Compute Gram matrices on normalized features\n",
    "        # K_X[i,j] = dot(X_norm[i], X_norm[j]) in [-1, 1]\n",
    "        K_X = X_norm @ X_norm.T  # [n, n]\n",
    "        K_Y = Y_norm @ Y_norm.T  # [n, n]\n",
    "\n",
    "        # HSIC (Hilbert-Schmidt Independence Criterion)\n",
    "        # Now numerically stable: elements in [-1, 1]^2 = [0, 1]\n",
    "        hsic_xy = (K_X * K_Y).sum()\n",
    "        hsic_xx = (K_X * K_X).sum()\n",
    "        hsic_yy = (K_Y * K_Y).sum()\n",
    "\n",
    "        # CKA = HSIC(X,Y) / sqrt(HSIC(X,X) * HSIC(Y,Y))\n",
    "        cka = hsic_xy / (torch.sqrt(hsic_xx * hsic_yy) + eps)\n",
    "\n",
    "        # Clamp to valid range (numerical safety)\n",
    "        cka = cka.clamp(0.0, 1.0)\n",
    "\n",
    "        # Return loss (1 - CKA): minimize loss = maximize alignment\n",
    "        return 1.0 - cka\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Layer Mapping for FDD\n",
    "# -----------------------------------------------------------------------------\n",
    "def get_fdd_layer_mapping(n_student_layers: int, n_teacher_layers: int,\n",
    "                          n_align_layers: int = 3) -> Dict[int, int]:\n",
    "    \"\"\"\n",
    "    Create layer mapping for Feature Dynamics Distillation.\n",
    "\n",
    "    Maps student layers to teacher layers for hidden state alignment.\n",
    "    Uses even spacing to cover early/middle/late representations.\n",
    "\n",
    "    Args:\n",
    "        n_student_layers: Number of student layers (e.g., 5)\n",
    "        n_teacher_layers: Number of teacher layers (e.g., 12)\n",
    "        n_align_layers: Number of layer pairs to align (default 3)\n",
    "\n",
    "    Returns:\n",
    "        Dict mapping student_layer_idx -> teacher_layer_idx\n",
    "\n",
    "    Example for 5 student, 12 teacher, 3 alignments:\n",
    "        {0: 2, 2: 7, 4: 11}  # Early, middle, late (with +2 offset)\n",
    "    \"\"\"\n",
    "    if n_align_layers > n_student_layers:\n",
    "        n_align_layers = n_student_layers\n",
    "\n",
    "    layer_map = {}\n",
    "\n",
    "    # Evenly space the student layers to align\n",
    "    student_indices = []\n",
    "    for i in range(n_align_layers):\n",
    "        # 0, 2, 4 for 3 alignments with 5 layers\n",
    "        idx = int(i * (n_student_layers - 1) / max(n_align_layers - 1, 1))\n",
    "        student_indices.append(idx)\n",
    "\n",
    "    # Map each student index to corresponding teacher layer\n",
    "    for s_idx in student_indices:\n",
    "        # Scale to teacher layers\n",
    "        t_idx = int((s_idx / (n_student_layers - 1)) * (n_teacher_layers - 1))\n",
    "        # Offset by 1-2 to avoid embedding layer\n",
    "        t_idx = max(2, min(t_idx + 2, n_teacher_layers - 1))\n",
    "        layer_map[s_idx] = t_idx\n",
    "\n",
    "    return layer_map\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Feature Dynamics Distillation Loss\n",
    "# -----------------------------------------------------------------------------\n",
    "def compute_fdd_loss(\n",
    "    student_hiddens: List[torch.Tensor],\n",
    "    teacher_hiddens: List[torch.Tensor],\n",
    "    layer_map: Dict[int, int],\n",
    "    loss_type: str = \"cka\"\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute Feature Dynamics Distillation (FDD) loss.\n",
    "\n",
    "    FDD views the transformer as solving an ODE: dh/dt = f(h, t)\n",
    "    where each layer is a discrete time step.\n",
    "\n",
    "    Instead of matching hidden states directly (which failed in v7),\n",
    "    we match the DYNAMICS (layer-to-layer changes): delta_h = h_{l+1} - h_l\n",
    "\n",
    "    This teaches the student HOW to transform features, not just WHAT features to have.\n",
    "\n",
    "    Args:\n",
    "        student_hiddens: List of student hidden states [h_0, h_1, ..., h_L]\n",
    "                        Each has shape [batch, seq, student_dim]\n",
    "        teacher_hiddens: List of teacher hidden states (from output_hidden_states=True)\n",
    "                        Each has shape [batch, seq, teacher_dim]\n",
    "        layer_map: Dict mapping student_layer_idx -> teacher_layer_idx\n",
    "        loss_type: \"cka\" (recommended) or \"mse\"\n",
    "\n",
    "    Returns:\n",
    "        FDD loss (scalar tensor)\n",
    "\n",
    "    Note: student_hiddens[0] is embedding, student_hiddens[1] is after layer 0, etc.\n",
    "    \"\"\"\n",
    "    total_loss = torch.tensor(0.0, device=student_hiddens[0].device)\n",
    "    n_pairs = 0\n",
    "\n",
    "    for s_layer, t_layer in layer_map.items():\n",
    "        # Validate indices\n",
    "        # student_hiddens: [embed, after_L0, after_L1, ..., after_L{n-1}]\n",
    "        # So layer i output is at index i+1\n",
    "        s_idx = s_layer + 1  # +1 because [0] is embedding\n",
    "        t_idx = t_layer + 1  # Same for teacher\n",
    "\n",
    "        # Check bounds\n",
    "        if s_idx + 1 >= len(student_hiddens):\n",
    "            continue\n",
    "        if t_idx + 1 >= len(teacher_hiddens):\n",
    "            continue\n",
    "\n",
    "        # Compute dynamics (velocity): delta_h = h_{l+1} - h_l\n",
    "        # Student dynamics: change from layer s_layer to s_layer+1\n",
    "        delta_s = student_hiddens[s_idx + 1] - student_hiddens[s_idx]  # [batch, seq, s_dim]\n",
    "\n",
    "        # Teacher dynamics: change from layer t_layer to t_layer+1\n",
    "        delta_t = teacher_hiddens[t_idx + 1] - teacher_hiddens[t_idx]  # [batch, seq, t_dim]\n",
    "\n",
    "        # Flatten for CKA: [batch, seq, dim] -> [batch*seq, dim]\n",
    "        batch_size, seq_len = delta_s.size(0), delta_s.size(1)\n",
    "        delta_s_flat = delta_s.reshape(batch_size * seq_len, -1)  # [n, s_dim]\n",
    "        delta_t_flat = delta_t.reshape(batch_size * seq_len, -1)  # [n, t_dim]\n",
    "\n",
    "        # Compute loss\n",
    "        if loss_type == \"cka\":\n",
    "            pair_loss = cka_loss(delta_s_flat, delta_t_flat)\n",
    "        elif loss_type == \"mse\":\n",
    "            # MSE requires dimension matching - use projector if needed\n",
    "            # For now, skip if dimensions don't match\n",
    "            if delta_s_flat.size(1) != delta_t_flat.size(1):\n",
    "                continue\n",
    "            pair_loss = F.mse_loss(delta_s_flat, delta_t_flat)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown loss_type: {loss_type}\")\n",
    "\n",
    "        total_loss = total_loss + pair_loss\n",
    "        n_pairs += 1\n",
    "\n",
    "    # Average over pairs\n",
    "    if n_pairs > 0:\n",
    "        total_loss = total_loss / n_pairs\n",
    "\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# FDD Weight Scheduler (with warmup)\n",
    "# -----------------------------------------------------------------------------\n",
    "def get_fdd_weight(step: int, fdd_warmup_steps: int, fdd_weight: float) -> float:\n",
    "    \"\"\"\n",
    "    Get FDD weight for current step with warmup.\n",
    "\n",
    "    Args:\n",
    "        step: Current training step\n",
    "        fdd_warmup_steps: Steps before FDD kicks in\n",
    "        fdd_weight: Maximum FDD weight\n",
    "\n",
    "    Returns:\n",
    "        Current FDD weight (0 during warmup, then fdd_weight)\n",
    "    \"\"\"\n",
    "    if step < fdd_warmup_steps:\n",
    "        return 0.0\n",
    "    return fdd_weight\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# FDD Unit Tests\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"=\"*60)\n",
    "print(\"v14.1 FDD Component Tests\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "fdd_tests = []\n",
    "\n",
    "# Test 1: CKA of identical tensors should return 0 loss (CKA=1)\n",
    "print(\"\\n[1] CKA Identical Tensors Test\")\n",
    "X_test = torch.randn(100, 64)\n",
    "cka_identical = cka_loss(X_test, X_test)\n",
    "identical_pass = cka_identical.item() < 0.01  # Should be ~0\n",
    "print(f\"  CKA loss of identical tensors: {cka_identical.item():.6f}\")\n",
    "print(f\"  Expected: ~0.0 (CKA=1 means perfect alignment)\")\n",
    "print(f\"  {'PASS' if identical_pass else 'FAIL'}\")\n",
    "fdd_tests.append(('CKA identical', identical_pass))\n",
    "\n",
    "# Test 2: CKA of orthogonal tensors should return high loss\n",
    "print(\"\\n[2] CKA Orthogonal Tensors Test\")\n",
    "X_orth = torch.randn(100, 64)\n",
    "Y_orth = torch.randn(100, 64)  # Different random = nearly orthogonal\n",
    "cka_orthogonal = cka_loss(X_orth, Y_orth)\n",
    "orthogonal_pass = 0.5 < cka_orthogonal.item() <= 1.0  # Should be high\n",
    "print(f\"  CKA loss of orthogonal tensors: {cka_orthogonal.item():.4f}\")\n",
    "print(f\"  Expected: 0.5-1.0 (low alignment)\")\n",
    "print(f\"  {'PASS' if orthogonal_pass else 'FAIL'}\")\n",
    "fdd_tests.append(('CKA orthogonal', orthogonal_pass))\n",
    "\n",
    "# Test 3: CKA handles different dimensions\n",
    "print(\"\\n[3] CKA Dimension Agnostic Test\")\n",
    "X_small = torch.randn(100, 32)   # 32 dims\n",
    "Y_large = torch.randn(100, 128)  # 128 dims\n",
    "try:\n",
    "    cka_diff_dim = cka_loss(X_small, Y_large)\n",
    "    dim_pass = True\n",
    "    print(f\"  CKA with dims (32, 128): {cka_diff_dim.item():.4f}\")\n",
    "except Exception as e:\n",
    "    dim_pass = False\n",
    "    print(f\"  ERROR: {e}\")\n",
    "print(f\"  {'PASS' if dim_pass else 'FAIL'}\")\n",
    "fdd_tests.append(('CKA dimension agnostic', dim_pass))\n",
    "\n",
    "# Test 4: Layer mapping correctness\n",
    "print(\"\\n[4] Layer Mapping Test\")\n",
    "layer_map = get_fdd_layer_mapping(n_student_layers=5, n_teacher_layers=12, n_align_layers=3)\n",
    "# Actual computation: s_idx=0 -> t_idx=0+2=2, s_idx=2 -> t_idx=5+2=7, s_idx=4 -> t_idx=11 (clamped)\n",
    "expected_map = {0: 2, 2: 7, 4: 11}  # Corrected expectation\n",
    "map_pass = layer_map == expected_map\n",
    "print(f\"  Generated map: {layer_map}\")\n",
    "print(f\"  Expected map: {expected_map}\")\n",
    "print(f\"  {'PASS' if map_pass else 'FAIL'}\")\n",
    "fdd_tests.append(('Layer mapping', map_pass))\n",
    "\n",
    "# Test 5: FDD loss computation\n",
    "print(\"\\n[5] FDD Loss Computation Test\")\n",
    "# Mock hidden states\n",
    "student_hiddens_mock = [torch.randn(2, 16, 320) for _ in range(6)]  # embed + 5 layers\n",
    "teacher_hiddens_mock = [torch.randn(2, 16, 768) for _ in range(13)]  # embed + 12 layers\n",
    "try:\n",
    "    fdd_loss_val = compute_fdd_loss(\n",
    "        student_hiddens_mock,\n",
    "        teacher_hiddens_mock,\n",
    "        layer_map,\n",
    "        loss_type=\"cka\"\n",
    "    )\n",
    "    fdd_pass = 0.0 <= fdd_loss_val.item() <= 1.0\n",
    "    print(f\"  FDD loss: {fdd_loss_val.item():.4f}\")\n",
    "    print(f\"  Expected: [0, 1]\")\n",
    "except Exception as e:\n",
    "    fdd_pass = False\n",
    "    print(f\"  ERROR: {e}\")\n",
    "print(f\"  {'PASS' if fdd_pass else 'FAIL'}\")\n",
    "fdd_tests.append(('FDD loss computation', fdd_pass))\n",
    "\n",
    "# Test 6: FDD weight scheduler\n",
    "print(\"\\n[6] FDD Weight Scheduler Test\")\n",
    "w_0 = get_fdd_weight(0, 500, 0.1)\n",
    "w_400 = get_fdd_weight(400, 500, 0.1)\n",
    "w_500 = get_fdd_weight(500, 500, 0.1)\n",
    "w_1000 = get_fdd_weight(1000, 500, 0.1)\n",
    "scheduler_pass = (w_0 == 0.0 and w_400 == 0.0 and w_500 == 0.1 and w_1000 == 0.1)\n",
    "print(f\"  weight(0): {w_0} (should be 0)\")\n",
    "print(f\"  weight(400): {w_400} (should be 0)\")\n",
    "print(f\"  weight(500): {w_500} (should be 0.001)\")\n",
    "print(f\"  weight(1000): {w_1000} (should be 0.001)\")\n",
    "print(f\"  {'PASS' if scheduler_pass else 'FAIL'}\")\n",
    "fdd_tests.append(('FDD weight scheduler', scheduler_pass))\n",
    "\n",
    "# Test 7: CKA float32 stability test (simulates mixed precision)\n",
    "print(\"\\n[7] CKA Float32 Stability Test\")\n",
    "# Simulate large values that would overflow in float16\n",
    "X_large = torch.randn(2048, 320) * 100  # Large values\n",
    "Y_large = torch.randn(2048, 768) * 100\n",
    "try:\n",
    "    cka_large = cka_loss(X_large, Y_large)\n",
    "    stability_pass = not (torch.isnan(cka_large) or torch.isinf(cka_large))\n",
    "    print(f\"  CKA with large values (n=2048): {cka_large.item():.4f}\")\n",
    "    print(f\"  No NaN/Inf: {stability_pass}\")\n",
    "except Exception as e:\n",
    "    stability_pass = False\n",
    "    print(f\"  ERROR: {e}\")\n",
    "print(f\"  {'PASS' if stability_pass else 'FAIL'}\")\n",
    "fdd_tests.append(('CKA float32 stability', stability_pass))\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "all_fdd_pass = all(p for _, p in fdd_tests)\n",
    "print(f\"FDD Component Tests: {'ALL PASS' if all_fdd_pass else 'SOME FAILED'}\")\n",
    "if not all_fdd_pass:\n",
    "    failed = [n for n, p in fdd_tests if not p]\n",
    "    print(f\"FAILED: {failed}\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# cell 7: v13 POCL (Progressive Overload Curriculum Learning)\n",
    "# =============================================================================\n",
    "# Reference: \"POCL: Progressive Overload Curriculum Learning\" (2025)\n",
    "# arXiv:2506.05695\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Sample Difficulty Scoring\n",
    "# -----------------------------------------------------------------------------\n",
    "def compute_sample_difficulty(student, teacher, dataloader, device, max_batches=50):\n",
    "    \"\"\"\n",
    "    Compute difficulty scores for each sample using student-teacher divergence.\n",
    "\n",
    "    Difficulty = average (CE loss + KL divergence) per sample.\n",
    "    Higher score = harder sample for the student.\n",
    "\n",
    "    Uses a small pre-trained student to get meaningful gradients.\n",
    "\n",
    "    Args:\n",
    "        student: Student model (should be briefly pre-trained)\n",
    "        teacher: Teacher model (frozen)\n",
    "        dataloader: Training data loader\n",
    "        device: Compute device\n",
    "        max_batches: Limit batches for efficiency\n",
    "\n",
    "    Returns:\n",
    "        Dict with sample indices and difficulty scores\n",
    "    \"\"\"\n",
    "    student.eval()\n",
    "    teacher.eval()\n",
    "\n",
    "    all_difficulties = []\n",
    "    all_indices = []\n",
    "    sample_idx = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(dataloader):\n",
    "            if batch_idx >= max_batches:\n",
    "                break\n",
    "\n",
    "            ids = batch[0].to(device, non_blocking=True)\n",
    "            batch_size = ids.size(0)\n",
    "\n",
    "            # Get logits\n",
    "            s_logits = student(ids)\n",
    "            t_logits = teacher(ids).logits\n",
    "\n",
    "            # Per-sample difficulty (average over sequence)\n",
    "            # 1. Cross-entropy with teacher as target\n",
    "            s_probs = F.softmax(s_logits, dim=-1)\n",
    "            t_probs = F.softmax(t_logits, dim=-1)\n",
    "\n",
    "            # KL divergence per sample\n",
    "            kl_div = F.kl_div(\n",
    "                F.log_softmax(s_logits, dim=-1),\n",
    "                t_probs,\n",
    "                reduction='none'\n",
    "            ).sum(dim=-1).mean(dim=-1)  # [batch_size]\n",
    "\n",
    "            # Cross-entropy per sample (using teacher hard targets)\n",
    "            t_tokens = t_logits.argmax(dim=-1)\n",
    "            ce_loss = F.cross_entropy(\n",
    "                s_logits.view(-1, s_logits.size(-1)),\n",
    "                t_tokens.view(-1),\n",
    "                reduction='none'\n",
    "            ).view(batch_size, -1).mean(dim=-1)  # [batch_size]\n",
    "\n",
    "            # Combined difficulty\n",
    "            difficulty = kl_div + ce_loss  # [batch_size]\n",
    "\n",
    "            all_difficulties.extend(difficulty.cpu().tolist())\n",
    "            all_indices.extend(range(sample_idx, sample_idx + batch_size))\n",
    "            sample_idx += batch_size\n",
    "\n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f\"  Scoring batch {batch_idx+1}/{max_batches}...\")\n",
    "\n",
    "    student.train()\n",
    "\n",
    "    return {\n",
    "        'indices': all_indices,\n",
    "        'difficulties': all_difficulties,\n",
    "        'num_samples': len(all_indices)\n",
    "    }\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Data Partitioning by Difficulty\n",
    "# -----------------------------------------------------------------------------\n",
    "def partition_by_difficulty(difficulties_dict, n_stages=3):\n",
    "    \"\"\"\n",
    "    Partition data into stages by difficulty (easy -> hard).\n",
    "\n",
    "    Stage 1: Easiest 33%\n",
    "    Stage 2: Easiest 66% (includes stage 1)\n",
    "    Stage 3: All 100% (includes stages 1+2)\n",
    "\n",
    "    Args:\n",
    "        difficulties_dict: Output from compute_sample_difficulty()\n",
    "        n_stages: Number of stages (default 3)\n",
    "\n",
    "    Returns:\n",
    "        List of index lists, one per stage (cumulative)\n",
    "    \"\"\"\n",
    "    indices = difficulties_dict['indices']\n",
    "    difficulties = difficulties_dict['difficulties']\n",
    "\n",
    "    # Sort by difficulty (ascending = easy first)\n",
    "    sorted_pairs = sorted(zip(indices, difficulties), key=lambda x: x[1])\n",
    "    sorted_indices = [idx for idx, _ in sorted_pairs]\n",
    "\n",
    "    n = len(sorted_indices)\n",
    "    stage_indices = []\n",
    "\n",
    "    for stage in range(n_stages):\n",
    "        # Cumulative: stage 1 = 33%, stage 2 = 66%, stage 3 = 100%\n",
    "        end_idx = int(n * (stage + 1) / n_stages)\n",
    "        stage_indices.append(sorted_indices[:end_idx])\n",
    "\n",
    "    return stage_indices\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Brief Pre-training for Difficulty Scoring\n",
    "# -----------------------------------------------------------------------------\n",
    "def pretrain_for_difficulty_scoring(student, teacher, train_loader, cfg, device, steps=100):\n",
    "    \"\"\"\n",
    "    Brief pre-training so difficulty scores are meaningful.\n",
    "\n",
    "    Without pre-training, student predictions are random garbage,\n",
    "    making all samples appear equally difficult.\n",
    "\n",
    "    Args:\n",
    "        student: Student model\n",
    "        teacher: Teacher model (frozen)\n",
    "        train_loader: Training data loader\n",
    "        cfg: Config object\n",
    "        device: Compute device\n",
    "        steps: Number of pre-training steps\n",
    "\n",
    "    Returns:\n",
    "        Student model (modified in-place)\n",
    "    \"\"\"\n",
    "    print(f\"Pre-training student for {steps} steps (for difficulty scoring)...\")\n",
    "\n",
    "    optimizer = torch.optim.AdamW(student.parameters(), lr=cfg.distill_lr, weight_decay=0.01)\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    student.train()\n",
    "    teacher.eval()\n",
    "\n",
    "    step = 0\n",
    "    pbar = tqdm(total=steps, desc='Pre-training')\n",
    "\n",
    "    for batch in train_loader:\n",
    "        if step >= steps:\n",
    "            break\n",
    "\n",
    "        ids = batch[0].to(device, non_blocking=True)\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            with torch.no_grad():\n",
    "                t_logits = teacher(ids).logits\n",
    "\n",
    "            s_logits = student(ids)\n",
    "\n",
    "            # Simple KL loss (no temperature complexity)\n",
    "            T = 2.0\n",
    "            s_log = F.log_softmax(s_logits / T, dim=-1)\n",
    "            t_prob = F.softmax(t_logits / T, dim=-1)\n",
    "            loss = F.kl_div(\n",
    "                s_log.view(-1, s_logits.size(-1)),\n",
    "                t_prob.view(-1, t_logits.size(-1)),\n",
    "                reduction='batchmean'\n",
    "            ) * (T ** 2)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        step += 1\n",
    "        pbar.update(1)\n",
    "        if step % 20 == 0:\n",
    "            pbar.set_postfix(loss=f\"{loss.item():.3f}\")\n",
    "\n",
    "    pbar.close()\n",
    "    print(f\"Pre-training complete. Final loss: {loss.item():.3f}\")\n",
    "\n",
    "    return student\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Get Stage Temperature (Fixed Schedule)\n",
    "# -----------------------------------------------------------------------------\n",
    "def get_pocl_temperature(step, total_steps, temp_schedule, n_stages=3):\n",
    "    \"\"\"\n",
    "    Get temperature for current POCL stage.\n",
    "\n",
    "    Args:\n",
    "        step: Current training step\n",
    "        total_steps: Total training steps\n",
    "        temp_schedule: Tuple of temperatures per stage (e.g., (1.0, 1.5, 2.0))\n",
    "        n_stages: Number of stages\n",
    "\n",
    "    Returns:\n",
    "        Temperature for current stage\n",
    "    \"\"\"\n",
    "    current_stage = get_pocl_stage(step, total_steps, n_stages)\n",
    "    return temp_schedule[current_stage]\n",
    "\n",
    "\n",
    "def get_pocl_stage(step, total_steps, n_stages=3):\n",
    "    \"\"\"\n",
    "    Get current POCL stage (0-indexed).\n",
    "\n",
    "    Uses rounded boundaries to ensure even distribution:\n",
    "    - 5000 steps, 3 stages: boundaries at 1667, 3333\n",
    "    - Stage 0: steps 0-1666\n",
    "    - Stage 1: steps 1667-3332\n",
    "    - Stage 2: steps 3333-4999\n",
    "    \"\"\"\n",
    "    for i in range(n_stages - 1):\n",
    "        boundary = round((i + 1) * total_steps / n_stages)\n",
    "        if step < boundary:\n",
    "            return i\n",
    "    return n_stages - 1\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# POCL Unit Tests\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"=\"*60)\n",
    "print(\"v13 POCL Component Tests\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Test 1: Temperature Schedule\n",
    "print(\"\\n[1] Temperature Schedule Test\")\n",
    "temp_schedule = (1.0, 1.5, 2.0)\n",
    "total = 5000\n",
    "\n",
    "t_start = get_pocl_temperature(0, total, temp_schedule)\n",
    "t_stage1_end = get_pocl_temperature(1666, total, temp_schedule)  # End of stage 1\n",
    "t_stage2_start = get_pocl_temperature(1667, total, temp_schedule)  # Start of stage 2\n",
    "t_stage2_end = get_pocl_temperature(3332, total, temp_schedule)\n",
    "t_stage3 = get_pocl_temperature(4000, total, temp_schedule)\n",
    "\n",
    "temp_pass = (t_start == 1.0 and t_stage1_end == 1.0 and t_stage2_start == 1.5 and t_stage3 == 2.0)\n",
    "print(f\"  T(0) = {t_start} (should be 1.0)\")\n",
    "print(f\"  T(1666) = {t_stage1_end} (should be 1.0, end of stage 1)\")\n",
    "print(f\"  T(1667) = {t_stage2_start} (should be 1.5, start of stage 2)\")\n",
    "print(f\"  T(4000) = {t_stage3} (should be 2.0, stage 3)\")\n",
    "print(f\"  {'PASS' if temp_pass else 'FAIL'}\")\n",
    "\n",
    "# Test 2: Stage Boundaries\n",
    "print(\"\\n[2] Stage Boundaries Test\")\n",
    "stages = [get_pocl_stage(s, total) for s in [0, 1666, 1667, 3332, 3333, 4999]]\n",
    "stage_pass = stages == [0, 0, 1, 1, 2, 2]\n",
    "print(f\"  Stages at [0, 1666, 1667, 3332, 3333, 4999]: {stages}\")\n",
    "print(f\"  Expected: [0, 0, 1, 1, 2, 2]\")\n",
    "print(f\"  {'PASS' if stage_pass else 'FAIL'}\")\n",
    "\n",
    "# Test 3: Partition by Difficulty (mock)\n",
    "print(\"\\n[3] Partition by Difficulty Test (mock data)\")\n",
    "mock_difficulties = {\n",
    "    'indices': list(range(9)),\n",
    "    'difficulties': [0.5, 1.5, 0.3, 2.0, 0.8, 1.2, 2.5, 0.1, 1.8],  # Easy: 7,2,0,4 | Med: 5,1 | Hard: 8,3,6\n",
    "    'num_samples': 9\n",
    "}\n",
    "partitions = partition_by_difficulty(mock_difficulties, n_stages=3)\n",
    "# After sorting: [7(0.1), 2(0.3), 0(0.5), 4(0.8), 5(1.2), 1(1.5), 8(1.8), 3(2.0), 6(2.5)]\n",
    "# Stage 1 (33%): indices [7, 2, 0] -> 3 samples\n",
    "# Stage 2 (66%): indices [7, 2, 0, 4, 5, 1] -> 6 samples\n",
    "# Stage 3 (100%): all 9 samples\n",
    "partition_pass = (len(partitions[0]) == 3 and len(partitions[1]) == 6 and len(partitions[2]) == 9)\n",
    "print(f\"  Stage 1 samples: {len(partitions[0])} (should be 3)\")\n",
    "print(f\"  Stage 2 samples: {len(partitions[1])} (should be 6)\")\n",
    "print(f\"  Stage 3 samples: {len(partitions[2])} (should be 9)\")\n",
    "print(f\"  Cumulative check: {partitions[0][0] in partitions[1] and partitions[1][0] in partitions[2]}\")\n",
    "print(f\"  {'PASS' if partition_pass else 'FAIL'}\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "all_pass = temp_pass and stage_pass and partition_pass\n",
    "print(f\"POCL Component Tests: {'ALL PASS' if all_pass else 'SOME FAILED'}\")\n",
    "if not all_pass:\n",
    "    print(\"WARNING: Fix failing tests before running training!\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# cell 7: hardware and spike stats collectors (same as v9)\n",
    "# =============================================================================\n",
    "class HardwareStatsCollector:\n",
    "    \"\"\"collect gpu memory, timing, and throughput metrics.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.gpu_memory_history = []\n",
    "        self.step_times = []\n",
    "        self.tokens_processed = 0\n",
    "        self.start_time = None\n",
    "\n",
    "    def start(self):\n",
    "        self.start_time = time.time()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "    def record_step(self, batch_size: int, seq_len: int):\n",
    "        if torch.cuda.is_available():\n",
    "            self.gpu_memory_history.append(torch.cuda.memory_allocated() / 1e9)\n",
    "        self.tokens_processed += batch_size * seq_len\n",
    "        self.step_times.append(time.time())\n",
    "\n",
    "    def get_throughput(self) -> float:\n",
    "        if len(self.step_times) < 2:\n",
    "            return 0.0\n",
    "        elapsed = self.step_times[-1] - self.step_times[0]\n",
    "        return self.tokens_processed / elapsed if elapsed > 0 else 0.0\n",
    "\n",
    "    def get_summary(self) -> Dict[str, Any]:\n",
    "        elapsed = time.time() - self.start_time if self.start_time else 0\n",
    "        return {\n",
    "            'peak_gpu_memory_gb': max(self.gpu_memory_history) if self.gpu_memory_history else 0,\n",
    "            'avg_gpu_memory_gb': float(np.mean(self.gpu_memory_history)) if self.gpu_memory_history else 0,\n",
    "            'total_training_time_s': elapsed,\n",
    "            'total_training_time_min': elapsed / 60,\n",
    "            'tokens_processed': self.tokens_processed,\n",
    "            'throughput_tokens_per_sec': self.get_throughput(),\n",
    "        }\n",
    "\n",
    "\n",
    "class SpikeStatsCollector:\n",
    "    \"\"\"collect per-layer spike density and amplitude evolution.\"\"\"\n",
    "\n",
    "    def __init__(self, n_layers: int):\n",
    "        self.n_layers = n_layers\n",
    "        self.density_history = {i: {'k': [], 'v': []} for i in range(n_layers)}\n",
    "        self.amplitude_history = {i: {'k': [], 'v': []} for i in range(n_layers)}\n",
    "        self.step_densities = []\n",
    "\n",
    "    def record(self, student, step: int):\n",
    "        stats = student.get_spike_stats()\n",
    "        all_densities = []\n",
    "        for i in range(self.n_layers):\n",
    "            layer_key = f'layer_{i}'\n",
    "            if layer_key in stats:\n",
    "                k_density = stats[layer_key].get('k', 0)\n",
    "                v_density = stats[layer_key].get('v', 0)\n",
    "                k_amp = stats[layer_key].get('k_amp', 1.0)\n",
    "                v_amp = stats[layer_key].get('v_amp', 1.0)\n",
    "\n",
    "                self.density_history[i]['k'].append(k_density)\n",
    "                self.density_history[i]['v'].append(v_density)\n",
    "                self.amplitude_history[i]['k'].append(k_amp)\n",
    "                self.amplitude_history[i]['v'].append(v_amp)\n",
    "                all_densities.extend([k_density, v_density])\n",
    "\n",
    "        if all_densities:\n",
    "            self.step_densities.append({'step': step, 'density': float(np.mean(all_densities))})\n",
    "\n",
    "    def get_summary(self) -> Dict[str, Any]:\n",
    "        per_layer = {}\n",
    "        all_k, all_v = [], []\n",
    "        all_k_amp, all_v_amp = [], []\n",
    "\n",
    "        for i in range(self.n_layers):\n",
    "            k_vals = self.density_history[i]['k']\n",
    "            v_vals = self.density_history[i]['v']\n",
    "            k_amps = self.amplitude_history[i]['k']\n",
    "            v_amps = self.amplitude_history[i]['v']\n",
    "\n",
    "            per_layer[f'layer_{i}'] = {\n",
    "                'k_mean': float(np.mean(k_vals)) if k_vals else 0,\n",
    "                'k_std': float(np.std(k_vals)) if k_vals else 0,\n",
    "                'k_final': float(k_vals[-1]) if k_vals else 0,\n",
    "                'v_mean': float(np.mean(v_vals)) if v_vals else 0,\n",
    "                'v_std': float(np.std(v_vals)) if v_vals else 0,\n",
    "                'v_final': float(v_vals[-1]) if v_vals else 0,\n",
    "                'k_amp_final': float(k_amps[-1]) if k_amps else 1.0,\n",
    "                'v_amp_final': float(v_amps[-1]) if v_amps else 1.0,\n",
    "            }\n",
    "            all_k.extend(k_vals)\n",
    "            all_v.extend(v_vals)\n",
    "            if k_amps: all_k_amp.append(k_amps[-1])\n",
    "            if v_amps: all_v_amp.append(v_amps[-1])\n",
    "\n",
    "        return {\n",
    "            'per_layer': per_layer,\n",
    "            'overall_k_density': float(np.mean(all_k)) if all_k else 0,\n",
    "            'overall_v_density': float(np.mean(all_v)) if all_v else 0,\n",
    "            'overall_density': float(np.mean(all_k + all_v)) if (all_k or all_v) else 0,\n",
    "            'amplitudes': {'k': all_k_amp, 'v': all_v_amp},\n",
    "            'density_history': self.step_densities,\n",
    "        }\n",
    "\n",
    "print(\"collectors defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# cell 8: spiking goose model (v14 - channel-wise spikes + gradient checkpointing)\n",
    "# =============================================================================\n",
    "class SpikingGooseRecurrentLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    RWKV-style recurrence with trainable ternary spiking.\n",
    "    \n",
    "    Supports channel-wise ternary spikes (when use_channel_wise=True)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, layer_idx=0, n_layers=4, spike_alpha=1.0,\n",
    "                 use_channel_wise: bool = False, threshold_mix: float = 0.35,\n",
    "                 surrogate_temp: float = 0.10):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.layer_idx = layer_idx\n",
    "        self.use_channel_wise = use_channel_wise\n",
    "        self.ln = nn.LayerNorm(d_model)\n",
    "\n",
    "        ratio = layer_idx / max(n_layers - 1, 1)\n",
    "        self.time_mix_k = nn.Parameter(torch.ones(d_model) * (1 - ratio))\n",
    "        self.time_mix_v = nn.Parameter(torch.ones(d_model) * (1 - ratio))\n",
    "        self.time_mix_r = nn.Parameter(torch.ones(d_model) * (1 - ratio))\n",
    "        self.decay_weight = nn.Parameter(torch.zeros(d_model) - 0.5)\n",
    "\n",
    "        self.key_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.value_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.receptance_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.output_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "        # v14: Use channel-wise spikes if enabled\n",
    "        if use_channel_wise:\n",
    "            self.k_spike = ChannelWiseTernarySpike(d_model, alpha_init=spike_alpha)\n",
    "            self.v_spike = ChannelWiseTernarySpike(d_model, alpha_init=spike_alpha)\n",
    "        else:\n",
    "            self.k_spike = TrainableTernarySpike(\n",
    "                alpha=spike_alpha,\n",
    "                threshold_mix=threshold_mix,\n",
    "                surrogate_temp=surrogate_temp,\n",
    "            )\n",
    "            self.v_spike = TrainableTernarySpike(\n",
    "                alpha=spike_alpha,\n",
    "                threshold_mix=threshold_mix,\n",
    "                surrogate_temp=surrogate_temp,\n",
    "            )\n",
    "\n",
    "        self.register_buffer('running_k_density', torch.tensor(0.0))\n",
    "        self.register_buffer('running_v_density', torch.tensor(0.0))\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        std = 0.1 / math.sqrt(self.d_model)\n",
    "        for m in [self.key_proj, self.value_proj, self.receptance_proj, self.output_proj]:\n",
    "            nn.init.normal_(m.weight, std=std)\n",
    "\n",
    "    def forward(self, x, return_spikes: bool = False, detach_spikes: bool = True):\n",
    "        B, T, D = x.shape\n",
    "        x_norm = self.ln(x)\n",
    "        prev_x = F.pad(x_norm[:, :-1, :], (0, 0, 1, 0))\n",
    "\n",
    "        xk = x_norm * self.time_mix_k + prev_x * (1 - self.time_mix_k)\n",
    "        xv = x_norm * self.time_mix_v + prev_x * (1 - self.time_mix_v)\n",
    "        xr = x_norm * self.time_mix_r + prev_x * (1 - self.time_mix_r)\n",
    "\n",
    "        k_pre = self.key_proj(xk)\n",
    "        v_pre = self.value_proj(xv)\n",
    "\n",
    "        k_aux = {}\n",
    "        v_aux = {}\n",
    "        if return_spikes:\n",
    "            k, k_aux = self.k_spike(k_pre, return_aux=True)\n",
    "            v, v_aux = self.v_spike(v_pre, return_aux=True)\n",
    "        else:\n",
    "            k = self.k_spike(k_pre)\n",
    "            v = self.v_spike(v_pre)\n",
    "        r = torch.sigmoid(self.receptance_proj(xr))\n",
    "\n",
    "        kv = k * v\n",
    "        decay = torch.sigmoid(self.decay_weight)\n",
    "        t_idx = torch.arange(T, device=x.device, dtype=x.dtype)\n",
    "        decay_powers = decay.unsqueeze(0) ** t_idx.unsqueeze(1)\n",
    "\n",
    "        kv_weighted = kv / (decay_powers.unsqueeze(0) + 1e-8)\n",
    "        S = torch.cumsum(kv_weighted, dim=1) * decay_powers.unsqueeze(0)\n",
    "\n",
    "        if self.training:\n",
    "            with torch.no_grad():\n",
    "                self.running_k_density = 0.99 * self.running_k_density + 0.01 * (k != 0).float().mean()\n",
    "                self.running_v_density = 0.99 * self.running_v_density + 0.01 * (v != 0).float().mean()\n",
    "\n",
    "        out = x + r * self.output_proj(S)\n",
    "        if return_spikes:\n",
    "            k_out = k.detach() if detach_spikes else k\n",
    "            v_out = v.detach() if detach_spikes else v\n",
    "            return out, {\n",
    "                'k_spikes': k_out,\n",
    "                'v_spikes': v_out,\n",
    "                'k_soft_activity': k_aux.get('soft_activity'),\n",
    "                'v_soft_activity': v_aux.get('soft_activity'),\n",
    "            }\n",
    "        return out\n",
    "\n",
    "    def get_spike_density(self):\n",
    "        return {\n",
    "            'k': self.running_k_density.item(),\n",
    "            'v': self.running_v_density.item(),\n",
    "            'k_amp': self.k_spike.get_amplitude(),\n",
    "            'v_amp': self.v_spike.get_amplitude(),\n",
    "        }\n",
    "    \n",
    "    def get_channel_wise_stats(self) -> dict:\n",
    "        \"\"\"Get channel-wise spike statistics (only available if use_channel_wise=True).\"\"\"\n",
    "        if self.use_channel_wise:\n",
    "            return {\n",
    "                'k': self.k_spike.get_stats(),\n",
    "                'v': self.v_spike.get_stats(),\n",
    "            }\n",
    "        return None\n",
    "\n",
    "\n",
    "class GooseFFN(nn.Module):\n",
    "    def __init__(self, d_model, expand=4):\n",
    "        super().__init__()\n",
    "        self.ln = nn.LayerNorm(d_model)\n",
    "        self.w1 = nn.Linear(d_model, d_model * expand, bias=False)\n",
    "        self.w2 = nn.Linear(d_model * expand, d_model, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.w2(F.silu(self.w1(self.ln(x))))\n",
    "\n",
    "\n",
    "class StudentSpikingGoose(nn.Module):\n",
    "    \"\"\"\n",
    "    Spiking student model with trainable ternary activations.\n",
    "    \n",
    "    Supports channel-wise ternary spikes + gradient checkpointing.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cfg, use_checkpointing=True):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.use_checkpointing = use_checkpointing and USE_GRADIENT_CHECKPOINTING\n",
    "        \n",
    "        # v14: Check for channel-wise spikes flag\n",
    "        use_channel_wise = getattr(cfg, 'use_channel_wise_spikes', False)\n",
    "        \n",
    "        self.embed = nn.Embedding(cfg.vocab_size, cfg.d_model)\n",
    "        self.pos_embed = nn.Embedding(cfg.max_seq_len, cfg.d_model)\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.ModuleDict({\n",
    "                'rec': SpikingGooseRecurrentLayer(\n",
    "                    cfg.d_model, i, cfg.n_layers, cfg.spike_alpha,\n",
    "                    use_channel_wise=use_channel_wise,\n",
    "                    threshold_mix=cfg.spike_threshold_mix,\n",
    "                    surrogate_temp=cfg.spike_surrogate_temp,\n",
    "                ),\n",
    "                'ffn': GooseFFN(cfg.d_model),\n",
    "            })\n",
    "            for i in range(cfg.n_layers)\n",
    "        ])\n",
    "\n",
    "        self.ln_out = nn.LayerNorm(cfg.d_model)\n",
    "        self.head = nn.Linear(cfg.d_model, cfg.vocab_size, bias=False)\n",
    "        self.head.weight = self.embed.weight\n",
    "\n",
    "        nn.init.normal_(self.embed.weight, std=0.02)\n",
    "        nn.init.normal_(self.pos_embed.weight, std=0.02)\n",
    "\n",
    "    def _layer_forward(self, layer, x):\n",
    "        \"\"\"helper for gradient checkpointing - processes one layer.\"\"\"\n",
    "        x = layer['rec'](x)\n",
    "        x = layer['ffn'](x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, input_ids, return_hiddens=False, return_spike_info=False, detach_spikes: bool = True):\n",
    "        \"\"\"forward pass with optional hidden state return for alignment.\"\"\"\n",
    "        B, T = input_ids.shape\n",
    "        pos = torch.arange(T, device=input_ids.device).unsqueeze(0)\n",
    "        x = self.embed(input_ids) + self.pos_embed(pos)\n",
    "\n",
    "        hiddens = [x] if return_hiddens else None\n",
    "        spike_info = {} if return_spike_info else None\n",
    "\n",
    "        for layer_idx, layer in enumerate(self.layers):\n",
    "            # Checkpoint path is tensor-only; skip it when spike tensors are requested.\n",
    "            if self.use_checkpointing and self.training and not return_spike_info:\n",
    "                x = checkpoint(self._layer_forward, layer, x, use_reentrant=False)\n",
    "            else:\n",
    "                if return_spike_info:\n",
    "                    x, layer_spikes = layer['rec'](\n",
    "                        x,\n",
    "                        return_spikes=True,\n",
    "                        detach_spikes=detach_spikes,\n",
    "                    )\n",
    "                    x = layer['ffn'](x)\n",
    "                    spike_info[layer_idx] = layer_spikes\n",
    "                else:\n",
    "                    x = self._layer_forward(layer, x)\n",
    "\n",
    "            if return_hiddens:\n",
    "                hiddens.append(x)\n",
    "\n",
    "        logits = self.head(self.ln_out(x))\n",
    "\n",
    "        if return_hiddens and return_spike_info:\n",
    "            return logits, hiddens, {'spike_info': spike_info}\n",
    "        if return_hiddens:\n",
    "            return logits, hiddens\n",
    "        if return_spike_info:\n",
    "            return logits, {'spike_info': spike_info}\n",
    "        return logits\n",
    "\n",
    "    def get_spike_stats(self):\n",
    "        return {f'layer_{i}': layer['rec'].get_spike_density() for i, layer in enumerate(self.layers)}\n",
    "\n",
    "    def get_avg_spike_density(self):\n",
    "        densities = []\n",
    "        for layer in self.layers:\n",
    "            d = layer['rec'].get_spike_density()\n",
    "            densities.extend([d['k'], d['v']])\n",
    "        return float(np.mean(densities)) if densities else 0.0\n",
    "\n",
    "    def get_amplitudes(self):\n",
    "        return {f'layer_{i}': {'k': layer['rec'].k_spike.get_amplitude(), 'v': layer['rec'].v_spike.get_amplitude()}\n",
    "                for i, layer in enumerate(self.layers)}\n",
    "    \n",
    "    def get_channel_amplitude_variance(self) -> float:\n",
    "        \"\"\"Get total variance of channel-wise amplitudes (for regularization).\"\"\"\n",
    "        total_var = 0.0\n",
    "        for layer in self.layers:\n",
    "            rec = layer['rec']\n",
    "            if hasattr(rec.k_spike, 'amplitude') and rec.k_spike.amplitude.numel() > 1:\n",
    "                total_var += rec.k_spike.amplitude.var().item()\n",
    "                total_var += rec.v_spike.amplitude.var().item()\n",
    "        return total_var\n",
    "\n",
    "print(\"student model defined (v14: channel-wise spikes + gradient checkpointing)\")\n",
    "print(f\"  gradient checkpointing: {USE_GRADIENT_CHECKPOINTING}\")\n",
    "print(f\"  channel-wise spikes: {config.use_channel_wise_spikes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# cell 9: hidden-state projector + v14.1 FDD layer mapping\n",
    "# =============================================================================\n",
    "class HiddenStateProjector(nn.Module):\n",
    "    \"\"\"\n",
    "    Project student hidden states to teacher dimension for alignment.\n",
    "\n",
    "    student: (B, T, 320) -> (B, T, 768)\n",
    "\n",
    "    Maps student layers to selected teacher layers.\n",
    "\n",
    "    NOTE: This is kept for infrastructure but hidden alignment is DISABLED.\n",
    "    v14 uses FDD with CKA loss which is projector-free.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, student_dim: int, teacher_dim: int, n_student_layers: int):\n",
    "        super().__init__()\n",
    "        self.projectors = nn.ModuleList([\n",
    "            nn.Linear(student_dim, teacher_dim, bias=False)\n",
    "            for _ in range(n_student_layers)\n",
    "        ])\n",
    "        for proj in self.projectors:\n",
    "            nn.init.normal_(proj.weight, std=0.02)\n",
    "\n",
    "    def forward(self, student_hidden: torch.Tensor, layer_idx: int) -> torch.Tensor:\n",
    "        return self.projectors[layer_idx](student_hidden)\n",
    "\n",
    "\n",
    "def compute_hidden_alignment_loss(\n",
    "    teacher_hiddens: List[torch.Tensor],\n",
    "    student_hiddens: List[torch.Tensor],\n",
    "    projector: HiddenStateProjector,\n",
    "    teacher_layers: int = 12,\n",
    "    student_layers: int = 8\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute MSE loss between projected student and teacher hidden states.\n",
    "\n",
    "    NOTE: This is DISABLED in v14 (hidden_align_weight=0.0).\n",
    "    v14 uses FDD with CKA loss instead.\n",
    "    \"\"\"\n",
    "    # Map student layers to teacher layers\n",
    "    teacher_indices = [1, 2, 4, 5, 7, 8, 10, 11]\n",
    "\n",
    "    total_loss = 0.0\n",
    "    for s_idx, t_idx in enumerate(teacher_indices):\n",
    "        if s_idx >= len(student_hiddens) - 1:\n",
    "            break\n",
    "        if t_idx >= len(teacher_hiddens):\n",
    "            break\n",
    "\n",
    "        s_hidden = student_hiddens[s_idx + 1]\n",
    "        t_hidden = teacher_hiddens[t_idx]\n",
    "\n",
    "        s_proj = projector(s_hidden, s_idx)\n",
    "        total_loss = total_loss + F.mse_loss(s_proj, t_hidden)\n",
    "\n",
    "    return total_loss / len(teacher_indices)\n",
    "\n",
    "\n",
    "# Create projector (even if disabled, keeps infrastructure)\n",
    "projector = HiddenStateProjector(\n",
    "    student_dim=config.d_model,\n",
    "    teacher_dim=config.teacher_d_model,\n",
    "    n_student_layers=config.n_layers\n",
    ").to(DEVICE)\n",
    "\n",
    "projector_params = sum(p.numel() for p in projector.parameters())\n",
    "print(f\"hidden-state projector: {projector_params:,} params\")\n",
    "print(f\"  student dim: {config.d_model}\")\n",
    "print(f\"  teacher dim: {config.teacher_d_model}\")\n",
    "print(f\"  student layers: {config.n_layers}\")\n",
    "print(f\"  hidden_align_weight: {config.hidden_align_weight}\")\n",
    "print(f\"  STATUS: DISABLED (v14 uses FDD with CKA instead)\")\n",
    "\n",
    "# =============================================================================\n",
    "# v14: Create FDD layer mapping\n",
    "# =============================================================================\n",
    "fdd_layer_map = get_fdd_layer_mapping(\n",
    "    n_student_layers=config.n_layers,\n",
    "    n_teacher_layers=config.teacher_n_layers,\n",
    "    n_align_layers=config.fdd_n_align_layers\n",
    ")\n",
    "\n",
    "print(f\"\")\n",
    "print(f\"{config.VERSION} FDD Layer Mapping:\")\n",
    "print(f\"  Layer pairs to align: {config.fdd_n_align_layers}\")\n",
    "print(f\"  Mapping: {fdd_layer_map}\")\n",
    "print(f\"  Strategy: Align early/middle/late semantic layers\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# cell 10: cosine lr with warmup (same as v9)\n",
    "# =============================================================================\n",
    "def get_cosine_schedule_with_warmup(\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    warmup_steps: int,\n",
    "    total_steps: int,\n",
    ") -> torch.optim.lr_scheduler.LambdaLR:\n",
    "    \"\"\"\n",
    "    linear warmup then cosine decay to 0.\n",
    "    \"\"\"\n",
    "    def lr_lambda(step: int) -> float:\n",
    "        if step < warmup_steps:\n",
    "            return step / max(warmup_steps, 1)\n",
    "        else:\n",
    "            progress = (step - warmup_steps) / max(total_steps - warmup_steps, 1)\n",
    "            return 0.5 * (1.0 + math.cos(math.pi * progress))\n",
    "\n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "\n",
    "print(f\"cosine lr: {config.warmup_steps} warmup, {config.distill_steps} total\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# cell 11: load gpt-2 teacher (same as v9)\n",
    "# =============================================================================\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "print(\"loading gpt-2 teacher...\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "teacher = GPT2LMHeadModel.from_pretrained('gpt2').to(DEVICE)\n",
    "teacher.config.use_cache = False  # Disable KV caching (not needed for distillation)\n",
    "\n",
    "# Compile teacher for faster inference (PyTorch 2.0+)\n",
    "try:\n",
    "    teacher = torch.compile(teacher, mode='reduce-overhead')\n",
    "    print('teacher compiled with torch.compile')\n",
    "except Exception as e:\n",
    "    print(f'torch.compile not available: {e}')\n",
    "teacher.eval()\n",
    "for p in teacher.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "teacher_params = sum(p.numel() for p in teacher.parameters())\n",
    "print(f\"teacher: gpt-2 ({teacher_params:,} params)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# cell 13: data loading (v14 - efficient DataLoader)\n",
    "# =============================================================================\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"loading wikitext-2...\")\n",
    "dataset = load_dataset('wikitext', 'wikitext-2-raw-v1')\n",
    "\n",
    "def pre_tokenize(texts, max_len):\n",
    "    all_tokens = []\n",
    "    for text in tqdm(texts, desc=\"tokenizing\", leave=False):\n",
    "        if text.strip():\n",
    "            all_tokens.extend(tokenizer.encode(text, max_length=max_len*2, truncation=True))\n",
    "    chunks = [all_tokens[i:i+max_len] for i in range(0, len(all_tokens)-max_len+1, max_len//2) if len(all_tokens[i:i+max_len]) == max_len]\n",
    "    print(f\"created {len(chunks)} sequences\")\n",
    "    return torch.tensor(chunks, dtype=torch.long)\n",
    "\n",
    "train_tokens = pre_tokenize(dataset['train']['text'], config.max_seq_len)\n",
    "val_tokens = pre_tokenize(dataset['validation']['text'], config.max_seq_len)\n",
    "\n",
    "# v14: efficient DataLoader with workers and prefetch\n",
    "# Note: num_workers=0 for Kaggle/Colab compatibility, but prefetch still helps\n",
    "dataloader_kwargs = {\n",
    "    'batch_size': config.batch_size,\n",
    "    'pin_memory': True,\n",
    "    'num_workers': 0 if IS_KAGGLE or IS_COLAB else 2,  # workers disabled on cloud platforms\n",
    "    'prefetch_factor': None if IS_KAGGLE or IS_COLAB else 2,\n",
    "    'persistent_workers': False if IS_KAGGLE or IS_COLAB else True,\n",
    "}\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(train_tokens), shuffle=True, **dataloader_kwargs)\n",
    "val_loader = DataLoader(TensorDataset(val_tokens), shuffle=False, **dataloader_kwargs)\n",
    "\n",
    "print(f\"train: {len(train_loader)} batches, val: {len(val_loader)} batches\")\n",
    "print(f\"DataLoader: num_workers={dataloader_kwargs['num_workers']}, pin_memory={dataloader_kwargs['pin_memory']}\")\n",
    "if len(train_loader) == 0:\n",
    "    raise RuntimeError(\n",
    "        \"train_loader is empty after tokenization. \"\n",
    "        \"Check dataset availability and max_seq_len/chunking settings.\"\n",
    "    )\n",
    "if len(val_loader) == 0:\n",
    "    raise RuntimeError(\n",
    "        \"val_loader is empty after tokenization. \"\n",
    "        \"Check dataset availability and max_seq_len/chunking settings.\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# cell 14: create student model and projector (v14 - with compile)\n",
    "# =============================================================================\n",
    "print(\"creating student model (v14 - v14 baseline + POCL)...\")\n",
    "\n",
    "student = StudentSpikingGoose(config, use_checkpointing=USE_GRADIENT_CHECKPOINTING).to(DEVICE)\n",
    "student_params = sum(p.numel() for p in student.parameters())\n",
    "\n",
    "# v14: create projector (even if not used, for infrastructure preservation)\n",
    "projector = HiddenStateProjector(\n",
    "    student_dim=config.d_model,\n",
    "    teacher_dim=config.teacher_d_model,\n",
    "    n_student_layers=config.n_layers\n",
    ").to(DEVICE)\n",
    "projector_params = sum(p.numel() for p in projector.parameters())\n",
    "\n",
    "compression_ratio = teacher_params / student_params\n",
    "\n",
    "print(f\"student: asnn-goose v14 ({student_params:,} params)\")\n",
    "print(f\"projector: ({projector_params:,} params)\")\n",
    "print(f\"compression ratio: {compression_ratio:.1f}x\")\n",
    "print(f\"\")\n",
    "print(f\"{config.VERSION} architecture:\")\n",
    "print(f\"  d_model: {config.d_model}\")\n",
    "print(f\"  n_layers: {config.n_layers}\")\n",
    "print(f\"  params: ~{student_params // 1_000_000}M\")\n",
    "print(f\"\")\n",
    "\n",
    "# v14: compile model if available and enabled\n",
    "compile_success = False\n",
    "if USE_TORCH_COMPILE and TORCH_COMPILE_AVAILABLE:\n",
    "    try:\n",
    "        print(\"compiling student model with torch.compile...\")\n",
    "        # Use the compile() method as recommended by PyTorch docs\n",
    "        student = torch.compile(student, mode='reduce-overhead')\n",
    "        compile_success = True\n",
    "        print(\"compilation successful!\")\n",
    "    except Exception as e:\n",
    "        print(f\"torch.compile failed: {e}\")\n",
    "        print(\"continuing without compilation\")\n",
    "else:\n",
    "    print(f\"torch.compile skipped (USE_TORCH_COMPILE={USE_TORCH_COMPILE}, available={TORCH_COMPILE_AVAILABLE})\")\n",
    "\n",
    "print(f\"\")\n",
    "print(f\"speedups active:\")\n",
    "print(f\"  gradient checkpointing: {USE_GRADIENT_CHECKPOINTING}\")\n",
    "print(f\"  torch.compile: {compile_success}\")\n",
    "print(f\"  accumulation_steps: {config.accumulation_steps}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# cell 15: evaluation functions (same as v9)\n",
    "# =============================================================================\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, device, is_gpt2=False):\n",
    "    model.eval()\n",
    "    total_loss, total_tokens = 0, 0\n",
    "    with torch.inference_mode():\n",
    "      for batch in loader:\n",
    "        ids = batch[0].to(device, non_blocking=True)\n",
    "        with torch.cuda.amp.autocast():\n",
    "            logits = model(ids).logits if is_gpt2 else model(ids)\n",
    "        loss = F.cross_entropy(logits[:, :-1].reshape(-1, logits.size(-1)), ids[:, 1:].reshape(-1), reduction='sum')\n",
    "        total_loss += loss.item()\n",
    "        total_tokens += ids[:, 1:].numel()\n",
    "    if total_tokens == 0:\n",
    "        raise RuntimeError(\n",
    "            \"Evaluation loader produced zero tokens; cannot compute loss/PPL.\"\n",
    "        )\n",
    "    return total_loss / total_tokens\n",
    "\n",
    "def get_ppl(loss):\n",
    "    return math.exp(min(loss, 10))\n",
    "\n",
    "print(\"evaluation functions defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# cell 17: distillation training loop (v14.1.1 - FDD+CTKD+HardCE)\n",
    "# =============================================================================\n",
    "def get_spike_semantic_weight(step: int, warmup_steps: int, max_weight: float) -> float:\n",
    "    if step < warmup_steps:\n",
    "        return 0.0\n",
    "    ramp_steps = max(warmup_steps, 1)\n",
    "    ramp = min(1.0, (step - warmup_steps) / ramp_steps)\n",
    "    return max_weight * ramp\n",
    "\n",
    "\n",
    "def build_teacher_ternary_target(teacher_hidden: torch.Tensor, threshold_scale: float) -> torch.Tensor:\n",
    "    centered = teacher_hidden - teacher_hidden.mean(dim=-1, keepdim=True)\n",
    "    threshold = threshold_scale * centered.abs().mean(dim=-1, keepdim=True)\n",
    "    pos = centered > threshold\n",
    "    neg = centered < -threshold\n",
    "    target = torch.zeros_like(centered)\n",
    "    target = torch.where(pos, torch.ones_like(target), target)\n",
    "    target = torch.where(neg, -torch.ones_like(target), target)\n",
    "    return target\n",
    "\n",
    "\n",
    "def distill_v14(teacher, student, projector, train_loader, val_loader, cfg, device,\n",
    "                hw_stats, spike_stats, fdd_layer_map):\n",
    "    \"\"\"\n",
    "    v14 distillation with FDD (Feature Dynamics Distillation) + CTKD.\n",
    "\n",
    "    Key innovations:\n",
    "    1. FDD: Align layer dynamics (delta_h) using CKA loss\n",
    "    2. CTKD: Adversarial temperature learning (proven in v12.1, v13.1)\n",
    "    3. Safety: FDD kill-switch if PPL regresses\n",
    "\n",
    "    References:\n",
    "    - CKA: Kornblith et al., \"Similarity of Neural Network Representations\"\n",
    "    - CTKD: https://arxiv.org/abs/2211.16231\n",
    "    - FDD: Feature Dynamics Distillation (view transformer as ODE)\n",
    "    \"\"\"\n",
    "    training_logs = {\n",
    "        'loss_history': [],\n",
    "        'kl_loss_history': [],\n",
    "        'ce_loss_history': [],  # v14.1: hard distillation\n",
    "        'fdd_loss_history': [],\n",
    "        'spike_sem_loss_history': [],\n",
    "        'align_loss_history': [],\n",
    "        'ppl_history': [],\n",
    "        'lr_history': [],\n",
    "        'temp_history': [],\n",
    "        'lambda_history': [],\n",
    "        'fdd_weight_history': [],\n",
    "        'spike_sem_weight_history': [],\n",
    "        'stage_history': [],\n",
    "        'stage_transitions': [],\n",
    "        'early_stopped': False,\n",
    "        'early_stop_step': None,\n",
    "        'fdd_killed': False,\n",
    "        'fdd_kill_step': None,\n",
    "    }\n",
    "\n",
    "    # =========================================================================\n",
    "    # v14: CTKD Temperature (same as v12.1, v13.1)\n",
    "    # =========================================================================\n",
    "    if cfg.use_ctkd:\n",
    "        temp_module = CTKDTemperature(\n",
    "            tau_min=cfg.tau_min,\n",
    "            tau_max=cfg.tau_max,\n",
    "            init=cfg.tau_init\n",
    "        ).to(device)\n",
    "        print(f\"{config.VERSION}: CTKD with Gradient Reversal Layer\")\n",
    "        print(f\"     Temperature bounds: [{cfg.tau_min}, {cfg.tau_max}]\")\n",
    "        print(f\"     Initial temp: {cfg.tau_init}\")\n",
    "        print(f\"     Lambda warmup: {cfg.lambda_warmup_ratio*100:.0f}%\")\n",
    "    else:\n",
    "        temp_module = None\n",
    "        print(f\"Using fixed temperature: {cfg.temperature}\")\n",
    "\n",
    "    # =========================================================================\n",
    "    # v14: FDD Setup\n",
    "    # =========================================================================\n",
    "    fdd_enabled = cfg.use_fdd\n",
    "    fdd_killed = False\n",
    "    baseline_ppl = None  # Set at fdd_warmup_steps\n",
    "\n",
    "    if cfg.use_fdd:\n",
    "        print(f\"\")\n",
    "        print(f\"{config.VERSION}: Feature Dynamics Distillation (FDD)\")\n",
    "        print(f\"     Layer mapping: {fdd_layer_map}\")\n",
    "        print(f\"     Weight: {cfg.fdd_weight}\")\n",
    "        print(f\"     Warmup: {cfg.fdd_warmup_steps} steps\")\n",
    "        print(f\"     Loss type: {cfg.fdd_loss_type}\")\n",
    "        print(f\"     Kill threshold: {cfg.fdd_kill_threshold*100:.0f}% PPL increase\")\n",
    "\n",
    "    if cfg.use_spike_semantic_loss:\n",
    "        print(f\"\")\n",
    "        print(f\"{config.VERSION}: Spike Semantic Alignment\")\n",
    "        print(f\"     Weight: {cfg.spike_semantic_weight}\")\n",
    "        print(f\"     Warmup: {cfg.spike_semantic_warmup_steps} steps\")\n",
    "        print(f\"     Target threshold scale: {cfg.spike_target_threshold_scale}\")\n",
    "\n",
    "    # =========================================================================\n",
    "    # v14: Early Stopping Setup\n",
    "    # =========================================================================\n",
    "    best_ppl = float('inf')\n",
    "    best_step = 0\n",
    "    no_improve_steps = 0\n",
    "\n",
    "    if cfg.use_early_stopping:\n",
    "        print(f\"\")\n",
    "        print(f\"{config.VERSION}: Early Stopping\")\n",
    "        print(f\"     Patience: {cfg.early_stopping_patience} steps\")\n",
    "        print(f\"     Min delta: {cfg.min_ppl_delta} PPL\")\n",
    "\n",
    "    # =========================================================================\n",
    "    # Setup optimizer\n",
    "    # =========================================================================\n",
    "    param_groups = [\n",
    "        {'params': list(student.parameters()), 'lr': cfg.distill_lr}\n",
    "    ]\n",
    "\n",
    "    if cfg.hidden_align_weight > 0:\n",
    "        param_groups.append({'params': list(projector.parameters()), 'lr': cfg.distill_lr})\n",
    "\n",
    "    if temp_module is not None:\n",
    "        param_groups.append({'params': list(temp_module.parameters()), 'lr': cfg.distill_lr})\n",
    "\n",
    "    all_params = []\n",
    "    for group in param_groups:\n",
    "        all_params.extend(group['params'])\n",
    "\n",
    "    try:\n",
    "        optimizer = torch.optim.AdamW(param_groups, weight_decay=0.01, fused=True)\n",
    "        print(\"Using fused AdamW\")\n",
    "    except TypeError:\n",
    "        optimizer = torch.optim.AdamW(param_groups, weight_decay=0.01)\n",
    "\n",
    "    scheduler = get_cosine_schedule_with_warmup(optimizer, cfg.warmup_steps, cfg.distill_steps)\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    hw_stats.start()\n",
    "    step = 0\n",
    "    accum_step = 0\n",
    "    current_stage = 1\n",
    "\n",
    "    accumulation_steps = cfg.accumulation_steps\n",
    "    effective_batch = cfg.batch_size * accumulation_steps\n",
    "    print(f\"Gradient accumulation: {accumulation_steps} (effective batch = {effective_batch})\")\n",
    "    print(f\"Extended training: {cfg.distill_steps} steps\")\n",
    "\n",
    "    pbar = tqdm(total=cfg.distill_steps, desc='distilling (v14.1 - FDD+CTKD+HardCE)')\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    if len(train_loader) == 0:\n",
    "        pbar.close()\n",
    "        raise RuntimeError(\"train_loader is empty; aborting distillation loop.\")\n",
    "    if len(val_loader) == 0:\n",
    "        pbar.close()\n",
    "        raise RuntimeError(\"val_loader is empty; aborting distillation loop.\")\n",
    "\n",
    "    while step < cfg.distill_steps:\n",
    "        for batch in train_loader:\n",
    "            if step >= cfg.distill_steps:\n",
    "                break\n",
    "\n",
    "            # Check early stopping\n",
    "            if cfg.use_early_stopping and no_improve_steps >= cfg.early_stopping_patience:\n",
    "                print(f\"\\n  [Early Stopping] No improvement for {cfg.early_stopping_patience} steps\")\n",
    "                print(f\"     Best PPL: {best_ppl:.2f} at step {best_step}\")\n",
    "                training_logs['early_stopped'] = True\n",
    "                training_logs['early_stop_step'] = step\n",
    "                pbar.close()\n",
    "                return training_logs\n",
    "\n",
    "            ids = batch[0].to(device, non_blocking=True)\n",
    "\n",
    "            # Get lambda for CTKD\n",
    "            if cfg.use_ctkd:\n",
    "                current_lambda = get_lambda(\n",
    "                    step, cfg.distill_steps,\n",
    "                    lambda_max=cfg.lambda_max,\n",
    "                    warmup_ratio=cfg.lambda_warmup_ratio\n",
    "                )\n",
    "            else:\n",
    "                current_lambda = 0.0\n",
    "\n",
    "            # Get current FDD weight\n",
    "            if fdd_enabled and not fdd_killed:\n",
    "                current_fdd_weight = get_fdd_weight(step, cfg.fdd_warmup_steps, cfg.fdd_weight)\n",
    "            else:\n",
    "                current_fdd_weight = 0.0\n",
    "\n",
    "            if cfg.use_spike_semantic_loss:\n",
    "                current_spike_sem_weight = get_spike_semantic_weight(\n",
    "                    step,\n",
    "                    cfg.spike_semantic_warmup_steps,\n",
    "                    cfg.spike_semantic_weight,\n",
    "                )\n",
    "            else:\n",
    "                current_spike_sem_weight = 0.0\n",
    "\n",
    "            with torch.cuda.amp.autocast():\n",
    "                # Teacher forward (always get hidden states for FDD)\n",
    "                with torch.no_grad():\n",
    "                    t_out = teacher(ids, output_hidden_states=True)\n",
    "                    t_logits = t_out.logits\n",
    "                    t_hiddens = t_out.hidden_states  # tuple of tensors\n",
    "\n",
    "                # Student forward (always get hidden states for FDD)\n",
    "                student.train()\n",
    "                s_logits, s_hiddens, spike_aux = student(\n",
    "                    ids,\n",
    "                    return_hiddens=True,\n",
    "                    return_spike_info=True,\n",
    "                    detach_spikes=False,\n",
    "                )\n",
    "                spike_info = spike_aux.get('spike_info', {}) if isinstance(spike_aux, dict) else {}\n",
    "\n",
    "                # Get temperature\n",
    "                if cfg.use_ctkd and temp_module is not None:\n",
    "                    T = temp_module(current_lambda)\n",
    "                elif temp_module is not None:\n",
    "                    T = temp_module()\n",
    "                else:\n",
    "                    T = cfg.temperature\n",
    "\n",
    "                # KL divergence loss with temperature\n",
    "                s_log = F.log_softmax(s_logits / T, dim=-1)\n",
    "                t_prob = F.softmax(t_logits / T, dim=-1)\n",
    "                kl_loss = F.kl_div(\n",
    "                    s_log.view(-1, s_logits.size(-1)),\n",
    "                    t_prob.view(-1, t_logits.size(-1)),\n",
    "                    reduction='batchmean'\n",
    "                ) * (T ** 2)\n",
    "\n",
    "                # FDD loss (v14.1 with 100x weight increase)\n",
    "                if current_fdd_weight > 0:\n",
    "                    fdd_loss = compute_fdd_loss(\n",
    "                        s_hiddens,\n",
    "                        list(t_hiddens),  # Convert tuple to list\n",
    "                        fdd_layer_map,\n",
    "                        loss_type=cfg.fdd_loss_type\n",
    "                    )\n",
    "                else:\n",
    "                    fdd_loss = torch.tensor(0.0, device=device)\n",
    "\n",
    "                if current_spike_sem_weight > 0 and spike_info:\n",
    "                    sem_losses = []\n",
    "                    for s_layer, t_layer in fdd_layer_map.items():\n",
    "                        layer_spikes = spike_info.get(s_layer)\n",
    "                        if not isinstance(layer_spikes, dict):\n",
    "                            continue\n",
    "\n",
    "                        k_spikes = layer_spikes.get('k_spikes')\n",
    "                        v_spikes = layer_spikes.get('v_spikes')\n",
    "                        if k_spikes is None or v_spikes is None:\n",
    "                            continue\n",
    "\n",
    "                        spike_repr = 0.5 * (k_spikes + v_spikes)\n",
    "                        teacher_hidden = t_hiddens[t_layer + 1]\n",
    "\n",
    "                        if spike_repr.size(-1) != teacher_hidden.size(-1):\n",
    "                            min_dim = min(spike_repr.size(-1), teacher_hidden.size(-1))\n",
    "                            spike_repr = spike_repr[..., :min_dim]\n",
    "                            teacher_hidden = teacher_hidden[..., :min_dim]\n",
    "\n",
    "                        teacher_target = build_teacher_ternary_target(\n",
    "                            teacher_hidden,\n",
    "                            cfg.spike_target_threshold_scale,\n",
    "                        )\n",
    "                        sem_losses.append(F.mse_loss(spike_repr, teacher_target))\n",
    "\n",
    "                    if sem_losses:\n",
    "                        spike_sem_loss = torch.stack(sem_losses).mean()\n",
    "                    else:\n",
    "                        spike_sem_loss = torch.tensor(0.0, device=device)\n",
    "                else:\n",
    "                    spike_sem_loss = torch.tensor(0.0, device=device)\n",
    "\n",
    "                # v14.1: Hard distillation (CE with ground truth)\n",
    "                if cfg.ce_hard_weight > 0:\n",
    "                    shift_logits = s_logits[:, :-1, :].contiguous()\n",
    "                    shift_labels = ids[:, 1:].contiguous()\n",
    "                    ce_loss = F.cross_entropy(\n",
    "                        shift_logits.view(-1, shift_logits.size(-1)),\n",
    "                        shift_labels.view(-1),\n",
    "                        ignore_index=-100\n",
    "                    )\n",
    "                else:\n",
    "                    ce_loss = torch.tensor(0.0, device=device)\n",
    "\n",
    "                # Hidden alignment (usually disabled, kept for infrastructure)\n",
    "                if cfg.hidden_align_weight > 0:\n",
    "                    align_loss = compute_hidden_alignment_loss(\n",
    "                        t_hiddens, s_hiddens, projector,\n",
    "                        teacher_layers=cfg.teacher_n_layers,\n",
    "                        student_layers=cfg.n_layers\n",
    "                    )\n",
    "                else:\n",
    "                    align_loss = torch.tensor(0.0, device=device)\n",
    "\n",
    "                # Total loss (v14.1: added ce_hard_weight * ce_loss)\n",
    "                loss = (\n",
    "                    kl_loss\n",
    "                    + cfg.ce_hard_weight * ce_loss\n",
    "                    + current_fdd_weight * fdd_loss\n",
    "                    + current_spike_sem_weight * spike_sem_loss\n",
    "                    + cfg.hidden_align_weight * align_loss\n",
    "                )\n",
    "                loss = loss / accumulation_steps\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            accum_step += 1\n",
    "\n",
    "            if accum_step % accumulation_steps == 0:\n",
    "                scaler.unscale_(optimizer)\n",
    "                gn = torch.nn.utils.clip_grad_norm_(all_params, cfg.max_grad_norm)\n",
    "\n",
    "                if torch.isfinite(gn):\n",
    "                    scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "                hw_stats.record_step(ids.size(0) * accumulation_steps, ids.size(1))\n",
    "                spike_stats.record(student, step)\n",
    "\n",
    "                current_lr = optimizer.param_groups[0]['lr']\n",
    "                current_temp = temp_module.get_temperature() if temp_module is not None else cfg.temperature\n",
    "\n",
    "                # Log\n",
    "                training_logs['loss_history'].append({'step': step, 'loss': loss.item() * accumulation_steps})\n",
    "                training_logs['kl_loss_history'].append({'step': step, 'loss': kl_loss.item()})\n",
    "                training_logs['ce_loss_history'].append({'step': step, 'loss': ce_loss.item() if isinstance(ce_loss, torch.Tensor) else ce_loss})  # v14.1\n",
    "                training_logs['fdd_loss_history'].append({'step': step, 'loss': fdd_loss.item() if isinstance(fdd_loss, torch.Tensor) else fdd_loss})\n",
    "                training_logs['spike_sem_loss_history'].append({'step': step, 'loss': spike_sem_loss.item() if isinstance(spike_sem_loss, torch.Tensor) else spike_sem_loss})\n",
    "                training_logs['align_loss_history'].append({'step': step, 'loss': align_loss.item() if isinstance(align_loss, torch.Tensor) else align_loss})\n",
    "                training_logs['lr_history'].append({'step': step, 'lr': current_lr})\n",
    "                training_logs['temp_history'].append({'step': step, 'temperature': current_temp})\n",
    "                training_logs['lambda_history'].append({'step': step, 'lambda': current_lambda})\n",
    "                training_logs['fdd_weight_history'].append({'step': step, 'fdd_weight': current_fdd_weight})\n",
    "                training_logs['spike_sem_weight_history'].append({'step': step, 'weight': current_spike_sem_weight})\n",
    "                training_logs['stage_history'].append({'step': step, 'stage': 1})\n",
    "\n",
    "                # Update progress bar (v14.1: added CE loss)\n",
    "                fdd_str = f\"fdd={fdd_loss.item():.3f}\" if current_fdd_weight > 0 else \"fdd=of\"\n",
    "                ce_str = f\"ce={ce_loss.item():.3f}\" if cfg.ce_hard_weight > 0 else \"ce=of\"\n",
    "                sem_str = f\"sem={spike_sem_loss.item():.3f}\" if current_spike_sem_weight > 0 else \"sem=of\"\n",
    "                pbar.set_postfix(\n",
    "                    loss=f\"{loss.item() * accumulation_steps:.3f}\",\n",
    "                    kl=f\"{kl_loss.item():.3f}\",\n",
    "                    ce=ce_str,\n",
    "                    sem=sem_str,\n",
    "                    fdd=fdd_str,\n",
    "                    T=f\"{current_temp:.2f}\",\n",
    "                    lr=f\"{current_lr:.1e}\"\n",
    "                )\n",
    "                pbar.update(1)\n",
    "                step += 1\n",
    "\n",
    "                if step % cfg.eval_interval == 0:\n",
    "                    val_loss = evaluate(student, val_loader, device)\n",
    "                    val_ppl = get_ppl(val_loss)\n",
    "                    training_logs['ppl_history'].append({'step': step, 'ppl': val_ppl})\n",
    "\n",
    "                    amps = student.get_amplitudes()\n",
    "                    amp_str = ', '.join([f\"L{i}:{amps[f'layer_{i}']['k']:.2f}\" for i in range(min(4, cfg.n_layers))])\n",
    "\n",
    "                    # =========================================================\n",
    "                    # v14.1: FDD Kill Switch\n",
    "                    # =========================================================\n",
    "                    if step == cfg.fdd_warmup_steps and fdd_enabled:\n",
    "                        baseline_ppl = val_ppl\n",
    "                        print(f\"\\n  [FDD] Baseline PPL at warmup end: {baseline_ppl:.2f}\")\n",
    "\n",
    "                    if fdd_enabled and not fdd_killed and baseline_ppl is not None:\n",
    "                        ppl_increase = (val_ppl - baseline_ppl) / baseline_ppl\n",
    "                        if ppl_increase > cfg.fdd_kill_threshold:\n",
    "                            fdd_killed = True\n",
    "                            training_logs['fdd_killed'] = True\n",
    "                            training_logs['fdd_kill_step'] = step\n",
    "                            print(f\"\\n  [FDD KILLED] PPL increased {ppl_increase*100:.1f}% > {cfg.fdd_kill_threshold*100:.0f}%\")\n",
    "                            print(f\"     Baseline: {baseline_ppl:.2f}, Current: {val_ppl:.2f}\")\n",
    "                            print(f\"     Disabling FDD for remaining training\")\n",
    "\n",
    "                    # Early stopping check\n",
    "                    if val_ppl < best_ppl - cfg.min_ppl_delta:\n",
    "                        best_ppl = val_ppl\n",
    "                        best_step = step\n",
    "                        no_improve_steps = 0\n",
    "                        save_dict = {\n",
    "                            'student': student.state_dict(),\n",
    "                            'projector': projector.state_dict(),\n",
    "                            'step': step,\n",
    "                            'ppl': val_ppl,\n",
    "                        }\n",
    "                        if temp_module is not None:\n",
    "                            save_dict['temp_module'] = temp_module.state_dict()\n",
    "                        torch.save(save_dict, f'{OUTPUT_DIR}/checkpoints/v15_best.pt')\n",
    "                        improve_str = \" [NEW BEST]\"\n",
    "                    else:\n",
    "                        no_improve_steps += cfg.eval_interval\n",
    "                        improve_str = f\" (no improve: {no_improve_steps}/{cfg.early_stopping_patience})\"\n",
    "\n",
    "                    lambda_str = f\", lambda={current_lambda:.2f}\" if cfg.use_ctkd else \"\"\n",
    "                    fdd_status = \"KILLED\" if fdd_killed else f\"w={current_fdd_weight:.4f}\"\n",
    "                    sem_status = f\"w={current_spike_sem_weight:.4f}\" if current_spike_sem_weight > 0 else \"off\"\n",
    "                    print(\n",
    "                        f\"\\n  step {step}: ppl={val_ppl:.1f}, T={current_temp:.2f}{lambda_str}, \"\n",
    "                        f\"FDD:{fdd_status}, SEM:{sem_status}, amps=[{amp_str}...]{improve_str}\"\n",
    "                    )\n",
    "\n",
    "    pbar.close()\n",
    "    return training_logs\n",
    "\n",
    "print(\"distillation function defined (v14.1.1 - FDD+CTKD+HardCE)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# cell 18: run distillation (v14.1.1 - FDD+CTKD+HardCE)\n",
    "# =============================================================================\n",
    "print(\"=\"*60)\n",
    "print(\"v14.1: FDD (Feature Dynamics Distillation) + CTKD\")\n",
    "print(\"=\"*60)\n",
    "print(f\"  Architecture: {config.d_model}d x {config.n_layers}L (~22M params)\")\n",
    "print(f\"  Target: PPL < 400 (improve on v13.1's 434.44)\")\n",
    "print(f\"\")\n",
    "print(f\"{config.VERSION} Configuration:\")\n",
    "print(f\"  FDD: {config.use_fdd}\")\n",
    "print(f\"    Weight: {config.fdd_weight}\")\n",
    "print(f\"    Warmup: {config.fdd_warmup_steps} steps\")\n",
    "print(f\"    Layer map: {fdd_layer_map}\")\n",
    "print(f\"    Loss type: {config.fdd_loss_type}\")\n",
    "print(f\"    Kill threshold: {config.fdd_kill_threshold*100:.0f}%\")\n",
    "print(f\"  Spike semantic alignment: {config.use_spike_semantic_loss}\")\n",
    "if config.use_spike_semantic_loss:\n",
    "    print(f\"    Weight: {config.spike_semantic_weight}\")\n",
    "    print(f\"    Warmup: {config.spike_semantic_warmup_steps}\")\n",
    "    print(f\"    Target threshold scale: {config.spike_target_threshold_scale}\")\n",
    "print(f\"  CTKD: {config.use_ctkd} (proven from v12.1, v13.1)\")\n",
    "print(f\"  Extended training: {config.distill_steps} steps\")\n",
    "print(f\"  Early stopping: patience={config.early_stopping_patience}\")\n",
    "print(f\"  POCL: {config.use_pocl} (disabled - caused regression)\")\n",
    "print(\"\")\n",
    "\n",
    "# Instantiate collectors\n",
    "hw_stats = HardwareStatsCollector()\n",
    "spike_stats = SpikeStatsCollector(config.n_layers)\n",
    "print(\"Initialized HardwareStatsCollector and SpikeStatsCollector\")\n",
    "\n",
    "# Run distillation (FDD + CTKD)\n",
    "print(f\"\\nStarting distillation...\")\n",
    "\n",
    "distill_logs = distill_v14(\n",
    "    teacher, student, projector,\n",
    "    train_loader, val_loader,\n",
    "    config, DEVICE,\n",
    "    hw_stats, spike_stats,\n",
    "    fdd_layer_map  # v14: pass FDD layer mapping\n",
    ")\n",
    "\n",
    "# Report results\n",
    "print(f\"\\n\\n\" + \"=\"*60)\n",
    "print(\"v14.1 Distillation Complete!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if distill_logs['ppl_history']:\n",
    "    final_ppl = distill_logs['ppl_history'][-1]['ppl']\n",
    "    best_ppl_entry = min(distill_logs['ppl_history'], key=lambda x: x['ppl'])\n",
    "    print(f\"\\nFinal PPL: {final_ppl:.2f}\")\n",
    "    print(f\"Best PPL: {best_ppl_entry['ppl']:.2f} at step {best_ppl_entry['step']}\")\n",
    "\n",
    "if distill_logs['early_stopped']:\n",
    "    print(f\"\\nEarly stopped at step {distill_logs['early_stop_step']}\")\n",
    "else:\n",
    "    print(f\"\\nCompleted all {config.distill_steps} steps\")\n",
    "\n",
    "if distill_logs['fdd_killed']:\n",
    "    print(f\"\\nFDD was KILLED at step {distill_logs['fdd_kill_step']} (PPL regressed)\")\n",
    "else:\n",
    "    print(f\"\\nFDD remained active throughout training\")\n",
    "\n",
    "if distill_logs['temp_history']:\n",
    "    temps = [h['temperature'] for h in distill_logs['temp_history']]\n",
    "    print(f\"\\nTemperature evolution:\")\n",
    "    print(f\"  Start: {temps[0]:.2f}\")\n",
    "    print(f\"  End: {temps[-1]:.2f}\")\n",
    "\n",
    "if distill_logs['lambda_history']:\n",
    "    lambdas = [h['lambda'] for h in distill_logs['lambda_history']]\n",
    "    print(f\"\\nLambda evolution:\")\n",
    "    print(f\"  Start: {lambdas[0]:.2f}\")\n",
    "    print(f\"  End: {lambdas[-1]:.2f}\")\n",
    "\n",
    "if distill_logs['fdd_loss_history']:\n",
    "    fdd_losses = [h['loss'] for h in distill_logs['fdd_loss_history'] if h['loss'] > 0]\n",
    "    if fdd_losses:\n",
    "        print(f\"\\nFDD loss evolution:\")\n",
    "        print(f\"  Start (after warmup): {fdd_losses[0]:.4f}\")\n",
    "        print(f\"  End: {fdd_losses[-1]:.4f}\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# cell 20: lora implementation (same as v9)\n",
    "# =============================================================================\n",
    "class LoRALinear(nn.Module):\n",
    "    \"\"\"lora adapter for linear layers.\"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, rank=8, alpha=16.0):\n",
    "        super().__init__()\n",
    "        self.scaling = alpha / rank\n",
    "        self.lora_A = nn.Parameter(torch.zeros(rank, in_features))\n",
    "        self.lora_B = nn.Parameter(torch.zeros(out_features, rank))\n",
    "        nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return (x @ self.lora_A.T @ self.lora_B.T) * self.scaling\n",
    "\n",
    "\n",
    "def apply_lora(model, rank=8, alpha=16.0, targets=['key_proj', 'value_proj']):\n",
    "    \"\"\"apply lora adapters to specified modules.\"\"\"\n",
    "    lora_modules = {}\n",
    "    for name, module in model.named_modules():\n",
    "        if any(t in name for t in targets) and isinstance(module, nn.Linear):\n",
    "            lora = LoRALinear(module.in_features, module.out_features, rank, alpha).to(next(module.parameters()).device)\n",
    "            lora_modules[name] = lora\n",
    "            orig_forward = module.forward\n",
    "            def make_forward(orig, lora_mod):\n",
    "                def forward(x):\n",
    "                    return orig(x) + lora_mod(x)\n",
    "                return forward\n",
    "            module.forward = make_forward(orig_forward, lora)\n",
    "    print(f\"lora: {len(lora_modules)} modules, rank={rank}\")\n",
    "    return lora_modules\n",
    "\n",
    "print(\"lora defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# cell 21: ttt with lora (same as v9)\n",
    "# =============================================================================\n",
    "print(\"=\"*60)\n",
    "print(\"phase 2: test-time training with lora\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for p in student.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "lora_modules = apply_lora(student, config.lora_rank, config.lora_alpha)\n",
    "lora_params = sum(p.numel() for m in lora_modules.values() for p in m.parameters())\n",
    "\n",
    "pre_ttt_loss = evaluate(student, val_loader, DEVICE)\n",
    "pre_ttt_ppl = get_ppl(pre_ttt_loss)\n",
    "print(f\"\\npre-ttt ppl: {pre_ttt_ppl:.2f}\")\n",
    "\n",
    "lora_opt = torch.optim.AdamW([p for m in lora_modules.values() for p in m.parameters()], lr=config.ttt_lr)\n",
    "ttt_logs = {'loss_history': []}\n",
    "student.train()\n",
    "\n",
    "for step, batch in enumerate(val_loader):\n",
    "    if step >= config.ttt_steps:\n",
    "        break\n",
    "    ids = batch[0].to(DEVICE)\n",
    "    with torch.cuda.amp.autocast():\n",
    "        loss = F.cross_entropy(student(ids)[:, :-1].reshape(-1, config.vocab_size), ids[:, 1:].reshape(-1))\n",
    "    lora_opt.zero_grad()\n",
    "    loss.backward()\n",
    "    lora_opt.step()\n",
    "    ttt_logs['loss_history'].append({'step': step, 'loss': loss.item()})\n",
    "    if step % 20 == 0:\n",
    "        print(f\"  ttt {step}: loss={loss.item():.4f}\")\n",
    "\n",
    "post_ttt_loss = evaluate(student, val_loader, DEVICE)\n",
    "post_ttt_ppl = get_ppl(post_ttt_loss)\n",
    "print(f\"\\npost-ttt ppl: {post_ttt_ppl:.2f}\")\n",
    "print(f\"ttt improvement: {pre_ttt_ppl - post_ttt_ppl:.1f} ppl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# cell 22: final evaluation (v14.1.1 - FDD+CTKD+HardCE)\n",
    "# =============================================================================\n",
    "print(\"=\"*60)\n",
    "print(\"final evaluation (v14.1.1 - FDD+CTKD+HardCE)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "teacher_loss = evaluate(teacher, val_loader, DEVICE, is_gpt2=True)\n",
    "teacher_ppl = get_ppl(teacher_loss)\n",
    "student_loss = evaluate(student, val_loader, DEVICE)\n",
    "student_ppl = get_ppl(student_loss)\n",
    "\n",
    "# v14: Get final temperature and lambda from CTKD\n",
    "final_temp = distill_logs['temp_history'][-1]['temperature'] if distill_logs['temp_history'] else config.tau_init\n",
    "final_lambda = distill_logs['temp_history'][-1].get('lambda', config.lambda_max) if distill_logs['temp_history'] else config.lambda_max\n",
    "\n",
    "# VRAM logging\n",
    "vram_peak_gb = torch.cuda.max_memory_allocated() / 1e9 if torch.cuda.is_available() else 0\n",
    "\n",
    "print(f\"\")\n",
    "print(f\"{'model':<30} {'ppl':>10} {'params':>15}\")\n",
    "print(\"-\" * 55)\n",
    "print(f\"{'gpt-2 (teacher)':<30} {teacher_ppl:>10.2f} {teacher_params:>15,}\")\n",
    "print(f\"{'asnn-goose v14.1 (student)':<30} {student_ppl:>10.2f} {student_params:>15,}\")\n",
    "print(\"-\" * 55)\n",
    "print(f\"{'compression':<30} {compression_ratio:>10.1f}x\")\n",
    "print(f\"{'ppl gap':<30} {student_ppl - teacher_ppl:>10.2f}\")\n",
    "print(f\"{'spike density':<30} {student.get_avg_spike_density():>10.3f}\")\n",
    "print(f\"{'VRAM peak':<30} {vram_peak_gb:>10.2f}GB\")\n",
    "print(f\"{'final temperature':<30} {final_temp:>10.2f}\")\n",
    "print(f\"{'final lambda (GRL)':<30} {final_lambda:>10.3f}\")\n",
    "print(\"\")\n",
    "print(\"CTKD Implementation:\")\n",
    "print(f\"  tau range: [{config.tau_min:.1f}, {config.tau_max:.1f}]\")\n",
    "print(f\"  lambda warmup ratio: {config.lambda_warmup_ratio:.0%}\")\n",
    "print(f\"  GRL: Gradient Reversal Layer for adversarial min-max\")\n",
    "print(\"\")\n",
    "print(\"version comparison:\")\n",
    "print(f\"  v6: 627.3 PPL (baseline)\")\n",
    "print(f\"  v7: 1655 PPL (regression!)\")\n",
    "print(f\"  v8: 559 PPL (fixed)\")\n",
    "print(f\"  v9: 541.7 PPL (capacity increase)\")\n",
    "print(f\"  v10: 514.5 PPL (320d/5L baseline)\")\n",
    "print(f\"  v14: 512.67 PPL (channel-wise, WITH reg)\")\n",
    "print(f\"  v14.1: 512.04 PPL (channel-wise, NO reg)\")\n",
    "print(f\"  v12: FAILED (temp runaway without GRL)\")\n",
    "print(f\"  v14: {student_ppl:.2f} PPL (POCL, T={final_temp:.2f}, λ={final_lambda:.3f})\")\n",
    "if student_ppl < 500:\n",
    "    print(f\"  {config.VERSION} TARGET MET! PPL < 500\")\n",
    "elif student_ppl < 512.04:\n",
    "    print(f\"  v14.1 beats v14 by {424.81 - student_ppl:.1f} PPL\")\n",
    "else:\n",
    "    print(f\"  WARNING: v14.1 did not improve over v14\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# cell 23: visualization\n",
    "# =============================================================================\n",
    "figure_path = None\n",
    "\n",
    "if MATPLOTLIB_AVAILABLE:\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "    # distillation loss\n",
    "    d_steps = [l['step'] for l in distill_logs['loss_history']]\n",
    "    d_losses = [l['loss'] for l in distill_logs['loss_history']]\n",
    "    kl_losses = [l['loss'] for l in distill_logs['kl_loss_history']]\n",
    "    axes[0,0].plot(d_steps, d_losses, label='total', alpha=0.8)\n",
    "    axes[0,0].plot(d_steps, kl_losses, label='kl', alpha=0.7)\n",
    "\n",
    "    # CE loss (if available)\n",
    "    if 'ce_loss_history' in distill_logs and distill_logs['ce_loss_history']:\n",
    "        ce_losses = [l['loss'] for l in distill_logs['ce_loss_history']]\n",
    "        axes[0,0].plot(d_steps, ce_losses, label='ce', alpha=0.6)\n",
    "    axes[0,0].set_xlabel('step')\n",
    "    axes[0,0].set_ylabel('loss')\n",
    "    axes[0,0].set_title(f'distillation loss ({config.VERSION})')\n",
    "    axes[0,0].legend()\n",
    "\n",
    "    # validation ppl\n",
    "    p_steps = [l['step'] for l in distill_logs['ppl_history']]\n",
    "    p_ppls = [l['ppl'] for l in distill_logs['ppl_history']]\n",
    "    axes[0,1].plot(p_steps, p_ppls, 'orange', marker='o')\n",
    "    axes[0,1].axhline(y=teacher_ppl, color='green', linestyle='--', label=f'teacher ({teacher_ppl:.1f})')\n",
    "    axes[0,1].axhline(y=627.3, color='blue', linestyle=':', label='v6 (627.3)')\n",
    "    axes[0,1].axhline(y=541.7, color='purple', linestyle=':', label='v9 (541.7)')\n",
    "    axes[0,1].axhline(y=300, color='red', linestyle='--', label=f'{config.VERSION} target')\n",
    "    axes[0,1].set_xlabel('step')\n",
    "    axes[0,1].set_ylabel('ppl')\n",
    "    axes[0,1].set_title('validation ppl')\n",
    "    axes[0,1].legend()\n",
    "\n",
    "    # lr schedule\n",
    "    lr_steps = [l['step'] for l in distill_logs['lr_history']]\n",
    "    lr_vals = [l['lr'] for l in distill_logs['lr_history']]\n",
    "    axes[0,2].plot(lr_steps, lr_vals, 'purple')\n",
    "    axes[0,2].axvline(x=config.warmup_steps, color='gray', linestyle='--', label=f'warmup ({config.warmup_steps})')\n",
    "    axes[0,2].set_xlabel('step')\n",
    "    axes[0,2].set_ylabel('lr')\n",
    "    axes[0,2].set_title('learning rate')\n",
    "    axes[0,2].legend()\n",
    "\n",
    "    # spike density + amplitudes (first 4 layers)\n",
    "    spike_summary = spike_stats.get_summary()\n",
    "    layers = [f'layer_{i}' for i in range(min(4, config.n_layers))]\n",
    "    k_dens = [spike_summary['per_layer'][l]['k_final'] for l in layers]\n",
    "    v_dens = [spike_summary['per_layer'][l]['v_final'] for l in layers]\n",
    "    k_amps = [spike_summary['per_layer'][l]['k_amp_final'] for l in layers]\n",
    "    v_amps = [spike_summary['per_layer'][l]['v_amp_final'] for l in layers]\n",
    "\n",
    "    x = np.arange(len(layers))\n",
    "    axes[1,0].bar(x - 0.2, k_dens, 0.4, label='k density')\n",
    "    axes[1,0].bar(x + 0.2, v_dens, 0.4, label='v density')\n",
    "    ax2 = axes[1,0].twinx()\n",
    "    ax2.plot(x, k_amps, 'r-o', label='k amp')\n",
    "    ax2.plot(x, v_amps, 'b-s', label='v amp')\n",
    "    axes[1,0].set_xlabel('layer')\n",
    "    axes[1,0].set_ylabel('density')\n",
    "    ax2.set_ylabel('amplitude')\n",
    "    axes[1,0].set_title(f'spike density & amps (first 4/{config.n_layers} layers)')\n",
    "    axes[1,0].legend(loc='upper left')\n",
    "    ax2.legend(loc='upper right')\n",
    "\n",
    "    # ttt loss\n",
    "    t_steps = [l['step'] for l in ttt_logs['loss_history']]\n",
    "    t_losses = [l['loss'] for l in ttt_logs['loss_history']]\n",
    "    axes[1,1].plot(t_steps, t_losses, 'red')\n",
    "    axes[1,1].set_xlabel('step')\n",
    "    axes[1,1].set_ylabel('ce loss')\n",
    "    axes[1,1].set_title('ttt with lora')\n",
    "\n",
    "    # version comparison\n",
    "    # Historical versions for comparison (must match in length!)\n",
    "    versions = ['v6', 'v9', 'v10', 'v12.1', 'v13.1', 'v14', 'v14.3', config.VERSION]\n",
    "    # Teacher PPL is constant\n",
    "    t_ppls = [44.6] * len(versions)\n",
    "    # Student PPL history (from changelog)\n",
    "    s_ppls = [627.3, 541.7, 514.5, 445.61, 434.44, 424.81, 306.89, student_ppl]\n",
    "    assert len(versions) == len(t_ppls) == len(s_ppls), f\"Array length mismatch: {len(versions)}, {len(t_ppls)}, {len(s_ppls)}\"\n",
    "    x = np.arange(len(versions))\n",
    "    axes[1,2].bar(x - 0.2, t_ppls, 0.4, label='teacher', alpha=0.7)\n",
    "    axes[1,2].bar(x + 0.2, s_ppls, 0.4, label='student', alpha=0.7)\n",
    "    axes[1,2].axhline(y=300, color='red', linestyle='--', label=f'{config.VERSION} target', alpha=0.7)\n",
    "    axes[1,2].set_xticks(x)\n",
    "    axes[1,2].set_xticklabels(versions)\n",
    "    axes[1,2].set_ylabel('ppl')\n",
    "    axes[1,2].set_title('version comparison')\n",
    "    axes[1,2].legend()\n",
    "    axes[1,2].set_yscale('log')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    figure_path = f'{OUTPUT_DIR}/figures/v15_training_{RUN_TIMESTAMP}.png'\n",
    "    plt.savefig(figure_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"saved: {figure_path}\")\n",
    "else:\n",
    "    print(\"matplotlib unavailable: skipped visualization cell (cell 23).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# cell 24: build results dict (v13 - Proper CTKD with GRL)\n",
    "# =============================================================================\n",
    "print(\"building results (v13 - CTKD with Gradient Reversal Layer)...\")\n",
    "\n",
    "figure_base64 = None\n",
    "training_plot_filename = None\n",
    "if 'figure_path' in globals() and figure_path and os.path.exists(figure_path):\n",
    "    with open(figure_path, 'rb') as f:\n",
    "        figure_base64 = base64.b64encode(f.read()).decode('utf-8')\n",
    "    training_plot_filename = os.path.basename(figure_path)\n",
    "else:\n",
    "    print(\"figure not available; continuing without embedded training plot\")\n",
    "\n",
    "# v13: Extract final lambda\n",
    "final_lambda = distill_logs['temp_history'][-1].get('lambda', config.lambda_max) if distill_logs['temp_history'] else config.lambda_max\n",
    "\n",
    "results = {\n",
    "    'version': config.VERSION,\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'run_id': RUN_TIMESTAMP,\n",
    "    'platform': PLATFORM,\n",
    "    'description': 'CTKD with Gradient Reversal Layer - adversarial temperature learning',\n",
    "\n",
    "    f'{config.VERSION}_design': {\n",
    "        'principle': 'CTKD: Adversarial min-max optimization via GRL',\n",
    "        'innovation': 'Gradient Reversal Layer makes temperature MAXIMIZE KL while student MINIMIZES',\n",
    "        'rationale': 'Proper CTKD (ArXiv 2211.16231) requires adversarial training, not simple regularization',\n",
    "        'why_v12_failed': 'v12 used simple regularization - optimizer pushed T to max for easy KL',\n",
    "        'techniques': {\n",
    "            'ctkd_with_grl': 'ENABLED - Gradient Reversal Layer for adversarial min-max (v13 KEY)',\n",
    "            'lambda_scheduling': f'Cosine warmup 0->{config.lambda_max} with {config.lambda_warmup_ratio:.0%} warmup',\n",
    "            'sigmoid_bounding': f'T bounded to [{config.tau_min}, {config.tau_max}] via sigmoid (smooth gradients)',\n",
    "            'no_manual_reg': 'GRL eliminates need for manual temperature regularization',\n",
    "            'progressive_stages': 'DISABLED',\n",
    "            'channel_wise_spikes': 'DISABLED (structural symmetry issue)',\n",
    "        },\n",
    "        'grl_mechanism': {\n",
    "            'forward_pass': 'Identity: GRL(x) = x',\n",
    "            'backward_pass': 'Negation: dGRL/dx = -lambda',\n",
    "            'effect': 'Temperature gradients reversed -> T maximizes KL loss',\n",
    "        },\n",
    "        'temperature_config': {\n",
    "            'tau_min': config.tau_min,\n",
    "            'tau_max': config.tau_max,\n",
    "            'tau_init': config.tau_init,\n",
    "            'lambda_max': config.lambda_max,\n",
    "            'lambda_warmup_ratio': config.lambda_warmup_ratio,\n",
    "        },\n",
    "        'architecture': {\n",
    "            'd_model': config.d_model,\n",
    "            'n_layers': 5,\n",
    "            'params': '~22M',\n",
    "        },\n",
    "        'speedups': {\n",
    "            'gradient_checkpointing': USE_GRADIENT_CHECKPOINTING,\n",
    "            'torch_compile': compile_success,\n",
    "            'fused_optimizer': True,\n",
    "            'accumulation_steps': config.accumulation_steps,\n",
    "        },\n",
    "        'unchanged': [\n",
    "            'hidden_align_weight: 0.0',\n",
    "            'warmup_steps: 50',\n",
    "            'distill_steps: 3000',\n",
    "        ],\n",
    "    },\n",
    "\n",
    "    'architecture': {\n",
    "        'teacher': {'name': 'gpt2', 'params': teacher_params},\n",
    "        'student': {\n",
    "            'name': f'asnn-goose-{config.VERSION}',\n",
    "            'd_model': config.d_model,\n",
    "            'n_layers': config.n_layers,\n",
    "            'params': student_params,\n",
    "        },\n",
    "        'projector_params': projector_params,\n",
    "        'compression_ratio': compression_ratio,\n",
    "        'vram_peak_gb': vram_peak_gb,\n",
    "    },\n",
    "\n",
    "    'training_config': {\n",
    "        'distill_steps': config.distill_steps,\n",
    "        'tau_min': config.tau_min,\n",
    "        'tau_max': config.tau_max,\n",
    "        'tau_init': config.tau_init,\n",
    "        'final_temperature': final_temp,\n",
    "        'lambda_max': config.lambda_max,\n",
    "        'lambda_warmup_ratio': config.lambda_warmup_ratio,\n",
    "        'final_lambda': final_lambda,\n",
    "        'hidden_align_weight': config.hidden_align_weight,\n",
    "        'warmup_steps': config.warmup_steps,\n",
    "        'batch_size': config.batch_size,\n",
    "        'accumulation_steps': config.accumulation_steps,\n",
    "        'effective_batch': config.batch_size * config.accumulation_steps,\n",
    "        'distill_lr': config.distill_lr,\n",
    "        'max_grad_norm': config.max_grad_norm,\n",
    "    },\n",
    "\n",
    "    'results': {\n",
    "        'teacher_ppl': teacher_ppl,\n",
    "        'student_ppl': student_ppl,\n",
    "        'ppl_gap': student_ppl - teacher_ppl,\n",
    "        'spike_density': student.get_avg_spike_density(),\n",
    "        'amplitudes': student.get_amplitudes(),\n",
    "        'final_temperature': final_temp,\n",
    "        'final_lambda': final_lambda,\n",
    "        'target_met': student_ppl < 500,\n",
    "    },\n",
    "\n",
    "    'training_curves': {\n",
    "        'loss_history': distill_logs['loss_history'],\n",
    "        'kl_loss_history': distill_logs['kl_loss_history'],\n",
    "        'ce_loss_history': distill_logs.get('ce_loss_history', []),\n",
    "        'fdd_loss_history': distill_logs.get('fdd_loss_history', []),\n",
    "        'spike_sem_loss_history': distill_logs.get('spike_sem_loss_history', []),\n",
    "        'align_loss_history': distill_logs['align_loss_history'],\n",
    "        'ppl_history': distill_logs['ppl_history'],\n",
    "        'lr_history': distill_logs['lr_history'],\n",
    "        'temp_history': distill_logs['temp_history'],  # v13: includes temperature AND lambda\n",
    "        'lambda_history': distill_logs.get('lambda_history', []),\n",
    "        'spike_sem_weight_history': distill_logs.get('spike_sem_weight_history', []),\n",
    "    },\n",
    "\n",
    "    'hardware_stats': hw_stats.get_summary(),\n",
    "    'spike_analysis': spike_stats.get_summary(),\n",
    "\n",
    "    'ttt': {\n",
    "        'lora_params': lora_params,\n",
    "        'pre_ppl': pre_ttt_ppl,\n",
    "        'post_ppl': post_ttt_ppl,\n",
    "        'improvement': pre_ttt_ppl - post_ttt_ppl,\n",
    "        'loss_history': ttt_logs['loss_history'],\n",
    "    },\n",
    "\n",
    "    'comparison': {\n",
    "        'v6': {'student_ppl': 627.3, 'note': 'baseline'},\n",
    "        'v7': {'student_ppl': 1655, 'note': 'regression (align=1.0, T=4)'},\n",
    "        'v8': {'student_ppl': 559, 'note': 'fixed defaults (align=0, T=2)'},\n",
    "        'v9': {'student_ppl': 541.7, 'note': 'capacity increase (320d, 5L)'},\n",
    "        'v10': {'student_ppl': 514.5, 'note': '320d/5L baseline'},\n",
    "        'v14': {'student_ppl': 512.67, 'note': 'channel-wise WITH reg (bug)'},\n",
    "        'v14.1': {'student_ppl': 512.04, 'note': 'channel-wise NO reg (symmetry issue)'},\n",
    "        'v12': {'student_ppl': 'FAILED', 'note': 'temp runaway without GRL'},\n",
    "        'v13': {'student_ppl': student_ppl, 'note': f'POCL (T={final_temp:.2f}, L={final_lambda:.3f})'},\n",
    "    },\n",
    "\n",
    "    'figures': {\n",
    "        'training_plot': {\n",
    "            'filename': training_plot_filename,\n",
    "            'base64': figure_base64,\n",
    "        }\n",
    "    },\n",
    "\n",
    "    # validation_tests will be added in cell 26\n",
    "}\n",
    "\n",
    "print(\"results dict built (validation_tests pending)\")\n",
    "print(f\"  version: {config.VERSION} ({config.VERSION_DESC})\")\n",
    "print(f\"  final_temperature: {final_temp:.2f}\")\n",
    "print(f\"  final_lambda: {final_lambda:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# cell 25: validation tests (v14.1 - 12 tests with FDD)\n",
    "# =============================================================================\n",
    "# These tests validate correct v14.1 implementation\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"{config.VERSION} Validation Test Suite\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "tests = []\n",
    "\n",
    "# =============================================================================\n",
    "# Test 1: PPL Target (<400)\n",
    "# =============================================================================\n",
    "if distill_logs['ppl_history']:\n",
    "    best_ppl_entry = min(distill_logs['ppl_history'], key=lambda x: x['ppl'])\n",
    "    best_ppl = best_ppl_entry['ppl']\n",
    "    target_ppl = 400  # target for this version\n",
    "    ppl_pass = best_ppl < target_ppl\n",
    "    tests.append(('PPL < 400', ppl_pass, f\"best_ppl={best_ppl:.2f}, target={target_ppl}\"))\n",
    "else:\n",
    "    tests.append(('PPL < 400', False, \"No PPL history found\"))\n",
    "\n",
    "# =============================================================================\n",
    "# Test 2: PPL Improvement over v13.1 (434.44)\n",
    "# =============================================================================\n",
    "if distill_logs['ppl_history']:\n",
    "    best_ppl_entry = min(distill_logs['ppl_history'], key=lambda x: x['ppl'])\n",
    "    v13_1_ppl = 434.44\n",
    "    improvement = v13_1_ppl - best_ppl_entry['ppl']\n",
    "    improve_pass = improvement > 0\n",
    "    tests.append(('Improved over v13.1', improve_pass, f\"improvement={improvement:.2f} PPL\"))\n",
    "else:\n",
    "    tests.append(('Improved over v13.1', False, \"No PPL history\"))\n",
    "\n",
    "# =============================================================================\n",
    "# Test 3: Spike Density in Valid Range [0.1, 0.9]\n",
    "# =============================================================================\n",
    "if spike_stats.step_densities:\n",
    "    final_density = spike_stats.step_densities[-1]['density']\n",
    "    density_pass = 0.1 <= final_density <= 0.9\n",
    "    tests.append(('Spike density [0.1, 0.9]', density_pass, f\"density={final_density:.3f}\"))\n",
    "else:\n",
    "    tests.append(('Spike density [0.1, 0.9]', False, \"No spike history\"))\n",
    "\n",
    "# =============================================================================\n",
    "# Test 4: Amplitudes in Healthy Range [0.3, 3.0]\n",
    "# =============================================================================\n",
    "amps = student.get_amplitudes()\n",
    "amp_values = []\n",
    "for layer_name, layer_amps in amps.items():\n",
    "    amp_values.extend([layer_amps['k'], layer_amps['v']])\n",
    "amp_min, amp_max = min(amp_values), max(amp_values)\n",
    "amp_pass = 0.3 <= amp_min and amp_max <= 3.0\n",
    "tests.append(('Amplitudes [0.3, 3.0]', amp_pass, f\"range=[{amp_min:.2f}, {amp_max:.2f}]\"))\n",
    "\n",
    "# =============================================================================\n",
    "# Test 5: Training Completed (all steps or early stopped)\n",
    "# =============================================================================\n",
    "if distill_logs['early_stopped']:\n",
    "    training_pass = distill_logs['early_stop_step'] > config.distill_steps * 0.3\n",
    "    tests.append(('Training completed', training_pass, f\"Early stopped at {distill_logs['early_stop_step']} steps\"))\n",
    "else:\n",
    "    training_pass = len(distill_logs['loss_history']) >= config.distill_steps * 0.95\n",
    "    tests.append(('Training completed', training_pass, f\"Completed {len(distill_logs['loss_history'])}/{config.distill_steps} steps\"))\n",
    "\n",
    "# =============================================================================\n",
    "# Test 6: No NaN/Inf in Loss\n",
    "# =============================================================================\n",
    "nan_inf_found = False\n",
    "for h in distill_logs['loss_history']:\n",
    "    if h['loss'] != h['loss'] or h['loss'] == float('inf') or h['loss'] == float('-inf'):\n",
    "        nan_inf_found = True\n",
    "        break\n",
    "nan_pass = not nan_inf_found\n",
    "tests.append(('No NaN/Inf loss', nan_pass, \"All losses finite\" if nan_pass else \"Found NaN/Inf\"))\n",
    "\n",
    "# =============================================================================\n",
    "# Test 7: VRAM Usage Reasonable (<8GB)\n",
    "# =============================================================================\n",
    "if hasattr(hw_stats, 'get_summary'):\n",
    "    hw_summary = hw_stats.get_summary()\n",
    "    vram_gb = hw_summary.get('peak_gpu_memory_gb', 0)\n",
    "    vram_pass = vram_gb < 8.0\n",
    "    tests.append(('VRAM < 8GB', vram_pass, f\"peak={vram_gb:.2f}GB\"))\n",
    "else:\n",
    "    tests.append(('VRAM < 8GB', True, \"hw_stats not available\"))\n",
    "\n",
    "# =============================================================================\n",
    "# Test 8: FDD Was Active (v14.1)\n",
    "# =============================================================================\n",
    "if config.use_fdd:\n",
    "    # FDD should have been active at some point\n",
    "    fdd_losses = [h['loss'] for h in distill_logs['fdd_loss_history'] if h.get('loss') is not None and h['loss'] != 0]\n",
    "    fdd_active_pass = len(fdd_losses) > 0\n",
    "    if distill_logs['fdd_killed']:\n",
    "        status = f\"Active then KILLED at step {distill_logs['fdd_kill_step']}\"\n",
    "    else:\n",
    "        status = f\"Active for {len(fdd_losses)} steps\"\n",
    "    tests.append(('FDD was active', fdd_active_pass, status))\n",
    "else:\n",
    "    tests.append(('FDD was active', True, \"FDD disabled in config\"))\n",
    "\n",
    "# =============================================================================\n",
    "# Test 9: CTKD Temperature Evolved (v14.1)\n",
    "# =============================================================================\n",
    "if config.use_ctkd and distill_logs['temp_history']:\n",
    "    temps = [h['temperature'] for h in distill_logs['temp_history']]\n",
    "    start_temp = temps[0]\n",
    "    end_temp = temps[-1]\n",
    "    temp_evolved = abs(end_temp - start_temp) > 0.1  # Should have moved\n",
    "    tests.append(('Temperature evolved', temp_evolved, f\"start={start_temp:.2f}, end={end_temp:.2f}\"))\n",
    "else:\n",
    "    tests.append(('Temperature evolved', True, \"CTKD disabled or no temp history\"))\n",
    "\n",
    "# =============================================================================\n",
    "# Test 10: Early Stopping Working (if triggered)\n",
    "# =============================================================================\n",
    "if config.use_early_stopping:\n",
    "    if distill_logs['early_stopped']:\n",
    "        es_step = distill_logs['early_stop_step']\n",
    "        es_pass = config.distill_steps * 0.3 < es_step < config.distill_steps\n",
    "        tests.append(('Early stopping working', es_pass, f\"stopped at {es_step}\"))\n",
    "    else:\n",
    "        if distill_logs['ppl_history']:\n",
    "            best_ppl_entry = min(distill_logs['ppl_history'], key=lambda x: x['ppl'])\n",
    "            last_improvement_step = best_ppl_entry['step']\n",
    "            final_step = distill_logs['ppl_history'][-1]['step']\n",
    "            gap = final_step - last_improvement_step\n",
    "            es_pass = gap <= config.early_stopping_patience + config.eval_interval\n",
    "            tests.append(('Early stopping working', es_pass, f\"last improvement at step {last_improvement_step}\"))\n",
    "        else:\n",
    "            tests.append(('Early stopping working', True, \"No PPL history\"))\n",
    "else:\n",
    "    tests.append(('Early stopping working', True, \"Early stopping disabled\"))\n",
    "\n",
    "# =============================================================================\n",
    "# Test 11: FDD Loss Decreased (v14.1)\n",
    "# =============================================================================\n",
    "if config.use_fdd and distill_logs['fdd_loss_history']:\n",
    "    fdd_losses = [h['loss'] for h in distill_logs['fdd_loss_history'] if h.get('loss') is not None and h['loss'] != 0]\n",
    "    if len(fdd_losses) >= 10:\n",
    "        start_fdd = sum(fdd_losses[:5]) / 5\n",
    "        end_fdd = sum(fdd_losses[-5:]) / 5\n",
    "        fdd_decreased = end_fdd < start_fdd\n",
    "        tests.append(('FDD loss decreased', fdd_decreased, f\"start={start_fdd:.4f}, end={end_fdd:.4f}\"))\n",
    "    else:\n",
    "        tests.append(('FDD loss decreased', True, \"Not enough FDD data points\"))\n",
    "else:\n",
    "    tests.append(('FDD loss decreased', True, \"FDD disabled or no history\"))\n",
    "\n",
    "# =============================================================================\n",
    "# Test 12: Extended Training (5000 steps)\n",
    "# =============================================================================\n",
    "extended_pass = config.distill_steps >= 5000\n",
    "tests.append(('Extended training (5000+)', extended_pass, f\"distill_steps={config.distill_steps}\"))\n",
    "\n",
    "# =============================================================================\n",
    "# Report Results\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"TEST RESULTS\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "passed = 0\n",
    "failed = 0\n",
    "for name, result, details in tests:\n",
    "    status = \"PASS\" if result else \"FAIL\"\n",
    "    symbol = \"V\" if result else \"X\"\n",
    "    print(f\"[{symbol}] {name}: {details}\")\n",
    "    if result:\n",
    "        passed += 1\n",
    "    else:\n",
    "        failed += 1\n",
    "\n",
    "print(\"-\"*60)\n",
    "print(f\"SUMMARY: {passed}/{len(tests)} tests passed\")\n",
    "if failed > 0:\n",
    "    print(f\"WARNING: {failed} tests failed!\")\n",
    "else:\n",
    "    print(f\"ALL TESTS PASSED! {config.VERSION} implementation validated.\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Store results\n",
    "validation_results = {\n",
    "    'tests': tests,\n",
    "    'passed': passed,\n",
    "    'failed': failed,\n",
    "    'total': len(tests)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# cell 27: V15 SpikingBrain - Information Encoding Validation\n",
    "# =============================================================================\n",
    "# Validate that spike patterns encode meaningful semantic information\n",
    "# Prerequisite for v16 (sparse ops)\n",
    "\n",
    "import numpy as np\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "\n",
    "# =============================================================================\n",
    "# INLINE: SpikingBrain Validation Classes\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass\n",
    "class SpikeHealthMetrics:\n",
    "    \"\"\"Container for spike health metrics.\"\"\"\n",
    "    dead_neuron_pct: float\n",
    "    dead_neuron_indices: Dict[str, np.ndarray]\n",
    "    saturated_neuron_pct: float\n",
    "    saturated_neuron_indices: Dict[str, np.ndarray]\n",
    "    firing_rate_mean: float\n",
    "    firing_rate_std: float\n",
    "    per_channel_rates: Dict[str, np.ndarray]\n",
    "    health_pass: bool\n",
    "    alerts: List[str]\n",
    "\n",
    "@dataclass\n",
    "class SpikingBrainValidation:\n",
    "    \"\"\"Complete validation results.\"\"\"\n",
    "    health: SpikeHealthMetrics\n",
    "    mutual_information: Dict[str, float]\n",
    "    cka: Dict[str, float]\n",
    "    overall_pass: bool\n",
    "    summary: str\n",
    "\n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        return {\n",
    "            'health': {\n",
    "                'dead_neuron_pct': self.health.dead_neuron_pct,\n",
    "                'saturated_neuron_pct': self.health.saturated_neuron_pct,\n",
    "                'firing_rate_mean': self.health.firing_rate_mean,\n",
    "                'firing_rate_std': self.health.firing_rate_std,\n",
    "                'health_pass': self.health.health_pass,\n",
    "                'alerts': self.health.alerts,\n",
    "            },\n",
    "            'mutual_information': self.mutual_information,\n",
    "            'cka': self.cka,\n",
    "            'overall_pass': self.overall_pass,\n",
    "        }\n",
    "\n",
    "\n",
    "class MutualInformationEstimator:\n",
    "    \"\"\"Estimate MI between spikes and teacher hiddens using binning.\"\"\"\n",
    "\n",
    "    def __init__(self, n_dims: int = 8, n_bins: int = 32):\n",
    "        self.n_dims = n_dims\n",
    "        self.n_bins = n_bins\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def estimate(\n",
    "        self,\n",
    "        spikes: torch.Tensor,\n",
    "        teacher_hidden: torch.Tensor,\n",
    "    ) -> float:\n",
    "        \"\"\"Binning-based MI estimation.\"\"\"\n",
    "        # Flatten to 2D\n",
    "        spikes_flat = spikes.reshape(-1, spikes.shape[-1])[:, :self.n_dims]\n",
    "        teacher_flat = teacher_hidden.reshape(-1, teacher_hidden.shape[-1])[:, :self.n_dims]\n",
    "\n",
    "        n_samples = min(spikes_flat.shape[0], 10000)\n",
    "        spikes_flat = spikes_flat[:n_samples].cpu().numpy()\n",
    "        teacher_flat = teacher_flat[:n_samples].cpu().numpy()\n",
    "\n",
    "        # Bin teacher values\n",
    "        teacher_binned = np.zeros_like(teacher_flat, dtype=np.int32)\n",
    "        for d in range(self.n_dims):\n",
    "            col = teacher_flat[:, d]\n",
    "            bins = np.linspace(col.min() - 1e-10, col.max() + 1e-10, self.n_bins + 1)\n",
    "            teacher_binned[:, d] = np.digitize(col, bins) - 1\n",
    "\n",
    "        # Robust ternary discretization by sign (independent of learned amplitude).\n",
    "        spikes_discrete = np.ones_like(spikes_flat, dtype=np.int32)\n",
    "        spikes_discrete[spikes_flat > 1e-6] = 2\n",
    "        spikes_discrete[spikes_flat < -1e-6] = 0\n",
    "\n",
    "        # Compute MI per dimension and average\n",
    "        mi_per_dim = []\n",
    "        for d in range(self.n_dims):\n",
    "            # Joint histogram\n",
    "            joint = np.zeros((3, self.n_bins))\n",
    "            for i in range(n_samples):\n",
    "                s_idx = spikes_discrete[i, d]\n",
    "                t_idx = max(0, min(teacher_binned[i, d], self.n_bins - 1))\n",
    "                joint[s_idx, t_idx] += 1\n",
    "            joint = joint / n_samples + 1e-10\n",
    "\n",
    "            # Marginals\n",
    "            p_s = joint.sum(axis=1, keepdims=True)\n",
    "            p_t = joint.sum(axis=0, keepdims=True)\n",
    "\n",
    "            # MI\n",
    "            mi = np.sum(joint * np.log2(joint / (p_s * p_t + 1e-10)))\n",
    "            mi_per_dim.append(max(0, mi))\n",
    "\n",
    "        return float(np.mean(mi_per_dim))\n",
    "\n",
    "\n",
    "class RepresentationAnalyzer:\n",
    "    \"\"\"Compute CKA between spike patterns and teacher representations.\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def linear_cka(X: np.ndarray, Y: np.ndarray) -> float:\n",
    "        \"\"\"Compute linear CKA similarity.\"\"\"\n",
    "        X = X - X.mean(axis=0)\n",
    "        Y = Y - Y.mean(axis=0)\n",
    "\n",
    "        hsic_xy = np.linalg.norm(X.T @ Y, 'fro') ** 2\n",
    "        hsic_xx = np.linalg.norm(X.T @ X, 'fro') ** 2\n",
    "        hsic_yy = np.linalg.norm(Y.T @ Y, 'fro') ** 2\n",
    "\n",
    "        return float(hsic_xy / (np.sqrt(hsic_xx * hsic_yy) + 1e-10))\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def compute_cka(\n",
    "        self,\n",
    "        spikes: torch.Tensor,\n",
    "        teacher_hidden: torch.Tensor,\n",
    "        max_samples: int = 5000,\n",
    "    ) -> float:\n",
    "        \"\"\"Compute CKA between spikes and teacher hidden states.\"\"\"\n",
    "        spikes_flat = spikes.reshape(-1, spikes.shape[-1])\n",
    "        teacher_flat = teacher_hidden.reshape(-1, teacher_hidden.shape[-1])\n",
    "\n",
    "        n_samples = min(spikes_flat.shape[0], max_samples)\n",
    "        X = spikes_flat[:n_samples].float().cpu().numpy()\n",
    "        Y = teacher_flat[:n_samples].float().cpu().numpy()\n",
    "\n",
    "        return self.linear_cka(X, Y)\n",
    "\n",
    "\n",
    "class SpikingBrainValidator:\n",
    "    \"\"\"Main validator for V15 SpikingBrain validation.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        device: torch.device,\n",
    "        layer_map: Dict[int, int],\n",
    "        dead_threshold: float = 0.05,\n",
    "        saturated_threshold: float = 0.10,\n",
    "        mi_threshold: float = 0.1,\n",
    "        cka_threshold: float = 0.3,\n",
    "        firing_rate_range: Tuple[float, float] = (0.2, 0.6),\n",
    "    ):\n",
    "        self.device = device\n",
    "        self.layer_map = layer_map\n",
    "        self.dead_threshold = dead_threshold\n",
    "        self.saturated_threshold = saturated_threshold\n",
    "        self.mi_threshold = mi_threshold\n",
    "        self.cka_threshold = cka_threshold\n",
    "        self.firing_rate_range = firing_rate_range\n",
    "\n",
    "        self.mi_estimator = MutualInformationEstimator()\n",
    "        self.cka_analyzer = RepresentationAnalyzer()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def validate(\n",
    "        self,\n",
    "        student: torch.nn.Module,\n",
    "        teacher: torch.nn.Module,\n",
    "        dataloader,\n",
    "        max_batches: int = 20,\n",
    "    ) -> SpikingBrainValidation:\n",
    "        \"\"\"Run complete SpikingBrain validation.\"\"\"\n",
    "        student.eval()\n",
    "        teacher.eval()\n",
    "\n",
    "        # Collect spikes and teacher hiddens\n",
    "        all_spikes = {}  # layer_idx -> {'k': [], 'v': []}\n",
    "        all_teacher_hiddens = {}  # teacher_layer -> list of tensors\n",
    "\n",
    "        print(\"Collecting spikes and teacher representations...\")\n",
    "        for batch_idx, batch in enumerate(dataloader):\n",
    "            if batch_idx >= max_batches:\n",
    "                break\n",
    "\n",
    "            if isinstance(batch, dict):\n",
    "                input_ids = batch['input_ids'].to(self.device)\n",
    "            elif isinstance(batch, (list, tuple)):\n",
    "                input_ids = batch[0].to(self.device)\n",
    "            else:\n",
    "                raise TypeError(f\"Unsupported batch type for validation: {type(batch)}\")\n",
    "\n",
    "            # Student forward with spikes\n",
    "            student_out = student(input_ids, return_spike_info=True)\n",
    "            aux = {}\n",
    "            if isinstance(student_out, tuple):\n",
    "                if len(student_out) == 2:\n",
    "                    _, aux = student_out\n",
    "                elif len(student_out) == 3:\n",
    "                    _, _, aux = student_out\n",
    "            spike_info = aux.get('spike_info', {}) if isinstance(aux, dict) else {}\n",
    "\n",
    "            for layer_idx, layer_spikes in spike_info.items():\n",
    "                if layer_idx not in all_spikes:\n",
    "                    all_spikes[layer_idx] = {'k': [], 'v': []}\n",
    "                if isinstance(layer_spikes, dict):\n",
    "                    k_spikes = layer_spikes.get('k_spikes')\n",
    "                    v_spikes = layer_spikes.get('v_spikes')\n",
    "                    if k_spikes is not None:\n",
    "                        all_spikes[layer_idx]['k'].append(k_spikes.cpu())\n",
    "                    if v_spikes is not None:\n",
    "                        all_spikes[layer_idx]['v'].append(v_spikes.cpu())\n",
    "                elif isinstance(layer_spikes, list):\n",
    "                    for s in layer_spikes:\n",
    "                        if isinstance(s, dict):\n",
    "                            if 'k_spikes' in s:\n",
    "                                all_spikes[layer_idx]['k'].append(s['k_spikes'].cpu())\n",
    "                            if 'v_spikes' in s:\n",
    "                                all_spikes[layer_idx]['v'].append(s['v_spikes'].cpu())\n",
    "\n",
    "            # Teacher forward (get hidden states from mapped layers)\n",
    "            with torch.no_grad():\n",
    "                teacher_out = teacher(input_ids, output_hidden_states=True)\n",
    "                for student_layer, teacher_layer in self.layer_map.items():\n",
    "                    if teacher_layer not in all_teacher_hiddens:\n",
    "                        all_teacher_hiddens[teacher_layer] = []\n",
    "                    h = teacher_out.hidden_states[teacher_layer + 1].cpu()\n",
    "                    all_teacher_hiddens[teacher_layer].append(h)\n",
    "\n",
    "        # 1. Compute health metrics\n",
    "        print(\"Computing health metrics...\")\n",
    "        health = self._compute_health(all_spikes)\n",
    "\n",
    "        # 2. Compute MI\n",
    "        print(\"Estimating mutual information...\")\n",
    "        mi_results = self._compute_mi(all_spikes, all_teacher_hiddens)\n",
    "\n",
    "        # 3. Compute CKA\n",
    "        print(\"Computing CKA similarity...\")\n",
    "        cka_results = self._compute_cka(all_spikes, all_teacher_hiddens)\n",
    "\n",
    "        # 4. Overall pass check\n",
    "        overall_pass = (\n",
    "            health.health_pass and\n",
    "            mi_results.get('mutual_information', 0) >= self.mi_threshold and\n",
    "            cka_results.get('cka_mean', 0) >= self.cka_threshold\n",
    "        )\n",
    "\n",
    "        # 5. Generate summary\n",
    "        summary = self._generate_summary(health, mi_results, cka_results, overall_pass)\n",
    "\n",
    "        return SpikingBrainValidation(\n",
    "            health=health,\n",
    "            mutual_information=mi_results,\n",
    "            cka=cka_results,\n",
    "            overall_pass=overall_pass,\n",
    "            summary=summary,\n",
    "        )\n",
    "\n",
    "    def _compute_health(self, all_spikes: Dict[int, Dict[str, List[torch.Tensor]]]) -> SpikeHealthMetrics:\n",
    "        \"\"\"Compute spike health metrics.\"\"\"\n",
    "        dead_indices = {}\n",
    "        saturated_indices = {}\n",
    "        per_channel_rates = {}\n",
    "        all_rates = []\n",
    "\n",
    "        total_dead = 0\n",
    "        total_saturated = 0\n",
    "        total_channels = 0\n",
    "\n",
    "        for layer_idx, layer_spikes in all_spikes.items():\n",
    "            k_list = layer_spikes.get('k', [])\n",
    "            v_list = layer_spikes.get('v', [])\n",
    "            if not k_list and not v_list:\n",
    "                continue\n",
    "\n",
    "            if k_list and v_list:\n",
    "                k_stacked = torch.cat([s.view(-1, s.shape[-1]) for s in k_list], dim=0)\n",
    "                v_stacked = torch.cat([s.view(-1, s.shape[-1]) for s in v_list], dim=0)\n",
    "                active = ((k_stacked != 0) | (v_stacked != 0)).float()\n",
    "            else:\n",
    "                base_list = k_list if k_list else v_list\n",
    "                base = torch.cat([s.view(-1, s.shape[-1]) for s in base_list], dim=0)\n",
    "                active = (base != 0).float()\n",
    "\n",
    "            d_model = active.shape[-1]\n",
    "\n",
    "            # Per-channel firing rates\n",
    "            rates = active.mean(dim=0).numpy()\n",
    "            per_channel_rates[f'layer_{layer_idx}'] = rates\n",
    "            all_rates.append(rates)\n",
    "\n",
    "            # Dead neurons (firing rate < 0.001)\n",
    "            dead_mask = rates < 0.001\n",
    "            dead_indices[f'layer_{layer_idx}'] = np.where(dead_mask)[0]\n",
    "            total_dead += dead_mask.sum()\n",
    "\n",
    "            # Saturated neurons (always fire)\n",
    "            always_active = (active > 0.999).all(dim=0).numpy()\n",
    "            saturated_indices[f'layer_{layer_idx}'] = np.where(always_active)[0]\n",
    "            total_saturated += always_active.sum()\n",
    "\n",
    "            total_channels += d_model\n",
    "\n",
    "        if not all_rates:\n",
    "            return SpikeHealthMetrics(\n",
    "                dead_neuron_pct=1.0,\n",
    "                dead_neuron_indices={},\n",
    "                saturated_neuron_pct=0.0,\n",
    "                saturated_neuron_indices={},\n",
    "                firing_rate_mean=0.0,\n",
    "                firing_rate_std=0.0,\n",
    "                per_channel_rates={},\n",
    "                health_pass=False,\n",
    "                alerts=['No spike tensors captured during validation.'],\n",
    "            )\n",
    "\n",
    "        all_rates_flat = np.concatenate(all_rates)\n",
    "        dead_pct = total_dead / total_channels if total_channels > 0 else 0\n",
    "        saturated_pct = total_saturated / total_channels if total_channels > 0 else 0\n",
    "\n",
    "        # Check health\n",
    "        alerts = []\n",
    "        if dead_pct > self.dead_threshold:\n",
    "            alerts.append(f\"Dead neurons: {dead_pct*100:.1f}% > {self.dead_threshold*100:.0f}%\")\n",
    "        if saturated_pct > self.saturated_threshold:\n",
    "            alerts.append(f\"Saturated neurons: {saturated_pct*100:.1f}% > {self.saturated_threshold*100:.0f}%\")\n",
    "\n",
    "        fr_mean = float(np.mean(all_rates_flat))\n",
    "        if not (self.firing_rate_range[0] <= fr_mean <= self.firing_rate_range[1]):\n",
    "            alerts.append(f\"Firing rate {fr_mean:.3f} outside range {self.firing_rate_range}\")\n",
    "\n",
    "        health_pass = len(alerts) == 0\n",
    "\n",
    "        return SpikeHealthMetrics(\n",
    "            dead_neuron_pct=float(dead_pct),\n",
    "            dead_neuron_indices=dead_indices,\n",
    "            saturated_neuron_pct=float(saturated_pct),\n",
    "            saturated_neuron_indices=saturated_indices,\n",
    "            firing_rate_mean=fr_mean,\n",
    "            firing_rate_std=float(np.std(all_rates_flat)),\n",
    "            per_channel_rates=per_channel_rates,\n",
    "            health_pass=health_pass,\n",
    "            alerts=alerts,\n",
    "        )\n",
    "\n",
    "    def _compute_mi(\n",
    "        self,\n",
    "        all_spikes: Dict[int, Dict[str, List[torch.Tensor]]],\n",
    "        all_teacher_hiddens: Dict[int, List[torch.Tensor]],\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"Compute mutual information.\"\"\"\n",
    "        mi_per_layer = {}\n",
    "\n",
    "        for student_layer, teacher_layer in self.layer_map.items():\n",
    "            if student_layer not in all_spikes or teacher_layer not in all_teacher_hiddens:\n",
    "                continue\n",
    "\n",
    "            k_list = all_spikes[student_layer].get('k', [])\n",
    "            v_list = all_spikes[student_layer].get('v', [])\n",
    "            if not k_list and not v_list:\n",
    "                continue\n",
    "            hiddens = torch.cat(all_teacher_hiddens[teacher_layer], dim=0)\n",
    "\n",
    "            layer_vals = []\n",
    "            if k_list:\n",
    "                mi_k = self.mi_estimator.estimate(torch.cat(k_list, dim=0), hiddens)\n",
    "                mi_per_layer[f'layer_{student_layer}_to_{teacher_layer}_k'] = mi_k\n",
    "                layer_vals.append(mi_k)\n",
    "            if v_list:\n",
    "                mi_v = self.mi_estimator.estimate(torch.cat(v_list, dim=0), hiddens)\n",
    "                mi_per_layer[f'layer_{student_layer}_to_{teacher_layer}_v'] = mi_v\n",
    "                layer_vals.append(mi_v)\n",
    "            if layer_vals:\n",
    "                mi_per_layer[f'layer_{student_layer}_to_{teacher_layer}'] = float(np.mean(layer_vals))\n",
    "\n",
    "        mi_mean = np.mean(list(mi_per_layer.values())) if mi_per_layer else 0.0\n",
    "\n",
    "        return {\n",
    "            **mi_per_layer,\n",
    "            'mutual_information': float(mi_mean),\n",
    "        }\n",
    "\n",
    "    def _compute_cka(\n",
    "        self,\n",
    "        all_spikes: Dict[int, Dict[str, List[torch.Tensor]]],\n",
    "        all_teacher_hiddens: Dict[int, List[torch.Tensor]],\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"Compute CKA similarity.\"\"\"\n",
    "        cka_per_layer = {}\n",
    "\n",
    "        for student_layer, teacher_layer in self.layer_map.items():\n",
    "            if student_layer not in all_spikes or teacher_layer not in all_teacher_hiddens:\n",
    "                continue\n",
    "\n",
    "            k_list = all_spikes[student_layer].get('k', [])\n",
    "            v_list = all_spikes[student_layer].get('v', [])\n",
    "            if not k_list and not v_list:\n",
    "                continue\n",
    "            hiddens = torch.cat(all_teacher_hiddens[teacher_layer], dim=0)\n",
    "\n",
    "            layer_vals = []\n",
    "            if k_list:\n",
    "                cka_k = self.cka_analyzer.compute_cka(torch.cat(k_list, dim=0), hiddens)\n",
    "                cka_per_layer[f'layer_{student_layer}_to_{teacher_layer}_k'] = cka_k\n",
    "                layer_vals.append(cka_k)\n",
    "            if v_list:\n",
    "                cka_v = self.cka_analyzer.compute_cka(torch.cat(v_list, dim=0), hiddens)\n",
    "                cka_per_layer[f'layer_{student_layer}_to_{teacher_layer}_v'] = cka_v\n",
    "                layer_vals.append(cka_v)\n",
    "            if layer_vals:\n",
    "                cka_per_layer[f'layer_{student_layer}_to_{teacher_layer}'] = float(np.mean(layer_vals))\n",
    "\n",
    "        cka_mean = np.mean(list(cka_per_layer.values())) if cka_per_layer else 0.0\n",
    "\n",
    "        return {\n",
    "            **cka_per_layer,\n",
    "            'cka_mean': float(cka_mean),\n",
    "        }\n",
    "\n",
    "    def _generate_summary(\n",
    "        self,\n",
    "        health: SpikeHealthMetrics,\n",
    "        mi_results: Dict[str, float],\n",
    "        cka_results: Dict[str, float],\n",
    "        overall_pass: bool,\n",
    "    ) -> str:\n",
    "        \"\"\"Generate validation summary.\"\"\"\n",
    "        lines = [\n",
    "            \"=\" * 60,\n",
    "            \"SPIKINGBRAIN VALIDATION SUMMARY\",\n",
    "            \"=\" * 60,\n",
    "            \"\",\n",
    "            \"[HEALTH]\",\n",
    "            f\"  Dead neurons: {health.dead_neuron_pct*100:.1f}% {'PASS' if health.dead_neuron_pct < self.dead_threshold else 'FAIL'}\",\n",
    "            f\"  Saturated neurons: {health.saturated_neuron_pct*100:.1f}% {'PASS' if health.saturated_neuron_pct < self.saturated_threshold else 'FAIL'}\",\n",
    "            f\"  Firing rate: {health.firing_rate_mean:.3f} +/- {health.firing_rate_std:.3f}\",\n",
    "            \"\",\n",
    "            \"[INFORMATION]\",\n",
    "            f\"  Mutual Information: {mi_results.get('mutual_information', 0):.4f} {'PASS' if mi_results.get('mutual_information', 0) >= self.mi_threshold else 'FAIL'}\",\n",
    "            \"\",\n",
    "            \"[REPRESENTATION]\",\n",
    "            f\"  CKA (mean): {cka_results.get('cka_mean', 0):.4f} {'PASS' if cka_results.get('cka_mean', 0) >= self.cka_threshold else 'FAIL'}\",\n",
    "            \"\",\n",
    "            \"=\" * 60,\n",
    "            f\"OVERALL: {'PASS - Ready for v16 (sparse ops)' if overall_pass else 'NEEDS ATTENTION'}\",\n",
    "            \"=\" * 60,\n",
    "        ]\n",
    "        return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# RUN VALIDATION\n",
    "# =============================================================================\n",
    "\n",
    "print('='*60)\n",
    "print('V15: SPIKINGBRAIN INFORMATION ENCODING VALIDATION')\n",
    "print('='*60)\n",
    "\n",
    "# Initialize validator with v14 layer mapping\n",
    "validator = SpikingBrainValidator(\n",
    "    device=DEVICE,\n",
    "    layer_map={0: 2, 2: 7, 4: 11},  # Student -> Teacher layer mapping\n",
    "    dead_threshold=0.05,      # Alert if >5% dead neurons\n",
    "    saturated_threshold=0.10,  # Alert if >10% saturated neurons\n",
    "    mi_threshold=0.1,         # Minimum acceptable MI\n",
    "    cka_threshold=0.3,        # Minimum acceptable CKA\n",
    "    firing_rate_range=(0.2, 0.6),  # Healthy firing rate range\n",
    ")\n",
    "\n",
    "# Run validation\n",
    "v15_results = validator.validate(\n",
    "    student=student,\n",
    "    teacher=teacher,\n",
    "    dataloader=val_loader,\n",
    "    max_batches=20,\n",
    ")\n",
    "\n",
    "# Print summary\n",
    "print(v15_results.summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# cell 28: V15 SpikingBrain Visualizations\n",
    "# =============================================================================\n",
    "\n",
    "if not MATPLOTLIB_AVAILABLE:\n",
    "    print(\"matplotlib unavailable: skipping V15 SpikingBrain visualizations.\")\n",
    "else:\n",
    "    def plot_firing_rate_histogram(\n",
    "        firing_rates: np.ndarray,\n",
    "        target_rate: float = 0.38,\n",
    "        title: str = 'Firing Rate Distribution',\n",
    "        show: bool = True,\n",
    "    ):\n",
    "        \"\"\"Plot histogram of per-channel firing rates.\"\"\"\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "        ax.hist(firing_rates, bins=50, alpha=0.7, color='steelblue', edgecolor='black')\n",
    "        ax.axvline(target_rate, color='red', linestyle='--', linewidth=2, label=f'Target: {target_rate}')\n",
    "        ax.axvline(firing_rates.mean(), color='orange', linestyle='-', linewidth=2, label=f'Mean: {firing_rates.mean():.3f}')\n",
    "\n",
    "        # Healthy range shading\n",
    "        ax.axvspan(0.2, 0.6, alpha=0.1, color='green', label='Healthy range [0.2, 0.6]')\n",
    "\n",
    "        ax.set_xlabel('Firing Rate')\n",
    "        ax.set_ylabel('Count')\n",
    "        ax.set_title(title)\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{OUTPUT_DIR}/figures/v15_firing_rate_dist.png', dpi=150, bbox_inches='tight')\n",
    "        if show:\n",
    "            plt.show()\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "    def plot_cka_by_layer(\n",
    "        cka_values: Dict[str, float],\n",
    "        threshold: float = 0.3,\n",
    "        title: str = 'CKA Similarity by Layer',\n",
    "        show: bool = True,\n",
    "    ):\n",
    "        \"\"\"Plot CKA similarity as a bar chart.\"\"\"\n",
    "        # Filter to per-layer values only\n",
    "        layer_cka = {k: v for k, v in cka_values.items() if 'layer_' in k and 'mean' not in k}\n",
    "\n",
    "        if not layer_cka:\n",
    "            print(\"No per-layer CKA values to plot\")\n",
    "            return\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "        names = list(layer_cka.keys())\n",
    "        values = [layer_cka[n] for n in names]\n",
    "        colors = ['green' if v >= threshold else 'red' for v in values]\n",
    "\n",
    "        bars = ax.bar(range(len(names)), values, color=colors, alpha=0.7, edgecolor='black')\n",
    "        ax.axhline(threshold, color='orange', linestyle='--', linewidth=2, label=f'Threshold: {threshold}')\n",
    "        ax.axhline(cka_values.get('cka_mean', 0), color='blue', linestyle='-', linewidth=2, label=f'Mean: {cka_values.get(\"cka_mean\", 0):.3f}')\n",
    "\n",
    "        ax.set_xticks(range(len(names)))\n",
    "        ax.set_xticklabels([n.replace('layer_', 'L').replace('_to_', '->') for n in names], rotation=45, ha='right')\n",
    "        ax.set_ylabel('CKA Similarity')\n",
    "        ax.set_title(title)\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{OUTPUT_DIR}/figures/v15_cka_by_layer.png', dpi=150, bbox_inches='tight')\n",
    "        if show:\n",
    "            plt.show()\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "    # Collect all firing rates from health metrics\n",
    "    all_rates = []\n",
    "    for key, rates in v15_results.health.per_channel_rates.items():\n",
    "        if len(rates) > 0:\n",
    "            all_rates.append(rates)\n",
    "\n",
    "    if all_rates:\n",
    "        combined_rates = np.concatenate(all_rates)\n",
    "\n",
    "        # Firing rate histogram\n",
    "        plot_firing_rate_histogram(\n",
    "            firing_rates=combined_rates,\n",
    "            target_rate=0.38,\n",
    "            title=f'V15 Firing Rate Distribution (mean={combined_rates.mean():.3f})',\n",
    "            show=True,\n",
    "        )\n",
    "\n",
    "    # CKA by layer\n",
    "    if v15_results.cka:\n",
    "        plot_cka_by_layer(\n",
    "            cka_values=v15_results.cka,\n",
    "            threshold=0.3,\n",
    "            title='V15 CKA Similarity: Spikes vs Teacher',\n",
    "            show=True,\n",
    "        )\n",
    "\n",
    "    print(f'\\nVisualizations saved to {OUTPUT_DIR}/figures/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# cell 29: V15 Success Criteria Check\n",
    "# =============================================================================\n",
    "\n",
    "print('='*60)\n",
    "print('V15 SUCCESS CRITERIA')\n",
    "print('='*60)\n",
    "\n",
    "tests = []\n",
    "\n",
    "# Test 1: Dead neurons < 5%\n",
    "dead_pct = v15_results.health.dead_neuron_pct\n",
    "test1 = dead_pct < 0.05\n",
    "tests.append(('Dead neurons < 5%', test1, f'{dead_pct*100:.1f}%'))\n",
    "\n",
    "# Test 2: Saturated neurons < 10%\n",
    "sat_pct = v15_results.health.saturated_neuron_pct\n",
    "test2 = sat_pct < 0.10\n",
    "tests.append(('Saturated neurons < 10%', test2, f'{sat_pct*100:.1f}%'))\n",
    "\n",
    "# Test 3: MI > 0.1\n",
    "mi_val = v15_results.mutual_information.get('mutual_information', 0)\n",
    "test3 = mi_val > 0.1\n",
    "tests.append(('MI > 0.1', test3, f'{mi_val:.4f}'))\n",
    "\n",
    "# Test 4: CKA mean > 0.3\n",
    "cka_mean = v15_results.cka.get('cka_mean', 0)\n",
    "test4 = cka_mean > 0.3\n",
    "tests.append(('CKA mean > 0.3', test4, f'{cka_mean:.4f}'))\n",
    "\n",
    "# Test 5: Firing rate in healthy range [0.2, 0.6]\n",
    "fr_mean = v15_results.health.firing_rate_mean\n",
    "test5 = 0.2 <= fr_mean <= 0.6\n",
    "tests.append(('Firing rate [0.2, 0.6]', test5, f'{fr_mean:.3f}'))\n",
    "\n",
    "# Print results\n",
    "for name, passed, value in tests:\n",
    "    status = 'PASS' if passed else 'FAIL'\n",
    "    print(f'  [{status}] {name}: {value}')\n",
    "\n",
    "all_pass = all(t[1] for t in tests)\n",
    "print(f'\\nOverall: {\"ALL PASS - Ready for v16 (sparse ops)\" if all_pass else \"NEEDS ATTENTION\"}')\n",
    "print('='*60)\n",
    "\n",
    "# Store in results\n",
    "results['v15_spiking_brain'] = {\n",
    "    'validation': v15_results.to_dict(),\n",
    "    'tests': {name: {'passed': passed, 'value': value} for name, passed, value in tests},\n",
    "    'all_pass': all_pass,\n",
    "}\n",
    "\n",
    "print('\\nV15 results added to results dict')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# cell 26: FINAL save + autonomous v15 artifact bundle + single-file dossier\n",
    "# =============================================================================\n",
    "print(\"=\"*60)\n",
    "print(\"FINAL SAVE + AUTONOMY ARTIFACTS (v15)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Add validation_tests to results\n",
    "results['validation_tests'] = validation_results\n",
    "\n",
    "# Save final legacy results json (kept for backward compatibility)\n",
    "results_path = f'{OUTPUT_DIR}/results/results_{RUN_TIMESTAMP}.json'\n",
    "with open(results_path, 'w') as f:\n",
    "    json.dump(results, f, indent=2, default=str)\n",
    "\n",
    "print(f\"saved legacy results: {results_path}\")\n",
    "print(f\"size: {os.path.getsize(results_path) / 1024:.1f} KB\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Build canonical per-run artifact pack for autonomous ingestion\n",
    "# -----------------------------------------------------------------------------\n",
    "run_id = f\"{config.VERSION}_{RUN_TIMESTAMP}\".replace(\" \", \"_\").replace(\":\", \"-\")\n",
    "run_artifact_dir = f\"{OUTPUT_DIR}/{run_id}\"\n",
    "os.makedirs(run_artifact_dir, exist_ok=True)\n",
    "\n",
    "best_ppl = None\n",
    "best_step = None\n",
    "if distill_logs.get('ppl_history'):\n",
    "    best_entry = min(distill_logs['ppl_history'], key=lambda x: x['ppl'])\n",
    "    best_ppl = float(best_entry['ppl'])\n",
    "    best_step = int(best_entry['step'])\n",
    "\n",
    "metrics_payload = {\n",
    "    \"run_id\": run_id,\n",
    "    \"phase\": \"B\",\n",
    "    \"version\": config.VERSION,\n",
    "    \"timestamp_utc\": datetime.utcnow().isoformat() + \"Z\",\n",
    "    \"seed\": int(SEED),\n",
    "    \"teacher_ppl\": float(teacher_ppl),\n",
    "    \"student_ppl\": float(student_ppl),\n",
    "    \"best_student_ppl\": best_ppl,\n",
    "    \"best_step\": best_step,\n",
    "    \"ppl_gap\": float(student_ppl - teacher_ppl),\n",
    "    \"spike_density\": float(student.get_avg_spike_density()),\n",
    "    \"v15_overall_pass\": bool(v15_results.overall_pass),\n",
    "    \"v15_firing_rate_mean\": float(v15_results.health.firing_rate_mean),\n",
    "    \"v15_mutual_information\": float(v15_results.mutual_information.get('mutual_information', 0.0)),\n",
    "    \"v15_cka_mean\": float(v15_results.cka.get('cka_mean', 0.0)),\n",
    "}\n",
    "\n",
    "eval_suite_payload = {\n",
    "    \"schema_version\": \"1.0\",\n",
    "    \"run_id\": run_id,\n",
    "    \"phase\": \"B\",\n",
    "    \"version\": config.VERSION,\n",
    "    \"git_hash\": os.environ.get(\"GIT_COMMIT\", \"unknown\"),\n",
    "    \"seed\": int(SEED),\n",
    "    \"timestamp_utc\": datetime.utcnow().isoformat() + \"Z\",\n",
    "    \"tasks\": {\n",
    "        \"lm_ppl\": {\n",
    "            \"metric\": \"ppl\",\n",
    "            \"value\": float(student_ppl),\n",
    "            \"baseline_reference\": 306.89,\n",
    "        },\n",
    "        \"v15_spike_health\": {\n",
    "            \"dead_neuron_pct\": float(v15_results.health.dead_neuron_pct),\n",
    "            \"saturated_neuron_pct\": float(v15_results.health.saturated_neuron_pct),\n",
    "            \"firing_rate_mean\": float(v15_results.health.firing_rate_mean),\n",
    "            \"pass\": bool(v15_results.health.health_pass),\n",
    "        },\n",
    "        \"v15_information\": {\n",
    "            \"mutual_information\": float(v15_results.mutual_information.get('mutual_information', 0.0)),\n",
    "            \"cka_mean\": float(v15_results.cka.get('cka_mean', 0.0)),\n",
    "            \"overall_pass\": bool(v15_results.overall_pass),\n",
    "        },\n",
    "    },\n",
    "    \"gate_recommendation\": \"green\" if v15_results.overall_pass else \"red\",\n",
    "}\n",
    "\n",
    "v15_payload = {\n",
    "    \"run_id\": run_id,\n",
    "    \"phase\": \"B\",\n",
    "    \"version\": config.VERSION,\n",
    "    \"validation\": v15_results.to_dict(),\n",
    "    \"success_criteria_tests\": {name: {\"passed\": passed, \"value\": value} for name, passed, value in tests},\n",
    "    \"overall_pass\": bool(v15_results.overall_pass),\n",
    "}\n",
    "\n",
    "config_payload = {\n",
    "    \"run_id\": run_id,\n",
    "    \"phase\": \"B\",\n",
    "    \"version\": config.VERSION,\n",
    "    \"version_desc\": config.VERSION_DESC,\n",
    "    \"seed\": int(SEED),\n",
    "    \"platform\": PLATFORM,\n",
    "    \"device\": str(DEVICE),\n",
    "    \"output_dir\": OUTPUT_DIR,\n",
    "    \"run_timestamp\": RUN_TIMESTAMP,\n",
    "    \"config\": asdict(config),\n",
    "}\n",
    "\n",
    "with open(f\"{run_artifact_dir}/metrics.json\", \"w\") as f:\n",
    "    json.dump(metrics_payload, f, indent=2, default=str)\n",
    "\n",
    "with open(f\"{run_artifact_dir}/eval_suite.json\", \"w\") as f:\n",
    "    json.dump(eval_suite_payload, f, indent=2, default=str)\n",
    "\n",
    "with open(f\"{run_artifact_dir}/v15_spikingbrain.json\", \"w\") as f:\n",
    "    json.dump(v15_payload, f, indent=2, default=str)\n",
    "\n",
    "with open(f\"{run_artifact_dir}/seed.txt\", \"w\") as f:\n",
    "    f.write(str(SEED) + \"\\n\")\n",
    "\n",
    "config_yaml_path = f\"{run_artifact_dir}/config.yaml\"\n",
    "try:\n",
    "    import yaml\n",
    "    with open(config_yaml_path, \"w\") as f:\n",
    "        yaml.safe_dump(config_payload, f, sort_keys=False)\n",
    "except Exception as e:\n",
    "    # Fallback: keep required artifact name, store JSON-formatted content.\n",
    "    with open(config_yaml_path, \"w\") as f:\n",
    "        f.write(json.dumps(config_payload, indent=2, default=str))\n",
    "    print(f\"warning: yaml export fallback used ({e})\")\n",
    "\n",
    "# Copy key legacy outputs into artifact bundle\n",
    "try:\n",
    "    import shutil\n",
    "    if os.path.exists(results_path):\n",
    "        shutil.copy2(results_path, f\"{run_artifact_dir}/results.json\")\n",
    "except Exception as e:\n",
    "    print(f\"warning: could not copy legacy outputs: {e}\")\n",
    "\n",
    "print(\"\")\n",
    "print(\"Canonical artifacts written:\")\n",
    "print(f\"  {run_artifact_dir}/eval_suite.json\")\n",
    "print(f\"  {run_artifact_dir}/metrics.json\")\n",
    "print(f\"  {run_artifact_dir}/config.yaml\")\n",
    "print(f\"  {run_artifact_dir}/seed.txt\")\n",
    "print(f\"  {run_artifact_dir}/v15_spikingbrain.json\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Build a single-file detailed dossier (HTML with embedded figures and raw data)\n",
    "# -----------------------------------------------------------------------------\n",
    "import base64\n",
    "import math\n",
    "import statistics\n",
    "from io import BytesIO\n",
    "from html import escape as html_escape\n",
    "\n",
    "single_file_path = f\"{run_artifact_dir}/run_dossier_{run_id}.html\"\n",
    "single_file_primary_output = single_file_path\n",
    "\n",
    "def _series(history, value_key):\n",
    "    xs, ys = [], []\n",
    "    for row in history or []:\n",
    "        if not isinstance(row, dict):\n",
    "            continue\n",
    "        if 'step' not in row or value_key not in row:\n",
    "            continue\n",
    "        try:\n",
    "            y = float(row[value_key])\n",
    "            x = int(row['step'])\n",
    "        except Exception:\n",
    "            continue\n",
    "        if math.isfinite(y):\n",
    "            xs.append(x)\n",
    "            ys.append(y)\n",
    "    return xs, ys\n",
    "\n",
    "def _stats(values):\n",
    "    if not values:\n",
    "        return {}\n",
    "    return {\n",
    "        \"count\": len(values),\n",
    "        \"min\": float(min(values)),\n",
    "        \"max\": float(max(values)),\n",
    "        \"mean\": float(sum(values) / len(values)),\n",
    "        \"std\": float(statistics.pstdev(values)) if len(values) > 1 else 0.0,\n",
    "        \"last\": float(values[-1]),\n",
    "    }\n",
    "\n",
    "def _moving_avg(values, window=100):\n",
    "    if not values:\n",
    "        return []\n",
    "    out = []\n",
    "    for i in range(len(values)):\n",
    "        lo = max(0, i - window + 1)\n",
    "        chunk = values[lo:i+1]\n",
    "        out.append(sum(chunk) / len(chunk))\n",
    "    return out\n",
    "\n",
    "detailed_figures = {}\n",
    "detailed_metrics = {}\n",
    "\n",
    "if MATPLOTLIB_AVAILABLE:\n",
    "    def _save_fig(fig, name):\n",
    "        # Single-file mode: keep figures embedded only; do not emit sidecar PNGs.\n",
    "        buf = BytesIO()\n",
    "        fig.savefig(buf, format='png', dpi=180, bbox_inches='tight')\n",
    "        buf.seek(0)\n",
    "        b64 = base64.b64encode(buf.read()).decode('utf-8')\n",
    "        return \"embedded\", b64\n",
    "\n",
    "    def _plot_lines(name, title, xlabel, ylabel, lines, logy=False):\n",
    "        fig, ax = plt.subplots(figsize=(12, 5))\n",
    "        any_line = False\n",
    "        for line in lines:\n",
    "            label = line.get('label')\n",
    "            xs = line.get('x', [])\n",
    "            ys = line.get('y', [])\n",
    "            color = line.get('color')\n",
    "            if xs and ys:\n",
    "                ax.plot(xs, ys, label=label, linewidth=1.5, color=color)\n",
    "                any_line = True\n",
    "        if not any_line:\n",
    "            plt.close(fig)\n",
    "            return\n",
    "        if logy:\n",
    "            ax.set_yscale('log')\n",
    "        ax.set_title(title)\n",
    "        ax.set_xlabel(xlabel)\n",
    "        ax.set_ylabel(ylabel)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.legend()\n",
    "        path, b64 = _save_fig(fig, name)\n",
    "        detailed_figures[name] = {\"title\": title, \"path\": path, \"base64\": b64}\n",
    "        plt.close(fig)\n",
    "\n",
    "    tc = results.get('training_curves', {})\n",
    "\n",
    "    loss_x, loss_y = _series(tc.get('loss_history', []), 'loss')\n",
    "    kl_x, kl_y = _series(tc.get('kl_loss_history', []), 'loss')\n",
    "    align_x, align_y = _series(tc.get('align_loss_history', []), 'loss')\n",
    "    ce_x, ce_y = _series(tc.get('ce_loss_history', []), 'loss')\n",
    "    fdd_x, fdd_y = _series(tc.get('fdd_loss_history', []), 'loss')\n",
    "    ppl_x, ppl_y = _series(tc.get('ppl_history', []), 'ppl')\n",
    "    lr_x, lr_y = _series(tc.get('lr_history', []), 'lr')\n",
    "    temp_x, temp_y = _series(tc.get('temp_history', []), 'temperature')\n",
    "    lam_x, lam_y = _series(tc.get('temp_history', []), 'lambda')\n",
    "    if not lam_y:\n",
    "        lam_x, lam_y = _series(tc.get('lambda_history', []), 'lambda')\n",
    "\n",
    "    detailed_metrics[\"loss\"] = _stats(loss_y)\n",
    "    detailed_metrics[\"kl_loss\"] = _stats(kl_y)\n",
    "    detailed_metrics[\"align_loss\"] = _stats(align_y)\n",
    "    detailed_metrics[\"ce_loss\"] = _stats(ce_y)\n",
    "    detailed_metrics[\"fdd_loss\"] = _stats(fdd_y)\n",
    "    detailed_metrics[\"ppl\"] = _stats(ppl_y)\n",
    "    detailed_metrics[\"lr\"] = _stats(lr_y)\n",
    "    detailed_metrics[\"temperature\"] = _stats(temp_y)\n",
    "    detailed_metrics[\"lambda\"] = _stats(lam_y)\n",
    "\n",
    "    _plot_lines(\n",
    "        name=\"01_loss_components\",\n",
    "        title=\"Loss Components Over Steps\",\n",
    "        xlabel=\"step\",\n",
    "        ylabel=\"loss\",\n",
    "        lines=[\n",
    "            {\"label\": \"total_loss\", \"x\": loss_x, \"y\": loss_y, \"color\": \"black\"},\n",
    "            {\"label\": \"kl_loss\", \"x\": kl_x, \"y\": kl_y, \"color\": \"tab:blue\"},\n",
    "            {\"label\": \"ce_loss\", \"x\": ce_x, \"y\": ce_y, \"color\": \"tab:orange\"},\n",
    "            {\"label\": \"fdd_loss\", \"x\": fdd_x, \"y\": fdd_y, \"color\": \"tab:green\"},\n",
    "            {\"label\": \"align_loss\", \"x\": align_x, \"y\": align_y, \"color\": \"tab:red\"},\n",
    "        ],\n",
    "    )\n",
    "    _plot_lines(\n",
    "        name=\"02_loss_components_log\",\n",
    "        title=\"Loss Components Over Steps (log scale)\",\n",
    "        xlabel=\"step\",\n",
    "        ylabel=\"loss\",\n",
    "        lines=[\n",
    "            {\"label\": \"total_loss\", \"x\": loss_x, \"y\": loss_y, \"color\": \"black\"},\n",
    "            {\"label\": \"kl_loss\", \"x\": kl_x, \"y\": kl_y, \"color\": \"tab:blue\"},\n",
    "            {\"label\": \"ce_loss\", \"x\": ce_x, \"y\": ce_y, \"color\": \"tab:orange\"},\n",
    "            {\"label\": \"fdd_loss\", \"x\": fdd_x, \"y\": fdd_y, \"color\": \"tab:green\"},\n",
    "            {\"label\": \"align_loss\", \"x\": align_x, \"y\": align_y, \"color\": \"tab:red\"},\n",
    "        ],\n",
    "        logy=True,\n",
    "    )\n",
    "\n",
    "    loss_ma = _moving_avg(loss_y, window=100)\n",
    "    _plot_lines(\n",
    "        name=\"03_total_loss_smoothed\",\n",
    "        title=\"Total Loss (raw + moving average)\",\n",
    "        xlabel=\"step\",\n",
    "        ylabel=\"loss\",\n",
    "        lines=[\n",
    "            {\"label\": \"total_loss_raw\", \"x\": loss_x, \"y\": loss_y, \"color\": \"lightgray\"},\n",
    "            {\"label\": \"total_loss_ma100\", \"x\": loss_x, \"y\": loss_ma, \"color\": \"black\"},\n",
    "        ],\n",
    "    )\n",
    "    _plot_lines(\n",
    "        name=\"04_ppl_curve\",\n",
    "        title=\"Validation PPL Over Eval Steps\",\n",
    "        xlabel=\"step\",\n",
    "        ylabel=\"ppl\",\n",
    "        lines=[{\"label\": \"val_ppl\", \"x\": ppl_x, \"y\": ppl_y, \"color\": \"tab:purple\"}],\n",
    "    )\n",
    "    _plot_lines(\n",
    "        name=\"05_learning_rate\",\n",
    "        title=\"Learning Rate Schedule\",\n",
    "        xlabel=\"step\",\n",
    "        ylabel=\"lr\",\n",
    "        lines=[{\"label\": \"lr\", \"x\": lr_x, \"y\": lr_y, \"color\": \"tab:green\"}],\n",
    "    )\n",
    "    _plot_lines(\n",
    "        name=\"06_temperature\",\n",
    "        title=\"CTKD Temperature\",\n",
    "        xlabel=\"step\",\n",
    "        ylabel=\"temperature\",\n",
    "        lines=[{\"label\": \"temperature\", \"x\": temp_x, \"y\": temp_y, \"color\": \"tab:orange\"}],\n",
    "    )\n",
    "    _plot_lines(\n",
    "        name=\"07_lambda\",\n",
    "        title=\"CTKD Lambda / GRL Strength\",\n",
    "        xlabel=\"step\",\n",
    "        ylabel=\"lambda\",\n",
    "        lines=[{\"label\": \"lambda\", \"x\": lam_x, \"y\": lam_y, \"color\": \"tab:red\"}],\n",
    "    )\n",
    "\n",
    "    # Spike summary figures\n",
    "    spike_summary = results.get('spike_analysis', {})\n",
    "    per_layer = spike_summary.get('per_layer', {})\n",
    "    if per_layer:\n",
    "        layer_names = sorted(per_layer.keys(), key=lambda x: int(x.split('_')[-1]))\n",
    "        k_density = [float(per_layer[n].get('k_final', 0.0)) for n in layer_names]\n",
    "        v_density = [float(per_layer[n].get('v_final', 0.0)) for n in layer_names]\n",
    "        k_amp = [float(per_layer[n].get('k_amp_final', 0.0)) for n in layer_names]\n",
    "        v_amp = [float(per_layer[n].get('v_amp_final', 0.0)) for n in layer_names]\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(12, 5))\n",
    "        x = np.arange(len(layer_names))\n",
    "        ax.bar(x - 0.2, k_density, 0.4, label='k_density')\n",
    "        ax.bar(x + 0.2, v_density, 0.4, label='v_density')\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(layer_names, rotation=30)\n",
    "        ax.set_title(\"Per-layer Spike Density\")\n",
    "        ax.set_ylabel(\"density\")\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.legend()\n",
    "        path, b64 = _save_fig(fig, \"08_spike_density_per_layer\")\n",
    "        detailed_figures[\"08_spike_density_per_layer\"] = {\"title\": \"Per-layer Spike Density\", \"path\": path, \"base64\": b64}\n",
    "        plt.close(fig)\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(12, 5))\n",
    "        x = np.arange(len(layer_names))\n",
    "        ax.bar(x - 0.2, k_amp, 0.4, label='k_amplitude')\n",
    "        ax.bar(x + 0.2, v_amp, 0.4, label='v_amplitude')\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(layer_names, rotation=30)\n",
    "        ax.set_title(\"Per-layer Spike Amplitude\")\n",
    "        ax.set_ylabel(\"amplitude\")\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.legend()\n",
    "        path, b64 = _save_fig(fig, \"09_spike_amplitude_per_layer\")\n",
    "        detailed_figures[\"09_spike_amplitude_per_layer\"] = {\"title\": \"Per-layer Spike Amplitude\", \"path\": path, \"base64\": b64}\n",
    "        plt.close(fig)\n",
    "\n",
    "    # Density history timeline\n",
    "    density_history = spike_summary.get('density_history', [])\n",
    "    if density_history:\n",
    "        dx = []\n",
    "        dy = []\n",
    "        for row in density_history:\n",
    "            if isinstance(row, dict) and 'step' in row and 'density' in row:\n",
    "                try:\n",
    "                    xv = int(row['step'])\n",
    "                    yv = float(row['density'])\n",
    "                except Exception:\n",
    "                    continue\n",
    "                if math.isfinite(yv):\n",
    "                    dx.append(xv)\n",
    "                    dy.append(yv)\n",
    "        _plot_lines(\n",
    "            name=\"10_overall_spike_density_timeline\",\n",
    "            title=\"Overall Spike Density Timeline\",\n",
    "            xlabel=\"step\",\n",
    "            ylabel=\"density\",\n",
    "            lines=[{\"label\": \"overall_density\", \"x\": dx, \"y\": dy, \"color\": \"tab:blue\"}],\n",
    "        )\n",
    "\n",
    "    # TTT loss\n",
    "    ttt = results.get('ttt', {})\n",
    "    ttt_x, ttt_y = _series(ttt.get('loss_history', []), 'loss')\n",
    "    _plot_lines(\n",
    "        name=\"11_ttt_loss\",\n",
    "        title=\"TTT LoRA Loss\",\n",
    "        xlabel=\"step\",\n",
    "        ylabel=\"loss\",\n",
    "        lines=[{\"label\": \"ttt_loss\", \"x\": ttt_x, \"y\": ttt_y, \"color\": \"tab:brown\"}],\n",
    "    )\n",
    "\n",
    "    # Validation test pass/fail chart\n",
    "    test_rows = validation_results.get('tests', []) if isinstance(validation_results, dict) else []\n",
    "    if test_rows:\n",
    "        names = [str(t[0]) for t in test_rows]\n",
    "        vals = [1 if bool(t[1]) else 0 for t in test_rows]\n",
    "        fig, ax = plt.subplots(figsize=(14, 6))\n",
    "        x = np.arange(len(names))\n",
    "        colors = ['tab:green' if v == 1 else 'tab:red' for v in vals]\n",
    "        ax.bar(x, vals, color=colors)\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(names, rotation=45, ha='right')\n",
    "        ax.set_ylim(0, 1.2)\n",
    "        ax.set_title(\"Validation Test Outcomes\")\n",
    "        ax.set_ylabel(\"pass (1) / fail (0)\")\n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "        path, b64 = _save_fig(fig, \"12_validation_tests\")\n",
    "        detailed_figures[\"12_validation_tests\"] = {\"title\": \"Validation Test Outcomes\", \"path\": path, \"base64\": b64}\n",
    "        plt.close(fig)\n",
    "\n",
    "    # Hardware summary chart\n",
    "    hw = results.get('hardware_stats', {})\n",
    "    hw_labels = []\n",
    "    hw_vals = []\n",
    "    for k in [\"peak_gpu_memory_gb\", \"avg_gpu_memory_gb\", \"throughput_tokens_per_sec\", \"total_training_time_min\"]:\n",
    "        if k in hw:\n",
    "            try:\n",
    "                hw_labels.append(k)\n",
    "                hw_vals.append(float(hw[k]))\n",
    "            except Exception:\n",
    "                pass\n",
    "    if hw_labels:\n",
    "        fig, ax = plt.subplots(figsize=(12, 5))\n",
    "        x = np.arange(len(hw_labels))\n",
    "        ax.bar(x, hw_vals, color='tab:cyan')\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(hw_labels, rotation=20, ha='right')\n",
    "        ax.set_title(\"Hardware / Runtime Summary\")\n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "        path, b64 = _save_fig(fig, \"13_hardware_summary\")\n",
    "        detailed_figures[\"13_hardware_summary\"] = {\"title\": \"Hardware / Runtime Summary\", \"path\": path, \"base64\": b64}\n",
    "        plt.close(fig)\n",
    "else:\n",
    "    print(\"matplotlib unavailable: detailed figure generation skipped in single-file dossier.\")\n",
    "\n",
    "# Include legacy training plot if present\n",
    "legacy_plot = results.get('figures', {}).get('training_plot', {})\n",
    "if isinstance(legacy_plot, dict) and isinstance(legacy_plot.get('base64'), str) and legacy_plot['base64']:\n",
    "    detailed_figures[\"00_legacy_training_plot\"] = {\n",
    "        \"title\": \"Legacy Training Plot\",\n",
    "        \"path\": legacy_plot.get(\"filename\", \"legacy_training_plot.png\"),\n",
    "        \"base64\": legacy_plot[\"base64\"],\n",
    "    }\n",
    "\n",
    "consolidated_payload = {\n",
    "    \"schema_version\": \"1.0\",\n",
    "    \"run_id\": run_id,\n",
    "    \"timestamp_utc\": datetime.utcnow().isoformat() + \"Z\",\n",
    "    \"phase\": \"B\",\n",
    "    \"summary_metrics\": metrics_payload,\n",
    "    \"eval_suite\": eval_suite_payload,\n",
    "    \"v15_validation\": v15_payload,\n",
    "    \"curve_stats\": detailed_metrics,\n",
    "    \"detailed_figures_index\": {\n",
    "        k: {\"title\": v.get(\"title\"), \"path\": v.get(\"path\")}\n",
    "        for k, v in detailed_figures.items()\n",
    "    },\n",
    "}\n",
    "\n",
    "def _json_block(obj):\n",
    "    return html_escape(json.dumps(obj, indent=2, default=str))\n",
    "\n",
    "summary_rows = \"\".join(\n",
    "    f\"<tr><td>{html_escape(str(k))}</td><td>{html_escape(str(v))}</td></tr>\"\n",
    "    for k, v in metrics_payload.items()\n",
    ")\n",
    "\n",
    "curve_rows = \"\"\n",
    "for name, stats in detailed_metrics.items():\n",
    "    if not stats:\n",
    "        continue\n",
    "    curve_rows += (\n",
    "        f\"<tr><td>{html_escape(name)}</td>\"\n",
    "        f\"<td>{stats.get('count')}</td>\"\n",
    "        f\"<td>{stats.get('min')}</td>\"\n",
    "        f\"<td>{stats.get('max')}</td>\"\n",
    "        f\"<td>{stats.get('mean')}</td>\"\n",
    "        f\"<td>{stats.get('std')}</td>\"\n",
    "        f\"<td>{stats.get('last')}</td></tr>\"\n",
    "    )\n",
    "\n",
    "fig_blocks = \"\"\n",
    "for name, meta in sorted(detailed_figures.items()):\n",
    "    b64 = meta.get(\"base64\", \"\")\n",
    "    title = meta.get(\"title\", name)\n",
    "    fig_blocks += (\n",
    "        f\"<h3>{html_escape(title)}</h3>\"\n",
    "        f\"<p><code>{html_escape(name)}</code></p>\"\n",
    "        f\"<img src='data:image/png;base64,{b64}' style='max-width:100%;border:1px solid #ddd;padding:6px;background:#fff;'/>\"\n",
    "    )\n",
    "\n",
    "html_report = f\"\"\"<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "  <meta charset=\"utf-8\"/>\n",
    "  <title>Gerhard V15 Dossier - {html_escape(run_id)}</title>\n",
    "  <style>\n",
    "    body {{ font-family: Arial, sans-serif; margin: 24px; line-height: 1.45; color: #111; }}\n",
    "    h1, h2, h3 {{ margin-top: 24px; }}\n",
    "    table {{ border-collapse: collapse; width: 100%; margin: 12px 0; }}\n",
    "    th, td {{ border: 1px solid #ccc; padding: 8px; text-align: left; font-size: 13px; }}\n",
    "    th {{ background: #f3f5f7; }}\n",
    "    code, pre {{ background: #f5f5f5; padding: 2px 4px; }}\n",
    "    pre {{ padding: 12px; overflow-x: auto; }}\n",
    "    details {{ margin: 10px 0; }}\n",
    "  </style>\n",
    "</head>\n",
    "<body>\n",
    "  <h1>Gerhard V15 Single-File Dossier</h1>\n",
    "  <p><b>Run ID:</b> {html_escape(run_id)}<br/>\n",
    "     <b>Generated:</b> {html_escape(datetime.utcnow().isoformat() + \"Z\")}<br/>\n",
    "     <b>Phase:</b> B (v15 SpikingBrain validation)</p>\n",
    "\n",
    "  <h2>Executive Summary Metrics</h2>\n",
    "  <table>\n",
    "    <tr><th>Metric</th><th>Value</th></tr>\n",
    "    {summary_rows}\n",
    "  </table>\n",
    "\n",
    "  <h2>Curve Statistics</h2>\n",
    "  <table>\n",
    "    <tr>\n",
    "      <th>Curve</th><th>Count</th><th>Min</th><th>Max</th><th>Mean</th><th>Std</th><th>Last</th>\n",
    "    </tr>\n",
    "    {curve_rows}\n",
    "  </table>\n",
    "\n",
    "  <h2>Detailed Figures</h2>\n",
    "  {fig_blocks}\n",
    "\n",
    "  <h2>Raw Data (Embedded)</h2>\n",
    "  <details>\n",
    "    <summary>Consolidated payload</summary>\n",
    "    <pre>{_json_block(consolidated_payload)}</pre>\n",
    "  </details>\n",
    "  <details>\n",
    "    <summary>results.json snapshot</summary>\n",
    "    <pre>{_json_block(results)}</pre>\n",
    "  </details>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "with open(single_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(html_report)\n",
    "\n",
    "single_file_size_mb = os.path.getsize(single_file_path) / (1024 * 1024)\n",
    "print(\"\")\n",
    "print(f\"Single-file dossier saved: {single_file_path} ({single_file_size_mb:.2f} MB)\")\n",
    "print(f\"Primary one-file output: {single_file_primary_output}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Optional automatic registration into repo autonomous state/report system\n",
    "# -----------------------------------------------------------------------------\n",
    "registration_result = None\n",
    "registration_error = None\n",
    "repo_root = None\n",
    "\n",
    "candidate_roots = [\n",
    "    Path.cwd(),\n",
    "    Path.cwd().parent,\n",
    "    Path('/workspace/gerhard'),\n",
    "    Path('/kaggle/working/gerhard'),\n",
    "]\n",
    "\n",
    "for candidate in candidate_roots:\n",
    "    if (candidate / 'scripts' / 'register_notebook_run.py').exists():\n",
    "        repo_root = candidate\n",
    "        break\n",
    "\n",
    "if repo_root is not None:\n",
    "    try:\n",
    "        if str(repo_root) not in sys.path:\n",
    "            sys.path.append(str(repo_root))\n",
    "        from scripts.register_notebook_run import register_run\n",
    "\n",
    "        registration_result = register_run(\n",
    "            run_id=run_id,\n",
    "            phase='B',\n",
    "            source_dir=Path(run_artifact_dir),\n",
    "            repo_root=repo_root,\n",
    "            summary='v15 notebook pass with canonical artifact bundle and single-file dossier',\n",
    "            next_action='Proceed according to gate decision (continue on green, pause on red).',\n",
    "        )\n",
    "        print(\"\")\n",
    "        print(\"Autonomous registration complete:\")\n",
    "        print(registration_result)\n",
    "    except Exception as e:\n",
    "        registration_error = str(e)\n",
    "        print(\"\")\n",
    "        print(f\"Autonomous registration skipped due to error: {registration_error}\")\n",
    "else:\n",
    "    registration_error = (\n",
    "        \"register_notebook_run.py not found under candidate repo roots. \"\n",
    "        \"Run registration manually later with this source dir.\"\n",
    "    )\n",
    "    print(\"\")\n",
    "    print(\"Autonomous registration helper not found in this environment.\")\n",
    "    print(f\"Manual source dir for later registration: {run_artifact_dir}\")\n",
    "\n",
    "results['autonomy_artifacts'] = {\n",
    "    'run_id': run_id,\n",
    "    'artifact_dir': run_artifact_dir,\n",
    "    'single_file_dossier': single_file_path,\n",
    "    'single_file_primary_output': single_file_primary_output,\n",
    "    'required_files': [\n",
    "        'eval_suite.json',\n",
    "        'metrics.json',\n",
    "        'config.yaml',\n",
    "        'seed.txt',\n",
    "        'v15_spikingbrain.json',\n",
    "        f'run_dossier_{run_id}.html',\n",
    "    ],\n",
    "    'registration_result': registration_result,\n",
    "    'registration_error': registration_error,\n",
    "}\n",
    "\n",
    "# update legacy results snapshot with autonomy metadata\n",
    "with open(results_path, 'w') as f:\n",
    "    json.dump(results, f, indent=2, default=str)\n",
    "\n",
    "def _attempt_auto_download(path: str):\n",
    "    if IS_COLAB:\n",
    "        try:\n",
    "            from google.colab import files\n",
    "            files.download(path)\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"colab download failed: {e}\")\n",
    "            return False\n",
    "    # Try Jupyter front-end auto-open in non-colab notebook environments\n",
    "    try:\n",
    "        from IPython.display import Javascript, display\n",
    "        abs_path = os.path.abspath(path).replace(\"\\\\\", \"/\")\n",
    "        display(Javascript(f\"window.open('/files/{abs_path}', '_blank');\"))\n",
    "        print(f\"triggered browser download/open for: /files/{abs_path}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"non-colab auto-download not available: {e}\")\n",
    "        return False\n",
    "\n",
    "print(\"\")\n",
    "print(\"Auto-download single-file dossier\")\n",
    "_attempt_auto_download(single_file_primary_output)\n",
    "print(f\"single-file dossier path: {single_file_primary_output}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}