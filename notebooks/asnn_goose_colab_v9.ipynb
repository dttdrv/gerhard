{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "cell-0",
   "source": "# asnn-goose v9: model capacity increase (320d × 5L)\n\n## abstract\n\nv8 achieved **PPL 559** (11% improvement over v6's 627). v9 increases model capacity to reduce PPL further toward our target of **440**.\n\n**v8 results (baseline for v9):**\n- teacher ppl: 44.6\n- student ppl: **559** (target was <627 ✅)\n- tests: 8/8 passed\n- amplitudes: 0.74-1.08 (learned)\n- spike density: 0.384\n\n**v9 strategy: increase capacity**\n\n| attribute | v8 | v9 | change |\n|-----------|-----|-----|--------|\n| d_model | 256 | **320** | +25% |\n| n_layers | 4 | **5** | +25% |\n| params | ~16M | **~30M** | +87% |\n| VRAM | ~1.5GB | **~2.5GB** | +67% |\n\n**expected:**\n- student ppl: **<520** (from 559)\n- tests: 9/9 passed (new: amplitude health)\n- VRAM: <8GB on T4 (16GB available)\n\n---\n\n**eptesicus laboratories - lumis-next initiative**\n\n### references\n- hinton et al. (2015) \"distilling the knowledge in a neural network\"\n- radford et al. (2019) \"language models are unsupervised multitask learners\" (gpt-2)\n- lv et al. (2023) \"spikebert: a language spikformer learned from bert with knowledge distillation\"\n- shen et al. (2024) \"spikingmamba: towards energy-efficient large language models\"\n- wei et al. (2023) \"ternary spike: learning ternary spikes for spiking neural networks\"\n- hu et al. (2022) \"lora: low-rank adaptation of large language models\""
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "id": "cell-1",
   "source": "# =============================================================================\n# cell 1: environment setup\n# =============================================================================\nimport os\nos.environ['TOKENIZERS_PARALLELISM'] = 'false'\n\nimport sys\nimport time\nimport math\nimport json\nimport base64\nfrom pathlib import Path\nfrom datetime import datetime\nfrom dataclasses import dataclass, asdict\nfrom typing import Dict, List, Optional, Tuple, Any\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# generate timestamp for this run\nRUN_TIMESTAMP = datetime.now().strftime('%Y-%m-%d_%H%M%S')\nprint(f\"run timestamp: {RUN_TIMESTAMP}\")\n\n# detect platform\nIS_KAGGLE = 'KAGGLE_KERNEL_RUN_TYPE' in os.environ\nIS_COLAB = 'COLAB_GPU' in os.environ or 'google.colab' in sys.modules\nPLATFORM = 'kaggle' if IS_KAGGLE else 'colab' if IS_COLAB else 'local'\nOUTPUT_DIR = '/kaggle/working/outputs' if IS_KAGGLE else 'outputs'\n\nfor subdir in ['figures', 'checkpoints', 'logs', 'results']:\n    os.makedirs(f'{OUTPUT_DIR}/{subdir}', exist_ok=True)\n\nprint(f\"platform: {PLATFORM}\")\nprint(f\"output directory: {OUTPUT_DIR}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "id": "cell-2",
   "source": "# =============================================================================\n# cell 2: pytorch and hardware setup\n# =============================================================================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, TensorDataset\nimport numpy as np\n\nimport matplotlib\nmatplotlib.use('Agg')\nimport matplotlib.pyplot as plt\n\nfrom tqdm.auto import tqdm\n\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nSEED = 42\n\ntorch.manual_seed(SEED)\nnp.random.seed(SEED)\n\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(SEED)\n    torch.backends.cuda.matmul.allow_tf32 = True\n    torch.backends.cudnn.allow_tf32 = True\n    torch.backends.cudnn.benchmark = True\n\n    gpu_name = torch.cuda.get_device_name(0)\n    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n    print(f\"gpu: {gpu_name}\")\n    print(f\"memory: {gpu_memory:.1f} gb\")\n\nprint(f\"device: {DEVICE}\")\nprint(f\"pytorch: {torch.__version__}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "cell-3",
   "source": "## 1. v9 design: model capacity increase\n\n### 1.1 rationale for capacity increase\n\nv8 achieved PPL 559 with a 16M parameter model. Research shows that model capacity is often the primary bottleneck for distillation quality. We increase:\n\n- **d_model**: 256 → 320 (+25%)\n- **n_layers**: 4 → 5 (+25%)\n- **params**: ~16M → ~30M (+87%)\n\n### 1.2 memory analysis\n\n```\n30M params = ~120MB (FP32) or ~60MB (FP16)\nOptimizer states (Adam) = ~240MB\nGradients = ~60MB (FP16)\nActivations = ~2GB (batch 8, seq 256, 5 layers, 320 dim)\nTotal = ~2.5GB\n\nT4 has 16GB → 13.5GB headroom (safe margin)\n```\n\n### 1.3 v9 changes from v8\n\n| component | v8 | v9 | action |\n|-----------|-----|-----|--------|\n| d_model | 256 | 320 | **increase** |\n| n_layers | 4 | 5 | **increase** |\n| teacher_indices | [3,6,9,12] | [2,5,7,10,12] | **remap** |\n| params | ~16M | ~30M | **result** |\n| temperature | 2.0 | 2.0 | keep |\n| hidden_align_weight | 0.0 | 0.0 | keep |\n| warmup_steps | 50 | 50 | keep |\n\n### 1.4 trainable ternary spike (from v8)\n\n$$s = a \\cdot \\text{sign}(x, \\theta)$$\n\nwhere $a$ is learnable per layer. gradient flow:\n- $\\partial L / \\partial x = \\partial L / \\partial s$ (ste)\n- $\\partial L / \\partial a = \\partial L / \\partial s \\cdot \\text{sign}$\n\n### 1.5 hidden-state alignment (optional, disabled by default)\n\n$$\\mathcal{L}_{\\text{align}} = \\frac{1}{L} \\sum_{l=1}^{L} \\| f(h^{(s)}_l) - h^{(t)}_l \\|_2^2$$\n\nkept for future experiments with reduced weight (0.001-0.01 range)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "id": "cell-4",
   "source": "# =============================================================================\n# cell 4: configuration (v9 - model capacity increase)\n# =============================================================================\n@dataclass\nclass Config:\n    # gpt-2 teacher (frozen, pre-trained)\n    teacher_name: str = \"gpt2\"\n\n    # student model architecture - v9: INCREASED CAPACITY\n    d_model: int = 320      # v8: 256 -> v9: 320 (+25%)\n    n_layers: int = 5       # v8: 4 -> v9: 5 (+25%)\n    vocab_size: int = 50257\n    max_seq_len: int = 256\n\n    # distillation training (same as v8)\n    distill_steps: int = 3000\n    distill_lr: float = 3e-4\n    temperature: float = 2.0      # proven in v6, v8\n    warmup_steps: int = 50        # minimal warmup\n\n    # hidden-state alignment (DISABLED by default, but code kept)\n    hidden_align_weight: float = 0.0  # disabled\n    teacher_d_model: int = 768        # gpt-2 hidden dim\n    teacher_n_layers: int = 12        # gpt-2 layers\n\n    # lora for ttt\n    lora_rank: int = 8\n    lora_alpha: float = 16.0\n    ttt_lr: float = 1e-4\n    ttt_steps: int = 100\n\n    # spiking parameters\n    spike_alpha: float = 1.0\n\n    # general training\n    batch_size: int = 8\n    max_grad_norm: float = 1.0\n    eval_interval: int = 100\n\nconfig = Config()\n\nprint(f\"configuration (v9 - model capacity increase):\")\nprint(f\"  teacher: {config.teacher_name} (124m params)\")\nprint(f\"  student: d={config.d_model}, layers={config.n_layers}\")\nprint(f\"  v9 changes: d_model 256→320, n_layers 4→5\")\nprint(f\"  distillation: {config.distill_steps} steps, T={config.temperature}\")\nprint(f\"  warmup: {config.warmup_steps} steps\")\nprint(f\"  hidden alignment: weight={config.hidden_align_weight} (disabled)\")\nprint(f\"  lora: rank={config.lora_rank}, ttt_steps={config.ttt_steps}\")\nprint(f\"\")\nprint(f\"expected memory: ~2.5GB (T4 has 16GB - plenty of headroom)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "cell-5",
   "source": "## 2. trainable ternary spiking (from v7)\n\n### 2.1 motivation\n\nfixed ternary values {-1, 0, +1} limit expressivity. the ternary spike paper (wei et al., 2023) shows that **trainable amplitude factors** per layer significantly improve accuracy:\n\n> \"we propose to learn the amplitude of ternary spikes, which allows different layers to have different spike magnitudes optimized during training.\"\n\n### 2.2 implementation\n\neach layer has a learnable `amplitude` parameter (initialized to 1.0). during forward pass:\n\n```python\nspikes = torch.zeros_like(x)\nspikes[x > threshold] = +amplitude  # trainable!\nspikes[x < -threshold] = -amplitude  # trainable!\n```\n\nthe amplitude receives gradients via the ste + multiplication chain.\n\n### 2.3 straight-through estimator (ste)\n\ngradients pass through unchanged during backpropagation:\n\n$$\\frac{\\partial \\mathcal{L}}{\\partial x} = \\frac{\\partial \\mathcal{L}}{\\partial s}$$"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "id": "cell-6",
   "source": "# =============================================================================\n# cell 6: trainable ternary spike (from v7, fixed implementation)\n# =============================================================================\nclass TrainableTernarySpike(nn.Module):\n    \"\"\"\n    trainable ternary spike with learnable amplitude.\n    \n    uses STE trick without custom autograd.Function:\n    1. compute spike pattern without gradient tracking\n    2. multiply by trainable amplitude (gradient flows here)\n    3. use (x - x.detach()) trick for STE gradient on x\n    \"\"\"\n\n    def __init__(self, alpha: float = 1.0):\n        super().__init__()\n        self.alpha = alpha\n        self.amplitude = nn.Parameter(torch.ones(1))\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        threshold = self.alpha * x.abs().mean(dim=-1, keepdim=True)\n        threshold = threshold.clamp(min=0.01, max=10.0)\n\n        with torch.no_grad():\n            pos_mask = (x > threshold).float()\n            neg_mask = (x < -threshold).float()\n            spike_signs = pos_mask - neg_mask\n\n        spikes = self.amplitude * spike_signs\n        return spikes + (x - x.detach())\n\n    def get_amplitude(self) -> float:\n        return self.amplitude.item()\n\n\n# test\nprint(\"testing TrainableTernarySpike...\")\n_spike = TrainableTernarySpike().to(DEVICE)\n_x = torch.randn(2, 16, 64, device=DEVICE, requires_grad=True)\n_y = _spike(_x)\n_y.sum().backward()\nprint(f\"  amplitude: {_spike.get_amplitude():.4f}\")\nprint(f\"  gradient for x: {'exists' if _x.grad is not None else 'none'}\")\nprint(f\"  gradient for amplitude: {_spike.amplitude.grad.item():.4f}\")\ndel _spike, _x, _y"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "id": "cell-7",
   "source": "# =============================================================================\n# cell 7: hardware and spike stats collectors\n# =============================================================================\nclass HardwareStatsCollector:\n    \"\"\"collect gpu memory, timing, and throughput metrics.\"\"\"\n\n    def __init__(self):\n        self.gpu_memory_history = []\n        self.step_times = []\n        self.tokens_processed = 0\n        self.start_time = None\n\n    def start(self):\n        self.start_time = time.time()\n        if torch.cuda.is_available():\n            torch.cuda.reset_peak_memory_stats()\n\n    def record_step(self, batch_size: int, seq_len: int):\n        if torch.cuda.is_available():\n            self.gpu_memory_history.append(torch.cuda.memory_allocated() / 1e9)\n        self.tokens_processed += batch_size * seq_len\n        self.step_times.append(time.time())\n\n    def get_throughput(self) -> float:\n        if len(self.step_times) < 2:\n            return 0.0\n        elapsed = self.step_times[-1] - self.step_times[0]\n        return self.tokens_processed / elapsed if elapsed > 0 else 0.0\n\n    def get_summary(self) -> Dict[str, Any]:\n        elapsed = time.time() - self.start_time if self.start_time else 0\n        return {\n            'peak_gpu_memory_gb': max(self.gpu_memory_history) if self.gpu_memory_history else 0,\n            'avg_gpu_memory_gb': float(np.mean(self.gpu_memory_history)) if self.gpu_memory_history else 0,\n            'total_training_time_s': elapsed,\n            'total_training_time_min': elapsed / 60,\n            'tokens_processed': self.tokens_processed,\n            'throughput_tokens_per_sec': self.get_throughput(),\n        }\n\n\nclass SpikeStatsCollector:\n    \"\"\"collect per-layer spike density and amplitude evolution.\"\"\"\n\n    def __init__(self, n_layers: int):\n        self.n_layers = n_layers\n        self.density_history = {i: {'k': [], 'v': []} for i in range(n_layers)}\n        self.amplitude_history = {i: {'k': [], 'v': []} for i in range(n_layers)}\n        self.step_densities = []\n\n    def record(self, student, step: int):\n        stats = student.get_spike_stats()\n        all_densities = []\n        for i in range(self.n_layers):\n            layer_key = f'layer_{i}'\n            if layer_key in stats:\n                k_density = stats[layer_key].get('k', 0)\n                v_density = stats[layer_key].get('v', 0)\n                k_amp = stats[layer_key].get('k_amp', 1.0)\n                v_amp = stats[layer_key].get('v_amp', 1.0)\n\n                self.density_history[i]['k'].append(k_density)\n                self.density_history[i]['v'].append(v_density)\n                self.amplitude_history[i]['k'].append(k_amp)\n                self.amplitude_history[i]['v'].append(v_amp)\n                all_densities.extend([k_density, v_density])\n\n        if all_densities:\n            self.step_densities.append({'step': step, 'density': float(np.mean(all_densities))})\n\n    def get_summary(self) -> Dict[str, Any]:\n        per_layer = {}\n        all_k, all_v = [], []\n        all_k_amp, all_v_amp = [], []\n\n        for i in range(self.n_layers):\n            k_vals = self.density_history[i]['k']\n            v_vals = self.density_history[i]['v']\n            k_amps = self.amplitude_history[i]['k']\n            v_amps = self.amplitude_history[i]['v']\n\n            per_layer[f'layer_{i}'] = {\n                'k_mean': float(np.mean(k_vals)) if k_vals else 0,\n                'k_std': float(np.std(k_vals)) if k_vals else 0,\n                'k_final': float(k_vals[-1]) if k_vals else 0,\n                'v_mean': float(np.mean(v_vals)) if v_vals else 0,\n                'v_std': float(np.std(v_vals)) if v_vals else 0,\n                'v_final': float(v_vals[-1]) if v_vals else 0,\n                'k_amp_final': float(k_amps[-1]) if k_amps else 1.0,\n                'v_amp_final': float(v_amps[-1]) if v_amps else 1.0,\n            }\n            all_k.extend(k_vals)\n            all_v.extend(v_vals)\n            if k_amps: all_k_amp.append(k_amps[-1])\n            if v_amps: all_v_amp.append(v_amps[-1])\n\n        return {\n            'per_layer': per_layer,\n            'overall_k_density': float(np.mean(all_k)) if all_k else 0,\n            'overall_v_density': float(np.mean(all_v)) if all_v else 0,\n            'overall_density': float(np.mean(all_k + all_v)) if (all_k or all_v) else 0,\n            'amplitudes': {'k': all_k_amp, 'v': all_v_amp},\n            'density_history': self.step_densities,\n        }\n\nprint(\"collectors defined\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "id": "cell-8",
   "source": "# =============================================================================\n# cell 8: spiking goose model\n# =============================================================================\nclass SpikingGooseRecurrentLayer(nn.Module):\n    \"\"\"rwkv-style recurrence with trainable ternary spiking.\"\"\"\n\n    def __init__(self, d_model, layer_idx=0, n_layers=4, spike_alpha=1.0):\n        super().__init__()\n        self.d_model = d_model\n        self.layer_idx = layer_idx\n        self.ln = nn.LayerNorm(d_model)\n\n        ratio = layer_idx / max(n_layers - 1, 1)\n        self.time_mix_k = nn.Parameter(torch.ones(d_model) * (1 - ratio))\n        self.time_mix_v = nn.Parameter(torch.ones(d_model) * (1 - ratio))\n        self.time_mix_r = nn.Parameter(torch.ones(d_model) * (1 - ratio))\n        self.decay_weight = nn.Parameter(torch.zeros(d_model) - 0.5)\n\n        self.key_proj = nn.Linear(d_model, d_model, bias=False)\n        self.value_proj = nn.Linear(d_model, d_model, bias=False)\n        self.receptance_proj = nn.Linear(d_model, d_model, bias=False)\n        self.output_proj = nn.Linear(d_model, d_model, bias=False)\n\n        self.k_spike = TrainableTernarySpike(alpha=spike_alpha)\n        self.v_spike = TrainableTernarySpike(alpha=spike_alpha)\n\n        self.register_buffer('running_k_density', torch.tensor(0.0))\n        self.register_buffer('running_v_density', torch.tensor(0.0))\n        self._init_weights()\n\n    def _init_weights(self):\n        std = 0.1 / math.sqrt(self.d_model)\n        for m in [self.key_proj, self.value_proj, self.receptance_proj, self.output_proj]:\n            nn.init.normal_(m.weight, std=std)\n\n    def forward(self, x):\n        B, T, D = x.shape\n        x_norm = self.ln(x)\n        prev_x = F.pad(x_norm[:, :-1, :], (0, 0, 1, 0))\n\n        xk = x_norm * self.time_mix_k + prev_x * (1 - self.time_mix_k)\n        xv = x_norm * self.time_mix_v + prev_x * (1 - self.time_mix_v)\n        xr = x_norm * self.time_mix_r + prev_x * (1 - self.time_mix_r)\n\n        k_pre = self.key_proj(xk)\n        v_pre = self.value_proj(xv)\n\n        k = self.k_spike(k_pre)\n        v = self.v_spike(v_pre)\n        r = torch.sigmoid(self.receptance_proj(xr))\n\n        kv = k * v\n        decay = torch.sigmoid(self.decay_weight)\n        t_idx = torch.arange(T, device=x.device, dtype=x.dtype)\n        decay_powers = decay.unsqueeze(0) ** t_idx.unsqueeze(1)\n\n        kv_weighted = kv / (decay_powers.unsqueeze(0) + 1e-8)\n        S = torch.cumsum(kv_weighted, dim=1) * decay_powers.unsqueeze(0)\n\n        if self.training:\n            with torch.no_grad():\n                self.running_k_density = 0.99 * self.running_k_density + 0.01 * (k != 0).float().mean()\n                self.running_v_density = 0.99 * self.running_v_density + 0.01 * (v != 0).float().mean()\n\n        return x + r * self.output_proj(S)\n\n    def get_spike_density(self):\n        return {\n            'k': self.running_k_density.item(),\n            'v': self.running_v_density.item(),\n            'k_amp': self.k_spike.get_amplitude(),\n            'v_amp': self.v_spike.get_amplitude(),\n        }\n\n\nclass GooseFFN(nn.Module):\n    def __init__(self, d_model, expand=4):\n        super().__init__()\n        self.ln = nn.LayerNorm(d_model)\n        self.w1 = nn.Linear(d_model, d_model * expand, bias=False)\n        self.w2 = nn.Linear(d_model * expand, d_model, bias=False)\n\n    def forward(self, x):\n        return x + self.w2(F.silu(self.w1(self.ln(x))))\n\n\nclass StudentSpikingGoose(nn.Module):\n    \"\"\"spiking student model with trainable ternary activations.\"\"\"\n\n    def __init__(self, cfg):\n        super().__init__()\n        self.cfg = cfg\n        self.embed = nn.Embedding(cfg.vocab_size, cfg.d_model)\n        self.pos_embed = nn.Embedding(cfg.max_seq_len, cfg.d_model)\n\n        self.layers = nn.ModuleList([\n            nn.ModuleDict({\n                'rec': SpikingGooseRecurrentLayer(cfg.d_model, i, cfg.n_layers, cfg.spike_alpha),\n                'ffn': GooseFFN(cfg.d_model),\n            })\n            for i in range(cfg.n_layers)\n        ])\n\n        self.ln_out = nn.LayerNorm(cfg.d_model)\n        self.head = nn.Linear(cfg.d_model, cfg.vocab_size, bias=False)\n        self.head.weight = self.embed.weight\n\n        nn.init.normal_(self.embed.weight, std=0.02)\n        nn.init.normal_(self.pos_embed.weight, std=0.02)\n\n    def forward(self, input_ids, return_hiddens=False):\n        \"\"\"forward pass with optional hidden state return for alignment.\"\"\"\n        B, T = input_ids.shape\n        pos = torch.arange(T, device=input_ids.device).unsqueeze(0)\n        x = self.embed(input_ids) + self.pos_embed(pos)\n\n        hiddens = [x] if return_hiddens else None\n\n        for layer in self.layers:\n            x = layer['rec'](x)\n            x = layer['ffn'](x)\n            if return_hiddens:\n                hiddens.append(x)\n\n        logits = self.head(self.ln_out(x))\n\n        if return_hiddens:\n            return logits, hiddens\n        return logits\n\n    def get_spike_stats(self):\n        return {f'layer_{i}': layer['rec'].get_spike_density() for i, layer in enumerate(self.layers)}\n\n    def get_avg_spike_density(self):\n        densities = []\n        for layer in self.layers:\n            d = layer['rec'].get_spike_density()\n            densities.extend([d['k'], d['v']])\n        return float(np.mean(densities)) if densities else 0.0\n\n    def get_amplitudes(self):\n        return {f'layer_{i}': {'k': layer['rec'].k_spike.get_amplitude(), 'v': layer['rec'].v_spike.get_amplitude()}\n                for i, layer in enumerate(self.layers)}\n\nprint(\"student model defined (v8: trainable amplitudes, optional alignment)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "id": "cell-9",
   "source": "# =============================================================================\n# cell 9: hidden-state alignment (KEPT from v8, optional)\n# =============================================================================\nclass HiddenStateProjector(nn.Module):\n    \"\"\"\n    project student hidden states to teacher dimension for alignment.\n    \n    student: (B, T, 320) -> (B, T, 768)  # v9: 320 dim\n    \n    maps 5 student layers to selected teacher layers.\n    \n    NOTE: kept from v8 for future experiments. only used if hidden_align_weight > 0.\n    \"\"\"\n\n    def __init__(self, student_dim: int, teacher_dim: int, n_student_layers: int):\n        super().__init__()\n        self.projectors = nn.ModuleList([\n            nn.Linear(student_dim, teacher_dim, bias=False)\n            for _ in range(n_student_layers)\n        ])\n        for proj in self.projectors:\n            nn.init.normal_(proj.weight, std=0.02)\n\n    def forward(self, student_hidden: torch.Tensor, layer_idx: int) -> torch.Tensor:\n        return self.projectors[layer_idx](student_hidden)\n\n\ndef compute_hidden_alignment_loss(\n    teacher_hiddens: List[torch.Tensor],\n    student_hiddens: List[torch.Tensor],\n    projector: HiddenStateProjector,\n    teacher_layers: int = 12,\n    student_layers: int = 5,  # v9: 5 student layers\n) -> torch.Tensor:\n    \"\"\"\n    compute mse loss between projected student hiddens and teacher hiddens.\n    \n    v9 maps 5 student layers to 12 teacher layers:\n      student 0 -> teacher 2 (3rd layer)\n      student 1 -> teacher 5 (6th layer)\n      student 2 -> teacher 7 (8th layer)\n      student 3 -> teacher 10 (11th layer)\n      student 4 -> teacher 12 (output)\n    \n    NOTE: kept from v8 for future experiments. only called if hidden_align_weight > 0.\n    \"\"\"\n    loss = 0.0\n    # v9: updated mapping for 5 student layers\n    teacher_indices = [2, 5, 7, 10, 12]  # v8 was [3, 6, 9, 12] for 4 layers\n\n    for s_idx, t_idx in enumerate(teacher_indices):\n        if s_idx < len(student_hiddens) - 1 and t_idx < len(teacher_hiddens):\n            s_h = student_hiddens[s_idx + 1]\n            t_h = teacher_hiddens[t_idx]\n            s_h_proj = projector(s_h, s_idx)\n            loss += F.mse_loss(s_h_proj, t_h)\n\n    return loss / len(teacher_indices)\n\n\nprint(\"hidden-state alignment defined (v9: 5 student layers)\")\nprint(f\"  student layers: {config.n_layers} (d={config.d_model})\")\nprint(f\"  teacher layers: {config.teacher_n_layers} (d={config.teacher_d_model})\")\nprint(f\"  layer mapping: [2, 5, 7, 10, 12] (v8 was [3, 6, 9, 12])\")\nprint(f\"  current weight: {config.hidden_align_weight} (set > 0 to enable)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "id": "cell-10",
   "source": "# =============================================================================\n# cell 10: cosine lr with warmup\n# =============================================================================\ndef get_cosine_schedule_with_warmup(\n    optimizer: torch.optim.Optimizer,\n    warmup_steps: int,\n    total_steps: int,\n) -> torch.optim.lr_scheduler.LambdaLR:\n    \"\"\"\n    linear warmup then cosine decay to 0.\n    \"\"\"\n    def lr_lambda(step: int) -> float:\n        if step < warmup_steps:\n            return step / max(warmup_steps, 1)\n        else:\n            progress = (step - warmup_steps) / max(total_steps - warmup_steps, 1)\n            return 0.5 * (1.0 + math.cos(math.pi * progress))\n\n    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n\n\nprint(f\"cosine lr: {config.warmup_steps} warmup, {config.distill_steps} total\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "id": "cell-11",
   "source": "# =============================================================================\n# cell 11: load gpt-2 teacher\n# =============================================================================\nfrom transformers import GPT2LMHeadModel, GPT2Tokenizer\n\nprint(\"loading gpt-2 teacher...\")\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\ntokenizer.pad_token = tokenizer.eos_token\n\nteacher = GPT2LMHeadModel.from_pretrained('gpt2').to(DEVICE)\nteacher.eval()\nfor p in teacher.parameters():\n    p.requires_grad = False\n\nteacher_params = sum(p.numel() for p in teacher.parameters())\nprint(f\"teacher: gpt-2 ({teacher_params:,} params)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "cell-12",
   "source": "## 3. gpt-2 as teacher\n\n### 3.1 model specifications\n\n| attribute | gpt-2 (teacher) | asnn-goose v9 (student) |\n|-----------|-----------------|-------------------------|\n| parameters | 124m | **~30m** |\n| layers | 12 | **5** |\n| hidden dim | 768 | **320** |\n| attention | softmax (dense) | linear + ternary spikes |\n| ppl (wikitext-2) | ~30 | target: **<520** (v9) |\n\n### 3.2 extracting hidden states\n\nfor hidden-state alignment (if enabled), we extract intermediate representations:\n\n```python\noutputs = teacher(ids, output_hidden_states=True)\nteacher_hiddens = outputs.hidden_states  # [embed, layer0, ..., layer11]\n```\n\n### 3.3 distillation loss\n\n$$\\mathcal{L}_{\\text{kd}} = T^2 \\cdot \\text{KL}(p^{(t)} \\| p^{(s)})$$\n\nwith $T=2$ (proven in v6, v8)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "id": "cell-13",
   "source": "# =============================================================================\n# cell 13: data loading\n# =============================================================================\nfrom datasets import load_dataset\n\nprint(\"loading wikitext-2...\")\ndataset = load_dataset('wikitext', 'wikitext-2-raw-v1')\n\ndef pre_tokenize(texts, max_len):\n    all_tokens = []\n    for text in tqdm(texts, desc=\"tokenizing\", leave=False):\n        if text.strip():\n            all_tokens.extend(tokenizer.encode(text, max_length=max_len*2, truncation=True))\n    chunks = [all_tokens[i:i+max_len] for i in range(0, len(all_tokens)-max_len+1, max_len//2) if len(all_tokens[i:i+max_len]) == max_len]\n    print(f\"created {len(chunks)} sequences\")\n    return torch.tensor(chunks, dtype=torch.long)\n\ntrain_tokens = pre_tokenize(dataset['train']['text'], config.max_seq_len)\nval_tokens = pre_tokenize(dataset['validation']['text'], config.max_seq_len)\n\ntrain_loader = DataLoader(TensorDataset(train_tokens), batch_size=config.batch_size, shuffle=True, pin_memory=True)\nval_loader = DataLoader(TensorDataset(val_tokens), batch_size=config.batch_size, shuffle=False, pin_memory=True)\n\nprint(f\"train: {len(train_loader)} batches, val: {len(val_loader)} batches\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "id": "cell-14",
   "source": "# =============================================================================\n# cell 14: create student model and projector\n# =============================================================================\nprint(\"creating student model (v9 - increased capacity)...\")\n\nstudent = StudentSpikingGoose(config).to(DEVICE)\nstudent_params = sum(p.numel() for p in student.parameters())\n\n# v9: create projector (even if not used, for infrastructure preservation)\nprojector = HiddenStateProjector(\n    student_dim=config.d_model,\n    teacher_dim=config.teacher_d_model,\n    n_student_layers=config.n_layers\n).to(DEVICE)\nprojector_params = sum(p.numel() for p in projector.parameters())\n\ncompression_ratio = teacher_params / student_params\n\nprint(f\"student: asnn-goose v9 ({student_params:,} params)\")\nprint(f\"projector: ({projector_params:,} params)\")\nprint(f\"compression ratio: {compression_ratio:.1f}x\")\nprint(f\"\")\nprint(f\"v9 capacity increase:\")\nprint(f\"  d_model: 256 → {config.d_model} (+25%)\")\nprint(f\"  n_layers: 4 → {config.n_layers} (+25%)\")\nprint(f\"  params: ~16M → ~{student_params // 1_000_000}M (+{(student_params / 16_000_000 - 1) * 100:.0f}%)\")\nprint(f\"\")\nprint(f\"settings (same as v8):\")\nprint(f\"  temperature: {config.temperature}\")\nprint(f\"  hidden alignment: weight={config.hidden_align_weight} (disabled)\")\nif config.hidden_align_weight > 0:\n    print(f\"  alignment ENABLED: projector will be used\")\nelse:\n    print(f\"  alignment DISABLED: projector created but not trained\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "id": "cell-15",
   "source": "# =============================================================================\n# cell 15: evaluation functions\n# =============================================================================\n@torch.no_grad()\ndef evaluate(model, loader, device, is_gpt2=False):\n    model.eval()\n    total_loss, total_tokens = 0, 0\n    for batch in loader:\n        ids = batch[0].to(device)\n        with torch.cuda.amp.autocast():\n            logits = model(ids).logits if is_gpt2 else model(ids)\n        loss = F.cross_entropy(logits[:, :-1].reshape(-1, logits.size(-1)), ids[:, 1:].reshape(-1), reduction='sum')\n        total_loss += loss.item()\n        total_tokens += ids[:, 1:].numel()\n    return total_loss / total_tokens\n\ndef get_ppl(loss):\n    return math.exp(min(loss, 10))\n\nprint(\"evaluation functions defined\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "cell-16",
   "source": "## 4. distillation training (v9)\n\n### 4.1 loss function (conditional alignment)\n\nv9 uses kl divergence with **optional** hidden-state alignment (same as v8):\n\n$$\\mathcal{L}_{\\text{total}} = \\mathcal{L}_{\\text{kd}} + \\lambda \\cdot \\mathcal{L}_{\\text{align}}$$\n\nwhere:\n- $\\mathcal{L}_{\\text{kd}} = T^2 \\cdot \\text{KL}(p^{(t)} \\| p^{(s)})$ with $T=2$\n- $\\mathcal{L}_{\\text{align}}$ only computed if $\\lambda > 0$\n- **v9 default**: $\\lambda = 0$ (alignment disabled)\n\n### 4.2 training schedule\n\n- **warmup**: 50 steps\n- **decay**: 2950 steps (cosine)\n- **total**: 3000 steps\n\n### 4.3 v9 changes\n\n| aspect | v8 | v9 |\n|--------|-----|-----|\n| d_model | 256 | **320** |\n| n_layers | 4 | **5** |\n| params | ~16M | **~30M** |\n| training | same | same |\n\n### 4.4 loss tracking\n\nboth `kl_loss_history` and `align_loss_history` tracked separately (even when align=0)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "id": "cell-17",
   "source": "# =============================================================================\n# cell 17: distillation training loop (v9 - same as v8, larger model)\n# =============================================================================\ndef distill_v9(teacher, student, projector, train_loader, val_loader, cfg, device,\n               hw_stats, spike_stats):\n    \"\"\"\n    v9 distillation: same as v8, but with larger model capacity.\n    \n    key settings (unchanged from v8):\n    - temperature = 2.0\n    - warmup = 50 steps\n    - hidden_align_weight = 0.0 (disabled)\n    \n    v9 changes:\n    - d_model: 256 → 320\n    - n_layers: 4 → 5\n    - params: ~16M → ~30M\n    \"\"\"\n    training_logs = {\n        'loss_history': [],\n        'kl_loss_history': [],\n        'align_loss_history': [],\n        'ppl_history': [],\n        'lr_history': [],\n    }\n\n    # combine student and projector parameters (projector only trained if weight > 0)\n    if cfg.hidden_align_weight > 0:\n        all_params = list(student.parameters()) + list(projector.parameters())\n    else:\n        all_params = list(student.parameters())\n    \n    optimizer = torch.optim.AdamW(all_params, lr=cfg.distill_lr, weight_decay=0.01)\n    scheduler = get_cosine_schedule_with_warmup(optimizer, cfg.warmup_steps, cfg.distill_steps)\n    scaler = torch.cuda.amp.GradScaler()\n\n    hw_stats.start()\n    step = 0\n    best_val = float('inf')\n\n    pbar = tqdm(total=cfg.distill_steps, desc='distilling (v9)')\n\n    while step < cfg.distill_steps:\n        for batch in train_loader:\n            if step >= cfg.distill_steps:\n                break\n\n            ids = batch[0].to(device, non_blocking=True)\n\n            with torch.cuda.amp.autocast():\n                # teacher forward (with hidden states only if needed)\n                with torch.no_grad():\n                    if cfg.hidden_align_weight > 0:\n                        t_out = teacher(ids, output_hidden_states=True)\n                        t_logits = t_out.logits\n                        t_hiddens = t_out.hidden_states\n                    else:\n                        t_logits = teacher(ids).logits\n\n                # student forward (with hidden states only if needed)\n                student.train()\n                if cfg.hidden_align_weight > 0:\n                    s_logits, s_hiddens = student(ids, return_hiddens=True)\n                else:\n                    s_logits = student(ids)\n\n                # kl divergence loss with T=2\n                T = cfg.temperature\n                s_log = F.log_softmax(s_logits / T, dim=-1)\n                t_prob = F.softmax(t_logits / T, dim=-1)\n                kl_loss = F.kl_div(\n                    s_log.view(-1, s_logits.size(-1)),\n                    t_prob.view(-1, t_logits.size(-1)),\n                    reduction='batchmean'\n                ) * (T ** 2)\n\n                # optional hidden-state alignment\n                if cfg.hidden_align_weight > 0:\n                    align_loss = compute_hidden_alignment_loss(\n                        t_hiddens, s_hiddens, projector,\n                        teacher_layers=cfg.teacher_n_layers,\n                        student_layers=cfg.n_layers\n                    )\n                    loss = kl_loss + cfg.hidden_align_weight * align_loss\n                else:\n                    align_loss = torch.tensor(0.0, device=device)\n                    loss = kl_loss\n\n            optimizer.zero_grad(set_to_none=True)\n            scaler.scale(loss).backward()\n            scaler.unscale_(optimizer)\n            gn = torch.nn.utils.clip_grad_norm_(all_params, cfg.max_grad_norm)\n\n            if torch.isfinite(gn):\n                scaler.step(optimizer)\n            scaler.update()\n            scheduler.step()\n\n            hw_stats.record_step(ids.size(0), ids.size(1))\n            spike_stats.record(student, step)\n\n            current_lr = optimizer.param_groups[0]['lr']\n\n            # track both losses separately (even if align is 0)\n            training_logs['loss_history'].append({'step': step, 'loss': loss.item()})\n            training_logs['kl_loss_history'].append({'step': step, 'loss': kl_loss.item()})\n            training_logs['align_loss_history'].append({'step': step, 'loss': align_loss.item()})\n            training_logs['lr_history'].append({'step': step, 'lr': current_lr})\n\n            pbar.set_postfix(\n                loss=f\"{loss.item():.3f}\",\n                kl=f\"{kl_loss.item():.3f}\",\n                align=f\"{align_loss.item():.3f}\",\n                lr=f\"{current_lr:.1e}\"\n            )\n            pbar.update(1)\n            step += 1\n\n            if step % cfg.eval_interval == 0:\n                val_loss = evaluate(student, val_loader, device)\n                val_ppl = get_ppl(val_loss)\n                training_logs['ppl_history'].append({'step': step, 'ppl': val_ppl})\n\n                amps = student.get_amplitudes()\n                amp_str = ', '.join([f\"L{i}:{amps[f'layer_{i}']['k']:.2f}\" for i in range(cfg.n_layers)])\n                print(f\"\\n  step {step}: ppl={val_ppl:.1f}, amps=[{amp_str}]\")\n\n                if val_loss < best_val:\n                    best_val = val_loss\n                    torch.save({\n                        'student': student.state_dict(),\n                        'projector': projector.state_dict(),\n                    }, f'{OUTPUT_DIR}/checkpoints/v9_best.pt')\n\n    pbar.close()\n    return training_logs\n\nprint(\"distillation function defined (v9 - larger capacity)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "id": "cell-18",
   "source": "# =============================================================================\n# cell 18: run distillation\n# =============================================================================\nprint(\"=\"*60)\nprint(\"phase 1: distillation (v9 - model capacity increase)\")\nprint(\"=\"*60)\nprint(\"\")\nprint(\"v9 changes from v8:\")\nprint(f\"  d_model: 256 → {config.d_model} (+25%)\")\nprint(f\"  n_layers: 4 → {config.n_layers} (+25%)\")\nprint(f\"  params: ~16M → ~{student_params // 1_000_000}M\")\nprint(\"\")\nprint(\"settings (unchanged from v8):\")\nprint(f\"  temperature: {config.temperature}\")\nprint(f\"  warmup: {config.warmup_steps} steps\")\nprint(f\"  hidden alignment: weight={config.hidden_align_weight} (disabled)\")\nprint(\"\")\n\nhw_stats = HardwareStatsCollector()\nspike_stats = SpikeStatsCollector(config.n_layers)\n\ndistill_logs = distill_v9(\n    teacher, student, projector, train_loader, val_loader,\n    config, DEVICE, hw_stats, spike_stats\n)\n\nprint(\"\")\nprint(f\"distillation complete!\")\nprint(f\"throughput: {hw_stats.get_summary()['throughput_tokens_per_sec']:.0f} tokens/sec\")\nprint(\"\")\nprint(\"final amplitudes:\")\nfor k, v in student.get_amplitudes().items():\n    print(f\"  {k}: k={v['k']:.4f}, v={v['v']:.4f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "cell-19",
   "source": "## 5. test-time training (ttt) with lora\n\n### 5.1 motivation\n\ntest-time training adapts the model to new data distributions at inference time. lora (hu et al., 2022) provides efficient adaptation:\n\n$$W' = W_0 + BA$$\n\nwhere:\n- $W_0$: frozen base weights\n- $B \\in \\mathbb{R}^{d_{out} \\times r}$: low-rank up-projection\n- $A \\in \\mathbb{R}^{r \\times d_{in}}$: low-rank down-projection\n- $r = 8$: rank\n\n### 5.2 application to spiking models\n\nwe apply lora to key and value projections, enabling adaptation of spike patterns at test time."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "id": "cell-20",
   "source": "# =============================================================================\n# cell 20: lora implementation\n# =============================================================================\nclass LoRALinear(nn.Module):\n    \"\"\"lora adapter for linear layers.\"\"\"\n\n    def __init__(self, in_features, out_features, rank=8, alpha=16.0):\n        super().__init__()\n        self.scaling = alpha / rank\n        self.lora_A = nn.Parameter(torch.zeros(rank, in_features))\n        self.lora_B = nn.Parameter(torch.zeros(out_features, rank))\n        nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n\n    def forward(self, x):\n        return (x @ self.lora_A.T @ self.lora_B.T) * self.scaling\n\n\ndef apply_lora(model, rank=8, alpha=16.0, targets=['key_proj', 'value_proj']):\n    \"\"\"apply lora adapters to specified modules.\"\"\"\n    lora_modules = {}\n    for name, module in model.named_modules():\n        if any(t in name for t in targets) and isinstance(module, nn.Linear):\n            lora = LoRALinear(module.in_features, module.out_features, rank, alpha).to(next(module.parameters()).device)\n            lora_modules[name] = lora\n            orig_forward = module.forward\n            def make_forward(orig, lora_mod):\n                def forward(x):\n                    return orig(x) + lora_mod(x)\n                return forward\n            module.forward = make_forward(orig_forward, lora)\n    print(f\"lora: {len(lora_modules)} modules, rank={rank}\")\n    return lora_modules\n\nprint(\"lora defined\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "id": "cell-21",
   "source": "# =============================================================================\n# cell 21: ttt with lora\n# =============================================================================\nprint(\"=\"*60)\nprint(\"phase 2: test-time training with lora\")\nprint(\"=\"*60)\n\nfor p in student.parameters():\n    p.requires_grad = False\n\nlora_modules = apply_lora(student, config.lora_rank, config.lora_alpha)\nlora_params = sum(p.numel() for m in lora_modules.values() for p in m.parameters())\n\npre_ttt_loss = evaluate(student, val_loader, DEVICE)\npre_ttt_ppl = get_ppl(pre_ttt_loss)\nprint(f\"\\npre-ttt ppl: {pre_ttt_ppl:.2f}\")\n\nlora_opt = torch.optim.AdamW([p for m in lora_modules.values() for p in m.parameters()], lr=config.ttt_lr)\nttt_logs = {'loss_history': []}\nstudent.train()\n\nfor step, batch in enumerate(val_loader):\n    if step >= config.ttt_steps:\n        break\n    ids = batch[0].to(DEVICE)\n    with torch.cuda.amp.autocast():\n        loss = F.cross_entropy(student(ids)[:, :-1].reshape(-1, config.vocab_size), ids[:, 1:].reshape(-1))\n    lora_opt.zero_grad()\n    loss.backward()\n    lora_opt.step()\n    ttt_logs['loss_history'].append({'step': step, 'loss': loss.item()})\n    if step % 20 == 0:\n        print(f\"  ttt {step}: loss={loss.item():.4f}\")\n\npost_ttt_loss = evaluate(student, val_loader, DEVICE)\npost_ttt_ppl = get_ppl(post_ttt_loss)\nprint(f\"\\npost-ttt ppl: {post_ttt_ppl:.2f}\")\nprint(f\"ttt improvement: {pre_ttt_ppl - post_ttt_ppl:.1f} ppl\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "id": "cell-22",
   "source": "# =============================================================================\n# cell 22: final evaluation\n# =============================================================================\nprint(\"=\"*60)\nprint(\"final evaluation\")\nprint(\"=\"*60)\n\nteacher_loss = evaluate(teacher, val_loader, DEVICE, is_gpt2=True)\nteacher_ppl = get_ppl(teacher_loss)\nstudent_loss = evaluate(student, val_loader, DEVICE)\nstudent_ppl = get_ppl(student_loss)\n\n# v9: add VRAM logging\nvram_peak_gb = torch.cuda.max_memory_allocated() / 1e9 if torch.cuda.is_available() else 0\n\nprint(f\"\")\nprint(f\"{'model':<25} {'ppl':>10} {'params':>15}\")\nprint(\"-\" * 50)\nprint(f\"{'gpt-2 (teacher)':<25} {teacher_ppl:>10.2f} {teacher_params:>15,}\")\nprint(f\"{'asnn-goose v9 (student)':<25} {student_ppl:>10.2f} {student_params:>15,}\")\nprint(\"-\" * 50)\nprint(f\"{'compression':<25} {compression_ratio:>10.1f}x\")\nprint(f\"{'ppl gap':<25} {student_ppl - teacher_ppl:>10.2f}\")\nprint(f\"{'spike density':<25} {student.get_avg_spike_density():>10.3f}\")\nprint(f\"{'VRAM peak':<25} {vram_peak_gb:>10.2f}GB\")\nprint(\"\")\nprint(\"version comparison:\")\nprint(f\"  v6: 627.3 PPL (baseline)\")\nprint(f\"  v7: 1655 PPL (regression!)\")\nprint(f\"  v8: 559 PPL (fixed)\")\nprint(f\"  v9: {student_ppl:.2f} PPL (capacity increase)\")\nif student_ppl < 520:\n    print(f\"  v9 TARGET MET! PPL < 520\")\nelif student_ppl < 559:\n    print(f\"  v9 beats v8 by {559 - student_ppl:.1f} PPL\")\nelse:\n    print(f\"  WARNING: v9 did not improve over v8\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "id": "cell-23",
   "source": "# =============================================================================\n# cell 23: visualization\n# =============================================================================\nfig, axes = plt.subplots(2, 3, figsize=(15, 10))\n\n# distillation loss (combined + separate)\nd_steps = [l['step'] for l in distill_logs['loss_history']]\nd_losses = [l['loss'] for l in distill_logs['loss_history']]\nkl_losses = [l['loss'] for l in distill_logs['kl_loss_history']]\nalign_losses = [l['loss'] for l in distill_logs['align_loss_history']]\naxes[0,0].plot(d_steps, d_losses, label='total', alpha=0.8)\naxes[0,0].plot(d_steps, kl_losses, label='kl', alpha=0.7)\nif config.hidden_align_weight > 0:\n    axes[0,0].plot(d_steps, align_losses, label='align', alpha=0.7)\naxes[0,0].set_xlabel('step')\naxes[0,0].set_ylabel('loss')\naxes[0,0].set_title('distillation loss (v9)')\naxes[0,0].legend()\n\n# validation ppl\np_steps = [l['step'] for l in distill_logs['ppl_history']]\np_ppls = [l['ppl'] for l in distill_logs['ppl_history']]\naxes[0,1].plot(p_steps, p_ppls, 'orange', marker='o')\naxes[0,1].axhline(y=teacher_ppl, color='green', linestyle='--', label=f'teacher ({teacher_ppl:.1f})')\naxes[0,1].axhline(y=627.3, color='blue', linestyle=':', label='v6 (627.3)')\naxes[0,1].axhline(y=559, color='purple', linestyle=':', label='v8 (559)')\naxes[0,1].axhline(y=520, color='red', linestyle='--', label='v9 target (520)')\naxes[0,1].set_xlabel('step')\naxes[0,1].set_ylabel('ppl')\naxes[0,1].set_title('validation ppl')\naxes[0,1].legend()\n\n# lr schedule\nlr_steps = [l['step'] for l in distill_logs['lr_history']]\nlr_vals = [l['lr'] for l in distill_logs['lr_history']]\naxes[0,2].plot(lr_steps, lr_vals, 'purple')\naxes[0,2].axvline(x=config.warmup_steps, color='gray', linestyle='--', label=f'warmup ({config.warmup_steps})')\naxes[0,2].set_xlabel('step')\naxes[0,2].set_ylabel('lr')\naxes[0,2].set_title('learning rate')\naxes[0,2].legend()\n\n# spike density + amplitudes\nspike_summary = spike_stats.get_summary()\nlayers = list(spike_summary['per_layer'].keys())\nk_dens = [spike_summary['per_layer'][l]['k_final'] for l in layers]\nv_dens = [spike_summary['per_layer'][l]['v_final'] for l in layers]\nk_amps = [spike_summary['per_layer'][l]['k_amp_final'] for l in layers]\nv_amps = [spike_summary['per_layer'][l]['v_amp_final'] for l in layers]\n\nx = np.arange(len(layers))\naxes[1,0].bar(x - 0.2, k_dens, 0.4, label='k density')\naxes[1,0].bar(x + 0.2, v_dens, 0.4, label='v density')\nax2 = axes[1,0].twinx()\nax2.plot(x, k_amps, 'r-o', label='k amp')\nax2.plot(x, v_amps, 'b-s', label='v amp')\naxes[1,0].set_xlabel('layer')\naxes[1,0].set_ylabel('density')\nax2.set_ylabel('amplitude')\naxes[1,0].set_title('spike density & amplitudes (v9: 5 layers)')\naxes[1,0].legend(loc='upper left')\nax2.legend(loc='upper right')\n\n# ttt loss\nt_steps = [l['step'] for l in ttt_logs['loss_history']]\nt_losses = [l['loss'] for l in ttt_logs['loss_history']]\naxes[1,1].plot(t_steps, t_losses, 'red')\naxes[1,1].set_xlabel('step')\naxes[1,1].set_ylabel('ce loss')\naxes[1,1].set_title('ttt with lora')\n\n# version comparison\nversions = ['v6', 'v7', 'v8', 'v9']\nt_ppls = [44.6, 44.6, 44.6, teacher_ppl]\ns_ppls = [627.3, 1655, 559, student_ppl]\nx = np.arange(len(versions))\naxes[1,2].bar(x - 0.2, t_ppls, 0.4, label='teacher', alpha=0.7)\naxes[1,2].bar(x + 0.2, s_ppls, 0.4, label='student', alpha=0.7)\naxes[1,2].axhline(y=520, color='red', linestyle='--', label='v9 target', alpha=0.7)\naxes[1,2].set_xticks(x)\naxes[1,2].set_xticklabels(versions)\naxes[1,2].set_ylabel('ppl')\naxes[1,2].set_title('version comparison')\naxes[1,2].legend()\naxes[1,2].set_yscale('log')\n\nplt.tight_layout()\nfigure_path = f'{OUTPUT_DIR}/figures/v9_training_{RUN_TIMESTAMP}.png'\nplt.savefig(figure_path, dpi=300, bbox_inches='tight')\nplt.show()\nprint(f\"saved: {figure_path}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "id": "cell-24",
   "source": "# =============================================================================\n# cell 24: results.json with base64 png (renamed from summary.json)\n# =============================================================================\nprint(\"building results...\")\n\nwith open(figure_path, 'rb') as f:\n    figure_base64 = base64.b64encode(f.read()).decode('utf-8')\n\nresults = {\n    'version': 'v9',\n    'timestamp': datetime.now().isoformat(),\n    'run_id': RUN_TIMESTAMP,\n    'platform': PLATFORM,\n    'description': 'model capacity increase (320d, 5L, ~30M params)',\n\n    'v9_design': {\n        'principle': 'increase model capacity for better distillation',\n        'changes': {\n            'd_model': '256 → 320 (+25%)',\n            'n_layers': '4 → 5 (+25%)',\n            'params': '~16M → ~30M (+87%)',\n            'teacher_indices': '[3,6,9,12] → [2,5,7,10,12]',\n        },\n        'unchanged': [\n            'temperature: 2.0',\n            'hidden_align_weight: 0.0',\n            'warmup_steps: 50',\n            'distill_steps: 3000',\n        ],\n    },\n\n    'architecture': {\n        'teacher': {'name': 'gpt2', 'params': teacher_params},\n        'student': {\n            'name': 'asnn-goose-v9',\n            'd_model': config.d_model,\n            'n_layers': config.n_layers,\n            'params': student_params,\n        },\n        'projector_params': projector_params,\n        'compression_ratio': compression_ratio,\n        'vram_peak_gb': vram_peak_gb,\n    },\n\n    'training_config': {\n        'distill_steps': config.distill_steps,\n        'temperature': config.temperature,\n        'hidden_align_weight': config.hidden_align_weight,\n        'warmup_steps': config.warmup_steps,\n        'batch_size': config.batch_size,\n        'distill_lr': config.distill_lr,\n        'max_grad_norm': config.max_grad_norm,\n    },\n\n    'results': {\n        'teacher_ppl': teacher_ppl,\n        'student_ppl': student_ppl,\n        'ppl_gap': student_ppl - teacher_ppl,\n        'spike_density': student.get_avg_spike_density(),\n        'amplitudes': student.get_amplitudes(),\n        'target_met': student_ppl < 520,\n    },\n\n    'training_curves': {\n        'loss_history': distill_logs['loss_history'],\n        'kl_loss_history': distill_logs['kl_loss_history'],\n        'align_loss_history': distill_logs['align_loss_history'],\n        'ppl_history': distill_logs['ppl_history'],\n        'lr_history': distill_logs['lr_history'],\n    },\n\n    'hardware_stats': hw_stats.get_summary(),\n    'spike_analysis': spike_stats.get_summary(),\n\n    'ttt': {\n        'lora_params': lora_params,\n        'pre_ppl': pre_ttt_ppl,\n        'post_ppl': post_ttt_ppl,\n        'improvement': pre_ttt_ppl - post_ttt_ppl,\n        'loss_history': ttt_logs['loss_history'],\n    },\n\n    'comparison': {\n        'v6': {'student_ppl': 627.3, 'note': 'baseline'},\n        'v7': {'student_ppl': 1655, 'note': 'regression (align=1.0, T=4)'},\n        'v8': {'student_ppl': 559, 'note': 'fixed defaults (align=0, T=2)'},\n        'v9': {'student_ppl': student_ppl, 'note': 'capacity increase (320d, 5L)'},\n    },\n\n    'figures': {\n        'training_plot': {\n            'filename': f'v9_training_{RUN_TIMESTAMP}.png',\n            'base64': figure_base64,\n        }\n    },\n}\n\n# Save as results.json (renamed from summary.json)\nresults_path = f'{OUTPUT_DIR}/results/results_{RUN_TIMESTAMP}.json'\nwith open(results_path, 'w') as f:\n    json.dump(results, f, indent=2, default=str)\n\nprint(f\"saved: {results_path}\")\nprint(f\"size: {os.path.getsize(results_path) / 1024:.1f} KB\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "id": "cell-25",
   "source": "# =============================================================================\n# cell 25: auto-download for colab\n# =============================================================================\nprint(\"=\"*60)\nprint(\"auto-download\")\nprint(\"=\"*60)\n\nif IS_COLAB:\n    try:\n        from google.colab import files\n        files.download(results_path)\n        files.download(figure_path)\n        print(\"downloads started!\")\n    except Exception as e:\n        print(f\"download failed: {e}\")\nelif IS_KAGGLE:\n    print(f\"kaggle: {results_path}\")\nelse:\n    print(f\"local: {results_path}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "id": "cell-26",
   "source": "# =============================================================================\n# cell 26: validation tests (9 total - added amplitude health)\n# =============================================================================\nprint(\"=\"*60)\nprint(\"validation tests (v9)\")\nprint(\"=\"*60)\n\ntest_results = {}\n\n# 1. teacher pre-trained\nprint(\"\\n[1] teacher pre-trained\")\ntest_results['teacher_pretrained'] = teacher_ppl < 50\nprint(f\"  teacher ppl: {teacher_ppl:.2f} - {'pass' if test_results['teacher_pretrained'] else 'fail'}\")\n\n# 2. student learned (v9 target: beat v8's 559, aim for <520)\nprint(\"\\n[2] student learned language\")\ntest_results['student_learned'] = student_ppl < 627\nprint(f\"  student ppl: {student_ppl:.2f} - {'pass' if test_results['student_learned'] else 'fail'} (target: < 627)\")\n\n# 3. ternary activations\nprint(\"\\n[3] ternary activations\")\nstudent.eval()\nwith torch.no_grad():\n    test_ids = next(iter(val_loader))[0].to(DEVICE)\n    layer = student.layers[0]['rec']\n    x = student.embed(test_ids) + student.pos_embed(torch.arange(test_ids.size(1), device=DEVICE).unsqueeze(0))\n    x_norm = layer.ln(x)\n    prev_x = F.pad(x_norm[:, :-1, :], (0, 0, 1, 0))\n    xk = x_norm * layer.time_mix_k + prev_x * (1 - layer.time_mix_k)\n    k_spike = layer.k_spike(layer.key_proj(xk))\n    unique_vals = len(set([round(v, 4) for v in k_spike.unique().cpu().tolist()]))\n    test_results['ternary'] = unique_vals <= 3\n    print(f\"  unique values: {unique_vals} - {'pass' if test_results['ternary'] else 'fail'}\")\n\n# 4. gradient flow\nprint(\"\\n[4] gradient flow (STE)\")\n_spike = TrainableTernarySpike().to(DEVICE)\n_x = torch.randn(2, 16, 64, device=DEVICE, requires_grad=True)\n_spike(_x).sum().backward()\ntest_results['gradient'] = _x.grad is not None and _x.grad.abs().sum() > 0\nprint(f\"  {'pass' if test_results['gradient'] else 'fail'}\")\n\n# 5. spike density\nprint(\"\\n[5] spike density in range\")\ndensity = student.get_avg_spike_density()\ntest_results['density'] = 0.1 < density < 0.9\nprint(f\"  density: {density:.3f} - {'pass' if test_results['density'] else 'fail'}\")\n\n# 6. lora applied\nprint(\"\\n[6] lora applied\")\ntest_results['lora'] = len(lora_modules) > 0\nprint(f\"  modules: {len(lora_modules)} - {'pass' if test_results['lora'] else 'fail'}\")\n\n# 7. improvement over v6 baseline\nprint(\"\\n[7] beats v6 baseline\")\ntest_results['beats_v6'] = student_ppl < 627.3\nprint(f\"  v6: 627.3, v9: {student_ppl:.2f} - {'pass' if test_results['beats_v6'] else 'fail'}\")\n\n# 8. amplitudes learned\nprint(\"\\n[8] amplitudes learned\")\namps = student.get_amplitudes()\ntest_results['amplitudes_learned'] = any(\n    abs(amps[f'layer_{i}']['k'] - 1.0) > 0.05 or abs(amps[f'layer_{i}']['v'] - 1.0) > 0.05\n    for i in range(config.n_layers)\n)\nprint(f\"  amplitudes: {[f\\\"{k}:{v['k']:.3f}\\\" for k,v in amps.items()]}\")\nprint(f\"  {'pass' if test_results['amplitudes_learned'] else 'fail'} - any amplitude != 1.0 by > 0.05\")\n\n# 9. NEW: amplitude health check (v9)\nprint(\"\\n[9] amplitude health (v9 new)\")\nall_healthy = True\nfor layer_idx, amp_dict in amps.items():\n    k_amp, v_amp = amp_dict['k'], amp_dict['v']\n    if not (0.3 < k_amp < 3.0) or not (0.3 < v_amp < 3.0):\n        print(f\"  WARNING: {layer_idx} unhealthy: k={k_amp:.3f}, v={v_amp:.3f}\")\n        all_healthy = False\ntest_results['amplitude_health'] = all_healthy\nprint(f\"  {'pass' if all_healthy else 'fail'} - all amplitudes in [0.3, 3.0]\")\n\n# save results to results dict\nresults['validation_tests'] = test_results\nwith open(results_path, 'w') as f:\n    json.dump(results, f, indent=2, default=str)\n\nprint(\"\\n\" + \"=\"*60)\npassed = sum(1 for v in test_results.values() if v)\nprint(f\"results: {passed}/{len(test_results)} passed\")\nif student_ppl < 520:\n    print(f\"v9 TARGET MET: PPL {student_ppl:.2f} < 520\")\nelif student_ppl < 559:\n    print(f\"v9 improved from v8: {559 - student_ppl:.1f} PPL reduction\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "cell-27",
   "source": "## 6. summary\n\n### 6.1 v9 design principle\n\n**increase model capacity for better distillation**\n\n| attribute | v8 | v9 | change |\n|-----------|-----|-----|--------|\n| d_model | 256 | **320** | +25% |\n| n_layers | 4 | **5** | +25% |\n| params | ~16M | **~30M** | +87% |\n| VRAM | ~1.5GB | **~2.5GB** | +67% |\n| temperature | 2.0 | 2.0 | - |\n| hidden_align_weight | 0.0 | 0.0 | - |\n\n### 6.2 version progression\n\n| version | teacher ppl | student ppl | key change |\n|---------|-------------|-------------|------------|\n| v6 | 44.6 | **627** | gpt-2 distillation |\n| v7 | 44.6 | 1655 | **regression** (align=1.0, T=4) |\n| v8 | 44.6 | **559** | fixed defaults |\n| **v9** | ~45 | **target: <520** | **capacity increase** |\n\n### 6.3 validation tests (9 total)\n\n1. teacher pre-trained (PPL < 50)\n2. student learned (PPL < 627)\n3. ternary activations verified\n4. gradient flow via STE\n5. spike density in [0.1, 0.9]\n6. LoRA applied\n7. beats v6 baseline\n8. amplitudes learned\n9. **amplitude health [0.3, 3.0]** (NEW in v9)\n\n### 6.4 next steps\n\nif v9 succeeds (PPL < 520):\n1. v10: curriculum temperature (CTKD)\n2. v11: progressive training\n3. v12: patient training\n\nif v9 doesn't meet target:\n- try v9.1 with 384d × 6L (~45M params)\n\n---\n\n*asnn-goose v9 - eptesicus laboratories - lumis-next initiative*\n\n### references\n\n- hinton, g., vinyals, o., & dean, j. (2015). distilling the knowledge in a neural network.\n- radford, a., et al. (2019). language models are unsupervised multitask learners.\n- lv, c., et al. (2023). spikebert: a language spikformer learned from bert with knowledge distillation.\n- shen, j., et al. (2024). spikingmamba: towards energy-efficient large language models.\n- wei, j., et al. (2023). ternary spike: learning ternary spikes for spiking neural networks.\n- hu, e. j., et al. (2022). lora: low-rank adaptation of large language models."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}