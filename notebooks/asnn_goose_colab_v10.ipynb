{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# asnn-goose v10: 100M model with speedups (512d × 8L)\n",
    "\n",
    "## abstract\n",
    "\n",
    "v9 achieved **PPL 541.7** (3% improvement over v8's 559). v10 jumps to a 100M parameter model with training speedups to offset the larger size.\n",
    "\n",
    "**v9 results (baseline for v10):**\n",
    "- teacher ppl: 44.6\n",
    "- student ppl: **541.7** (target was <520 ❌)\n",
    "- tests: 9/9 passed\n",
    "- amplitudes: 0.79-1.08 (learned)\n",
    "- spike density: 0.38\n",
    "- VRAM: 3.67GB\n",
    "\n",
    "**v10 strategy: 100M model + speedups**\n",
    "\n",
    "| attribute | v9 | v10 | change |\n",
    "|-----------|-----|-----|--------|\n",
    "| d_model | 320 | **512** | +60% |\n",
    "| n_layers | 5 | **8** | +60% |\n",
    "| params | ~22M | **~100M** | +350% |\n",
    "| VRAM est. | ~3.7GB | **~8-10GB** | +170% |\n",
    "\n",
    "**speedups (to offset larger model):**\n",
    "- gradient checkpointing (50% less activation memory)\n",
    "- torch.compile (15-30% speedup)\n",
    "- efficient DataLoader (workers, prefetch)\n",
    "- fused AdamW (5-10% speedup)\n",
    "- gradient accumulation (effective batch 16)\n",
    "\n",
    "**expected:**\n",
    "- student ppl: **<480** (from 541.7)\n",
    "- tests: 9/9 passed\n",
    "- VRAM: <12GB on T4 (16GB available)\n",
    "\n",
    "---\n",
    "\n",
    "**eptesicus laboratories - lumis-next initiative**\n",
    "\n",
    "### references\n",
    "- hinton et al. (2015) \"distilling the knowledge in a neural network\"\n",
    "- radford et al. (2019) \"language models are unsupervised multitask learners\" (gpt-2)\n",
    "- lv et al. (2023) \"spikebert: a language spikformer learned from bert with knowledge distillation\"\n",
    "- chen et al. (2016) \"training deep nets with sublinear memory cost\" (gradient checkpointing)\n",
    "- pytorch 2.0 docs: torch.compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# cell 1: environment setup (v10 - added compile flag)\n",
    "# =============================================================================\n",
    "import os\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import json\n",
    "import base64\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Dict, List, Optional, Tuple, Any\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# v10: torch.compile flag (disable if causing issues)\n",
    "USE_TORCH_COMPILE = True\n",
    "USE_GRADIENT_CHECKPOINTING = True\n",
    "\n",
    "# generate timestamp for this run\n",
    "RUN_TIMESTAMP = datetime.now().strftime('%Y-%m-%d_%H%M%S')\n",
    "print(f\"run timestamp: {RUN_TIMESTAMP}\")\n",
    "\n",
    "# detect platform\n",
    "IS_KAGGLE = 'KAGGLE_KERNEL_RUN_TYPE' in os.environ\n",
    "IS_COLAB = 'COLAB_GPU' in os.environ or 'google.colab' in sys.modules\n",
    "PLATFORM = 'kaggle' if IS_KAGGLE else 'colab' if IS_COLAB else 'local'\n",
    "OUTPUT_DIR = '/kaggle/working/outputs' if IS_KAGGLE else 'outputs'\n",
    "\n",
    "for subdir in ['figures', 'checkpoints', 'logs', 'results']:\n",
    "    os.makedirs(f'{OUTPUT_DIR}/{subdir}', exist_ok=True)\n",
    "\n",
    "print(f\"platform: {PLATFORM}\")\n",
    "print(f\"output directory: {OUTPUT_DIR}\")\n",
    "print(f\"torch.compile: {'enabled' if USE_TORCH_COMPILE else 'disabled'}\")\n",
    "print(f\"gradient checkpointing: {'enabled' if USE_GRADIENT_CHECKPOINTING else 'disabled'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# cell 2: pytorch and hardware setup (v10 - added compile precision)\n",
    "# =============================================================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "SEED = 42\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"gpu: {gpu_name}\")\n",
    "    print(f\"memory: {gpu_memory:.1f} gb\")\n",
    "\n",
    "# v10: set float32 matmul precision for torch.compile\n",
    "if USE_TORCH_COMPILE and hasattr(torch, 'set_float32_matmul_precision'):\n",
    "    torch.set_float32_matmul_precision('high')\n",
    "    print(\"float32 matmul precision: high (for torch.compile)\")\n",
    "\n",
    "print(f\"device: {DEVICE}\")\n",
    "print(f\"pytorch: {torch.__version__}\")\n",
    "\n",
    "# check torch.compile availability\n",
    "TORCH_COMPILE_AVAILABLE = hasattr(torch, 'compile') and torch.__version__ >= '2.0'\n",
    "print(f\"torch.compile available: {TORCH_COMPILE_AVAILABLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. v10 design: 100M model with speedups\n",
    "\n",
    "### 1.1 rationale for 100M model\n",
    "\n",
    "v9 achieved PPL 541.7 with a 22M parameter model - only 3% improvement over v8. Research shows that capacity alone has diminishing returns. We jump to 100M:\n",
    "\n",
    "- **d_model**: 320 → 512 (+60%)\n",
    "- **n_layers**: 5 → 8 (+60%)\n",
    "- **params**: ~22M → ~100M (+350%)\n",
    "\n",
    "### 1.2 speedup strategies\n",
    "\n",
    "| technique | benefit | source |\n",
    "|-----------|---------|--------|\n",
    "| **gradient checkpointing** | 50% less activation memory | Chen et al. 2016 |\n",
    "| **torch.compile** | 15-30% speedup | PyTorch 2.0 docs |\n",
    "| **efficient DataLoader** | 10-20% speedup | workers, prefetch |\n",
    "| **fused AdamW** | 5-10% speedup | PyTorch 2.0 |\n",
    "| **gradient accumulation** | effective batch 16 | stable gradients |\n",
    "\n",
    "### 1.3 memory analysis\n",
    "\n",
    "```\n",
    "100M params = ~400MB (FP32) or ~200MB (FP16)\n",
    "Optimizer states (Adam) = ~800MB\n",
    "Gradients = ~200MB (FP16)\n",
    "Activations (with checkpointing) = ~4GB (batch 8, seq 256, 8 layers, 512 dim)\n",
    "Total = ~6GB\n",
    "\n",
    "T4 has 16GB → 10GB headroom (safe margin with checkpointing)\n",
    "```\n",
    "\n",
    "### 1.4 v10 changes from v9\n",
    "\n",
    "| component | v9 | v10 | action |\n",
    "|-----------|-----|-----|--------|\n",
    "| d_model | 320 | 512 | **increase** |\n",
    "| n_layers | 5 | 8 | **increase** |\n",
    "| teacher_indices | [2,5,7,10,12] | [1,2,4,5,7,8,10,11] | **remap** |\n",
    "| params | ~22M | ~100M | **result** |\n",
    "| gradient checkpointing | none | **enabled** | **add** |\n",
    "| torch.compile | none | **enabled** | **add** |\n",
    "| accumulation_steps | 1 | **2** | **add** |\n",
    "| fused optimizer | no | **yes** | **add** |\n",
    "| temperature | 2.0 | 2.0 | keep |\n",
    "| hidden_align_weight | 0.0 | 0.0 | keep |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# cell 4: configuration (v10 - 100M model)\n",
    "# =============================================================================\n",
    "@dataclass\n",
    "class Config:\n",
    "    # gpt-2 teacher (frozen, pre-trained)\n",
    "    teacher_name: str = \"gpt2\"\n",
    "\n",
    "    # student model architecture - v10: 100M MODEL\n",
    "    d_model: int = 512      # v9: 320 -> v10: 512 (+60%)\n",
    "    n_layers: int = 8       # v9: 5 -> v10: 8 (+60%)\n",
    "    vocab_size: int = 50257\n",
    "    max_seq_len: int = 256\n",
    "\n",
    "    # distillation training (same as v9)\n",
    "    distill_steps: int = 3000\n",
    "    distill_lr: float = 3e-4\n",
    "    temperature: float = 2.0      # proven in v6, v8, v9\n",
    "    warmup_steps: int = 50        # minimal warmup\n",
    "\n",
    "    # v10: gradient accumulation for effective larger batch\n",
    "    accumulation_steps: int = 2   # effective batch = 8 * 2 = 16\n",
    "\n",
    "    # hidden-state alignment (DISABLED by default, but code kept)\n",
    "    hidden_align_weight: float = 0.0  # disabled\n",
    "    teacher_d_model: int = 768        # gpt-2 hidden dim\n",
    "    teacher_n_layers: int = 12        # gpt-2 layers\n",
    "\n",
    "    # lora for ttt\n",
    "    lora_rank: int = 8\n",
    "    lora_alpha: float = 16.0\n",
    "    ttt_lr: float = 1e-4\n",
    "    ttt_steps: int = 100\n",
    "\n",
    "    # spiking parameters\n",
    "    spike_alpha: float = 1.0\n",
    "\n",
    "    # general training\n",
    "    batch_size: int = 8\n",
    "    max_grad_norm: float = 1.0\n",
    "    eval_interval: int = 100\n",
    "\n",
    "config = Config()\n",
    "\n",
    "print(f\"configuration (v10 - 100M model with speedups):\")\n",
    "print(f\"  teacher: {config.teacher_name} (124m params)\")\n",
    "print(f\"  student: d={config.d_model}, layers={config.n_layers}\")\n",
    "print(f\"  v10 changes: d_model 320→512, n_layers 5→8\")\n",
    "print(f\"  distillation: {config.distill_steps} steps, T={config.temperature}\")\n",
    "print(f\"  warmup: {config.warmup_steps} steps\")\n",
    "print(f\"  hidden alignment: weight={config.hidden_align_weight} (disabled)\")\n",
    "print(f\"  lora: rank={config.lora_rank}, ttt_steps={config.ttt_steps}\")\n",
    "print(f\"\")\n",
    "print(f\"v10 speedups:\")\n",
    "print(f\"  gradient checkpointing: {USE_GRADIENT_CHECKPOINTING}\")\n",
    "print(f\"  torch.compile: {USE_TORCH_COMPILE}\")\n",
    "print(f\"  accumulation_steps: {config.accumulation_steps} (effective batch = {config.batch_size * config.accumulation_steps})\")\n",
    "print(f\"  fused optimizer: enabled\")\n",
    "print(f\"\")\n",
    "print(f\"expected memory: ~6-8GB with checkpointing (T4 has 16GB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. trainable ternary spiking (from v8)\n",
    "\n",
    "### 2.1 motivation\n",
    "\n",
    "fixed ternary values {-1, 0, +1} limit expressivity. the ternary spike paper (wei et al., 2023) shows that **trainable amplitude factors** per layer significantly improve accuracy.\n",
    "\n",
    "### 2.2 implementation\n",
    "\n",
    "each layer has a learnable `amplitude` parameter (initialized to 1.0). during forward pass:\n",
    "\n",
    "```python\n",
    "spikes = torch.zeros_like(x)\n",
    "spikes[x > threshold] = +amplitude  # trainable!\n",
    "spikes[x < -threshold] = -amplitude  # trainable!\n",
    "```\n",
    "\n",
    "### 2.3 straight-through estimator (ste)\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial x} = \\frac{\\partial \\mathcal{L}}{\\partial s}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# cell 6: trainable ternary spike (same as v9)\n",
    "# =============================================================================\n",
    "class TrainableTernarySpike(nn.Module):\n",
    "    \"\"\"\n",
    "    trainable ternary spike with learnable amplitude.\n",
    "    \n",
    "    uses STE trick without custom autograd.Function:\n",
    "    1. compute spike pattern without gradient tracking\n",
    "    2. multiply by trainable amplitude (gradient flows here)\n",
    "    3. use (x - x.detach()) trick for STE gradient on x\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, alpha: float = 1.0):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.amplitude = nn.Parameter(torch.ones(1))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        threshold = self.alpha * x.abs().mean(dim=-1, keepdim=True)\n",
    "        threshold = threshold.clamp(min=0.01, max=10.0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pos_mask = (x > threshold).float()\n",
    "            neg_mask = (x < -threshold).float()\n",
    "            spike_signs = pos_mask - neg_mask\n",
    "\n",
    "        spikes = self.amplitude * spike_signs\n",
    "        return spikes + (x - x.detach())\n",
    "\n",
    "    def get_amplitude(self) -> float:\n",
    "        return self.amplitude.item()\n",
    "\n",
    "\n",
    "# test\n",
    "print(\"testing TrainableTernarySpike...\")\n",
    "_spike = TrainableTernarySpike().to(DEVICE)\n",
    "_x = torch.randn(2, 16, 64, device=DEVICE, requires_grad=True)\n",
    "_y = _spike(_x)\n",
    "_y.sum().backward()\n",
    "print(f\"  amplitude: {_spike.get_amplitude():.4f}\")\n",
    "print(f\"  gradient for x: {'exists' if _x.grad is not None else 'none'}\")\n",
    "print(f\"  gradient for amplitude: {_spike.amplitude.grad.item():.4f}\")\n",
    "del _spike, _x, _y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# cell 7: hardware and spike stats collectors (same as v9)\n",
    "# =============================================================================\n",
    "class HardwareStatsCollector:\n",
    "    \"\"\"collect gpu memory, timing, and throughput metrics.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.gpu_memory_history = []\n",
    "        self.step_times = []\n",
    "        self.tokens_processed = 0\n",
    "        self.start_time = None\n",
    "\n",
    "    def start(self):\n",
    "        self.start_time = time.time()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "    def record_step(self, batch_size: int, seq_len: int):\n",
    "        if torch.cuda.is_available():\n",
    "            self.gpu_memory_history.append(torch.cuda.memory_allocated() / 1e9)\n",
    "        self.tokens_processed += batch_size * seq_len\n",
    "        self.step_times.append(time.time())\n",
    "\n",
    "    def get_throughput(self) -> float:\n",
    "        if len(self.step_times) < 2:\n",
    "            return 0.0\n",
    "        elapsed = self.step_times[-1] - self.step_times[0]\n",
    "        return self.tokens_processed / elapsed if elapsed > 0 else 0.0\n",
    "\n",
    "    def get_summary(self) -> Dict[str, Any]:\n",
    "        elapsed = time.time() - self.start_time if self.start_time else 0\n",
    "        return {\n",
    "            'peak_gpu_memory_gb': max(self.gpu_memory_history) if self.gpu_memory_history else 0,\n",
    "            'avg_gpu_memory_gb': float(np.mean(self.gpu_memory_history)) if self.gpu_memory_history else 0,\n",
    "            'total_training_time_s': elapsed,\n",
    "            'total_training_time_min': elapsed / 60,\n",
    "            'tokens_processed': self.tokens_processed,\n",
    "            'throughput_tokens_per_sec': self.get_throughput(),\n",
    "        }\n",
    "\n",
    "\n",
    "class SpikeStatsCollector:\n",
    "    \"\"\"collect per-layer spike density and amplitude evolution.\"\"\"\n",
    "\n",
    "    def __init__(self, n_layers: int):\n",
    "        self.n_layers = n_layers\n",
    "        self.density_history = {i: {'k': [], 'v': []} for i in range(n_layers)}\n",
    "        self.amplitude_history = {i: {'k': [], 'v': []} for i in range(n_layers)}\n",
    "        self.step_densities = []\n",
    "\n",
    "    def record(self, student, step: int):\n",
    "        stats = student.get_spike_stats()\n",
    "        all_densities = []\n",
    "        for i in range(self.n_layers):\n",
    "            layer_key = f'layer_{i}'\n",
    "            if layer_key in stats:\n",
    "                k_density = stats[layer_key].get('k', 0)\n",
    "                v_density = stats[layer_key].get('v', 0)\n",
    "                k_amp = stats[layer_key].get('k_amp', 1.0)\n",
    "                v_amp = stats[layer_key].get('v_amp', 1.0)\n",
    "\n",
    "                self.density_history[i]['k'].append(k_density)\n",
    "                self.density_history[i]['v'].append(v_density)\n",
    "                self.amplitude_history[i]['k'].append(k_amp)\n",
    "                self.amplitude_history[i]['v'].append(v_amp)\n",
    "                all_densities.extend([k_density, v_density])\n",
    "\n",
    "        if all_densities:\n",
    "            self.step_densities.append({'step': step, 'density': float(np.mean(all_densities))})\n",
    "\n",
    "    def get_summary(self) -> Dict[str, Any]:\n",
    "        per_layer = {}\n",
    "        all_k, all_v = [], []\n",
    "        all_k_amp, all_v_amp = [], []\n",
    "\n",
    "        for i in range(self.n_layers):\n",
    "            k_vals = self.density_history[i]['k']\n",
    "            v_vals = self.density_history[i]['v']\n",
    "            k_amps = self.amplitude_history[i]['k']\n",
    "            v_amps = self.amplitude_history[i]['v']\n",
    "\n",
    "            per_layer[f'layer_{i}'] = {\n",
    "                'k_mean': float(np.mean(k_vals)) if k_vals else 0,\n",
    "                'k_std': float(np.std(k_vals)) if k_vals else 0,\n",
    "                'k_final': float(k_vals[-1]) if k_vals else 0,\n",
    "                'v_mean': float(np.mean(v_vals)) if v_vals else 0,\n",
    "                'v_std': float(np.std(v_vals)) if v_vals else 0,\n",
    "                'v_final': float(v_vals[-1]) if v_vals else 0,\n",
    "                'k_amp_final': float(k_amps[-1]) if k_amps else 1.0,\n",
    "                'v_amp_final': float(v_amps[-1]) if v_amps else 1.0,\n",
    "            }\n",
    "            all_k.extend(k_vals)\n",
    "            all_v.extend(v_vals)\n",
    "            if k_amps: all_k_amp.append(k_amps[-1])\n",
    "            if v_amps: all_v_amp.append(v_amps[-1])\n",
    "\n",
    "        return {\n",
    "            'per_layer': per_layer,\n",
    "            'overall_k_density': float(np.mean(all_k)) if all_k else 0,\n",
    "            'overall_v_density': float(np.mean(all_v)) if all_v else 0,\n",
    "            'overall_density': float(np.mean(all_k + all_v)) if (all_k or all_v) else 0,\n",
    "            'amplitudes': {'k': all_k_amp, 'v': all_v_amp},\n",
    "            'density_history': self.step_densities,\n",
    "        }\n",
    "\n",
    "print(\"collectors defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# cell 8: spiking goose model (v10 - added gradient checkpointing)\n",
    "# =============================================================================\n",
    "class SpikingGooseRecurrentLayer(nn.Module):\n",
    "    \"\"\"rwkv-style recurrence with trainable ternary spiking.\"\"\"\n",
    "\n",
    "    def __init__(self, d_model, layer_idx=0, n_layers=4, spike_alpha=1.0):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.layer_idx = layer_idx\n",
    "        self.ln = nn.LayerNorm(d_model)\n",
    "\n",
    "        ratio = layer_idx / max(n_layers - 1, 1)\n",
    "        self.time_mix_k = nn.Parameter(torch.ones(d_model) * (1 - ratio))\n",
    "        self.time_mix_v = nn.Parameter(torch.ones(d_model) * (1 - ratio))\n",
    "        self.time_mix_r = nn.Parameter(torch.ones(d_model) * (1 - ratio))\n",
    "        self.decay_weight = nn.Parameter(torch.zeros(d_model) - 0.5)\n",
    "\n",
    "        self.key_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.value_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.receptance_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.output_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "        self.k_spike = TrainableTernarySpike(alpha=spike_alpha)\n",
    "        self.v_spike = TrainableTernarySpike(alpha=spike_alpha)\n",
    "\n",
    "        self.register_buffer('running_k_density', torch.tensor(0.0))\n",
    "        self.register_buffer('running_v_density', torch.tensor(0.0))\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        std = 0.1 / math.sqrt(self.d_model)\n",
    "        for m in [self.key_proj, self.value_proj, self.receptance_proj, self.output_proj]:\n",
    "            nn.init.normal_(m.weight, std=std)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, D = x.shape\n",
    "        x_norm = self.ln(x)\n",
    "        prev_x = F.pad(x_norm[:, :-1, :], (0, 0, 1, 0))\n",
    "\n",
    "        xk = x_norm * self.time_mix_k + prev_x * (1 - self.time_mix_k)\n",
    "        xv = x_norm * self.time_mix_v + prev_x * (1 - self.time_mix_v)\n",
    "        xr = x_norm * self.time_mix_r + prev_x * (1 - self.time_mix_r)\n",
    "\n",
    "        k_pre = self.key_proj(xk)\n",
    "        v_pre = self.value_proj(xv)\n",
    "\n",
    "        k = self.k_spike(k_pre)\n",
    "        v = self.v_spike(v_pre)\n",
    "        r = torch.sigmoid(self.receptance_proj(xr))\n",
    "\n",
    "        kv = k * v\n",
    "        decay = torch.sigmoid(self.decay_weight)\n",
    "        t_idx = torch.arange(T, device=x.device, dtype=x.dtype)\n",
    "        decay_powers = decay.unsqueeze(0) ** t_idx.unsqueeze(1)\n",
    "\n",
    "        kv_weighted = kv / (decay_powers.unsqueeze(0) + 1e-8)\n",
    "        S = torch.cumsum(kv_weighted, dim=1) * decay_powers.unsqueeze(0)\n",
    "\n",
    "        if self.training:\n",
    "            with torch.no_grad():\n",
    "                self.running_k_density = 0.99 * self.running_k_density + 0.01 * (k != 0).float().mean()\n",
    "                self.running_v_density = 0.99 * self.running_v_density + 0.01 * (v != 0).float().mean()\n",
    "\n",
    "        return x + r * self.output_proj(S)\n",
    "\n",
    "    def get_spike_density(self):\n",
    "        return {\n",
    "            'k': self.running_k_density.item(),\n",
    "            'v': self.running_v_density.item(),\n",
    "            'k_amp': self.k_spike.get_amplitude(),\n",
    "            'v_amp': self.v_spike.get_amplitude(),\n",
    "        }\n",
    "\n",
    "\n",
    "class GooseFFN(nn.Module):\n",
    "    def __init__(self, d_model, expand=4):\n",
    "        super().__init__()\n",
    "        self.ln = nn.LayerNorm(d_model)\n",
    "        self.w1 = nn.Linear(d_model, d_model * expand, bias=False)\n",
    "        self.w2 = nn.Linear(d_model * expand, d_model, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.w2(F.silu(self.w1(self.ln(x))))\n",
    "\n",
    "\n",
    "class StudentSpikingGoose(nn.Module):\n",
    "    \"\"\"\n",
    "    spiking student model with trainable ternary activations.\n",
    "    \n",
    "    v10: added gradient checkpointing support.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cfg, use_checkpointing=True):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.use_checkpointing = use_checkpointing and USE_GRADIENT_CHECKPOINTING\n",
    "        \n",
    "        self.embed = nn.Embedding(cfg.vocab_size, cfg.d_model)\n",
    "        self.pos_embed = nn.Embedding(cfg.max_seq_len, cfg.d_model)\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.ModuleDict({\n",
    "                'rec': SpikingGooseRecurrentLayer(cfg.d_model, i, cfg.n_layers, cfg.spike_alpha),\n",
    "                'ffn': GooseFFN(cfg.d_model),\n",
    "            })\n",
    "            for i in range(cfg.n_layers)\n",
    "        ])\n",
    "\n",
    "        self.ln_out = nn.LayerNorm(cfg.d_model)\n",
    "        self.head = nn.Linear(cfg.d_model, cfg.vocab_size, bias=False)\n",
    "        self.head.weight = self.embed.weight\n",
    "\n",
    "        nn.init.normal_(self.embed.weight, std=0.02)\n",
    "        nn.init.normal_(self.pos_embed.weight, std=0.02)\n",
    "\n",
    "    def _layer_forward(self, layer, x):\n",
    "        \"\"\"helper for gradient checkpointing - processes one layer.\"\"\"\n",
    "        x = layer['rec'](x)\n",
    "        x = layer['ffn'](x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, input_ids, return_hiddens=False):\n",
    "        \"\"\"forward pass with optional hidden state return for alignment.\"\"\"\n",
    "        B, T = input_ids.shape\n",
    "        pos = torch.arange(T, device=input_ids.device).unsqueeze(0)\n",
    "        x = self.embed(input_ids) + self.pos_embed(pos)\n",
    "\n",
    "        hiddens = [x] if return_hiddens else None\n",
    "\n",
    "        for layer in self.layers:\n",
    "            if self.use_checkpointing and self.training:\n",
    "                # v10: gradient checkpointing with use_reentrant=False (recommended)\n",
    "                x = checkpoint(self._layer_forward, layer, x, use_reentrant=False)\n",
    "            else:\n",
    "                x = self._layer_forward(layer, x)\n",
    "            \n",
    "            if return_hiddens:\n",
    "                hiddens.append(x)\n",
    "\n",
    "        logits = self.head(self.ln_out(x))\n",
    "\n",
    "        if return_hiddens:\n",
    "            return logits, hiddens\n",
    "        return logits\n",
    "\n",
    "    def get_spike_stats(self):\n",
    "        return {f'layer_{i}': layer['rec'].get_spike_density() for i, layer in enumerate(self.layers)}\n",
    "\n",
    "    def get_avg_spike_density(self):\n",
    "        densities = []\n",
    "        for layer in self.layers:\n",
    "            d = layer['rec'].get_spike_density()\n",
    "            densities.extend([d['k'], d['v']])\n",
    "        return float(np.mean(densities)) if densities else 0.0\n",
    "\n",
    "    def get_amplitudes(self):\n",
    "        return {f'layer_{i}': {'k': layer['rec'].k_spike.get_amplitude(), 'v': layer['rec'].v_spike.get_amplitude()}\n",
    "                for i, layer in enumerate(self.layers)}\n",
    "\n",
    "print(\"student model defined (v10: gradient checkpointing support)\")\n",
    "print(f\"  gradient checkpointing: {USE_GRADIENT_CHECKPOINTING}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# cell 9: hidden-state alignment (v10 - updated for 8 layers)\n",
    "# =============================================================================\n",
    "class HiddenStateProjector(nn.Module):\n",
    "    \"\"\"\n",
    "    project student hidden states to teacher dimension for alignment.\n",
    "    \n",
    "    student: (B, T, 512) -> (B, T, 768)  # v10: 512 dim\n",
    "    \n",
    "    maps 8 student layers to selected teacher layers.\n",
    "    \n",
    "    NOTE: kept from v9 for future experiments. only used if hidden_align_weight > 0.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, student_dim: int, teacher_dim: int, n_student_layers: int):\n",
    "        super().__init__()\n",
    "        self.projectors = nn.ModuleList([\n",
    "            nn.Linear(student_dim, teacher_dim, bias=False)\n",
    "            for _ in range(n_student_layers)\n",
    "        ])\n",
    "        for proj in self.projectors:\n",
    "            nn.init.normal_(proj.weight, std=0.02)\n",
    "\n",
    "    def forward(self, student_hidden: torch.Tensor, layer_idx: int) -> torch.Tensor:\n",
    "        return self.projectors[layer_idx](student_hidden)\n",
    "\n",
    "\n",
    "def compute_hidden_alignment_loss(\n",
    "    teacher_hiddens: List[torch.Tensor],\n",
    "    student_hiddens: List[torch.Tensor],\n",
    "    projector: HiddenStateProjector,\n",
    "    teacher_layers: int = 12,\n",
    "    student_layers: int = 8,  # v10: 8 student layers\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    compute mse loss between projected student hiddens and teacher hiddens.\n",
    "    \n",
    "    v10 maps 8 student layers to 12 teacher layers (more frequent alignment):\n",
    "      student 0 -> teacher 1\n",
    "      student 1 -> teacher 2\n",
    "      student 2 -> teacher 4\n",
    "      student 3 -> teacher 5\n",
    "      student 4 -> teacher 7\n",
    "      student 5 -> teacher 8\n",
    "      student 6 -> teacher 10\n",
    "      student 7 -> teacher 11\n",
    "    \n",
    "    NOTE: kept from v9 for future experiments. only called if hidden_align_weight > 0.\n",
    "    \"\"\"\n",
    "    loss = 0.0\n",
    "    # v10: updated mapping for 8 student layers (more frequent alignment)\n",
    "    teacher_indices = [1, 2, 4, 5, 7, 8, 10, 11]  # v9 was [2, 5, 7, 10, 12] for 5 layers\n",
    "\n",
    "    for s_idx, t_idx in enumerate(teacher_indices):\n",
    "        if s_idx < len(student_hiddens) - 1 and t_idx < len(teacher_hiddens):\n",
    "            s_h = student_hiddens[s_idx + 1]\n",
    "            t_h = teacher_hiddens[t_idx]\n",
    "            s_h_proj = projector(s_h, s_idx)\n",
    "            loss += F.mse_loss(s_h_proj, t_h)\n",
    "\n",
    "    return loss / len(teacher_indices)\n",
    "\n",
    "\n",
    "print(\"hidden-state alignment defined (v10: 8 student layers)\")\n",
    "print(f\"  student layers: {config.n_layers} (d={config.d_model})\")\n",
    "print(f\"  teacher layers: {config.teacher_n_layers} (d={config.teacher_d_model})\")\n",
    "print(f\"  layer mapping: [1, 2, 4, 5, 7, 8, 10, 11] (v9 was [2, 5, 7, 10, 12])\")\n",
    "print(f\"  current weight: {config.hidden_align_weight} (set > 0 to enable)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# cell 10: cosine lr with warmup (same as v9)\n",
    "# =============================================================================\n",
    "def get_cosine_schedule_with_warmup(\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    warmup_steps: int,\n",
    "    total_steps: int,\n",
    ") -> torch.optim.lr_scheduler.LambdaLR:\n",
    "    \"\"\"\n",
    "    linear warmup then cosine decay to 0.\n",
    "    \"\"\"\n",
    "    def lr_lambda(step: int) -> float:\n",
    "        if step < warmup_steps:\n",
    "            return step / max(warmup_steps, 1)\n",
    "        else:\n",
    "            progress = (step - warmup_steps) / max(total_steps - warmup_steps, 1)\n",
    "            return 0.5 * (1.0 + math.cos(math.pi * progress))\n",
    "\n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "\n",
    "print(f\"cosine lr: {config.warmup_steps} warmup, {config.distill_steps} total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# cell 11: load gpt-2 teacher (same as v9)\n",
    "# =============================================================================\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "print(\"loading gpt-2 teacher...\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "teacher = GPT2LMHeadModel.from_pretrained('gpt2').to(DEVICE)\n",
    "teacher.eval()\n",
    "for p in teacher.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "teacher_params = sum(p.numel() for p in teacher.parameters())\n",
    "print(f\"teacher: gpt-2 ({teacher_params:,} params)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. gpt-2 as teacher\n",
    "\n",
    "### 3.1 model specifications\n",
    "\n",
    "| attribute | gpt-2 (teacher) | asnn-goose v10 (student) |\n",
    "|-----------|-----------------|-------------------------|\n",
    "| parameters | 124m | **~100m** |\n",
    "| layers | 12 | **8** |\n",
    "| hidden dim | 768 | **512** |\n",
    "| attention | softmax (dense) | linear + ternary spikes |\n",
    "| ppl (wikitext-2) | ~30 | target: **<480** (v10) |\n",
    "\n",
    "### 3.2 distillation loss\n",
    "\n",
    "$$\\mathcal{L}_{\\text{kd}} = T^2 \\cdot \\text{KL}(p^{(t)} \\| p^{(s)})$$\n",
    "\n",
    "with $T=2$ (proven in v6, v8, v9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# cell 13: data loading (v10 - efficient DataLoader)\n",
    "# =============================================================================\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"loading wikitext-2...\")\n",
    "dataset = load_dataset('wikitext', 'wikitext-2-raw-v1')\n",
    "\n",
    "def pre_tokenize(texts, max_len):\n",
    "    all_tokens = []\n",
    "    for text in tqdm(texts, desc=\"tokenizing\", leave=False):\n",
    "        if text.strip():\n",
    "            all_tokens.extend(tokenizer.encode(text, max_length=max_len*2, truncation=True))\n",
    "    chunks = [all_tokens[i:i+max_len] for i in range(0, len(all_tokens)-max_len+1, max_len//2) if len(all_tokens[i:i+max_len]) == max_len]\n",
    "    print(f\"created {len(chunks)} sequences\")\n",
    "    return torch.tensor(chunks, dtype=torch.long)\n",
    "\n",
    "train_tokens = pre_tokenize(dataset['train']['text'], config.max_seq_len)\n",
    "val_tokens = pre_tokenize(dataset['validation']['text'], config.max_seq_len)\n",
    "\n",
    "# v10: efficient DataLoader with workers and prefetch\n",
    "# Note: num_workers=0 for Kaggle/Colab compatibility, but prefetch still helps\n",
    "dataloader_kwargs = {\n",
    "    'batch_size': config.batch_size,\n",
    "    'pin_memory': True,\n",
    "    'num_workers': 0 if IS_KAGGLE or IS_COLAB else 2,  # workers disabled on cloud platforms\n",
    "    'prefetch_factor': None if IS_KAGGLE or IS_COLAB else 2,\n",
    "    'persistent_workers': False if IS_KAGGLE or IS_COLAB else True,\n",
    "}\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(train_tokens), shuffle=True, **dataloader_kwargs)\n",
    "val_loader = DataLoader(TensorDataset(val_tokens), shuffle=False, **dataloader_kwargs)\n",
    "\n",
    "print(f\"train: {len(train_loader)} batches, val: {len(val_loader)} batches\")\n",
    "print(f\"DataLoader: num_workers={dataloader_kwargs['num_workers']}, pin_memory={dataloader_kwargs['pin_memory']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# cell 14: create student model and projector (v10 - with compile)\n",
    "# =============================================================================\n",
    "print(\"creating student model (v10 - 100M with speedups)...\")\n",
    "\n",
    "student = StudentSpikingGoose(config, use_checkpointing=USE_GRADIENT_CHECKPOINTING).to(DEVICE)\n",
    "student_params = sum(p.numel() for p in student.parameters())\n",
    "\n",
    "# v10: create projector (even if not used, for infrastructure preservation)\n",
    "projector = HiddenStateProjector(\n",
    "    student_dim=config.d_model,\n",
    "    teacher_dim=config.teacher_d_model,\n",
    "    n_student_layers=config.n_layers\n",
    ").to(DEVICE)\n",
    "projector_params = sum(p.numel() for p in projector.parameters())\n",
    "\n",
    "compression_ratio = teacher_params / student_params\n",
    "\n",
    "print(f\"student: asnn-goose v10 ({student_params:,} params)\")\n",
    "print(f\"projector: ({projector_params:,} params)\")\n",
    "print(f\"compression ratio: {compression_ratio:.1f}x\")\n",
    "print(f\"\")\n",
    "print(f\"v10 architecture:\")\n",
    "print(f\"  d_model: {config.d_model}\")\n",
    "print(f\"  n_layers: {config.n_layers}\")\n",
    "print(f\"  params: ~{student_params // 1_000_000}M\")\n",
    "print(f\"\")\n",
    "\n",
    "# v10: compile model if available and enabled\n",
    "compile_success = False\n",
    "if USE_TORCH_COMPILE and TORCH_COMPILE_AVAILABLE:\n",
    "    try:\n",
    "        print(\"compiling student model with torch.compile...\")\n",
    "        # Use the compile() method as recommended by PyTorch docs\n",
    "        student = torch.compile(student, mode='reduce-overhead')\n",
    "        compile_success = True\n",
    "        print(\"compilation successful!\")\n",
    "    except Exception as e:\n",
    "        print(f\"torch.compile failed: {e}\")\n",
    "        print(\"continuing without compilation\")\n",
    "else:\n",
    "    print(f\"torch.compile skipped (USE_TORCH_COMPILE={USE_TORCH_COMPILE}, available={TORCH_COMPILE_AVAILABLE})\")\n",
    "\n",
    "print(f\"\")\n",
    "print(f\"speedups active:\")\n",
    "print(f\"  gradient checkpointing: {USE_GRADIENT_CHECKPOINTING}\")\n",
    "print(f\"  torch.compile: {compile_success}\")\n",
    "print(f\"  accumulation_steps: {config.accumulation_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# cell 15: evaluation functions (same as v9)\n",
    "# =============================================================================\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, device, is_gpt2=False):\n",
    "    model.eval()\n",
    "    total_loss, total_tokens = 0, 0\n",
    "    for batch in loader:\n",
    "        ids = batch[0].to(device)\n",
    "        with torch.cuda.amp.autocast():\n",
    "            logits = model(ids).logits if is_gpt2 else model(ids)\n",
    "        loss = F.cross_entropy(logits[:, :-1].reshape(-1, logits.size(-1)), ids[:, 1:].reshape(-1), reduction='sum')\n",
    "        total_loss += loss.item()\n",
    "        total_tokens += ids[:, 1:].numel()\n",
    "    return total_loss / total_tokens\n",
    "\n",
    "def get_ppl(loss):\n",
    "    return math.exp(min(loss, 10))\n",
    "\n",
    "print(\"evaluation functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. distillation training (v10)\n",
    "\n",
    "### 4.1 loss function\n",
    "\n",
    "$$\\mathcal{L}_{\\text{total}} = \\mathcal{L}_{\\text{kd}} + \\lambda \\cdot \\mathcal{L}_{\\text{align}}$$\n",
    "\n",
    "where:\n",
    "- $\\mathcal{L}_{\\text{kd}} = T^2 \\cdot \\text{KL}(p^{(t)} \\| p^{(s)})$ with $T=2$\n",
    "- **v10 default**: $\\lambda = 0$ (alignment disabled)\n",
    "\n",
    "### 4.2 v10 speedups\n",
    "\n",
    "| technique | implementation |\n",
    "|-----------|----------------|\n",
    "| gradient checkpointing | `checkpoint(..., use_reentrant=False)` |\n",
    "| torch.compile | `torch.compile(model, mode='reduce-overhead')` |\n",
    "| fused optimizer | `AdamW(..., fused=True)` |\n",
    "| gradient accumulation | `accumulation_steps=2` |\n",
    "\n",
    "### 4.3 v10 changes from v9\n",
    "\n",
    "| aspect | v9 | v10 |\n",
    "|--------|-----|-----|\n",
    "| d_model | 320 | **512** |\n",
    "| n_layers | 5 | **8** |\n",
    "| params | ~22M | **~100M** |\n",
    "| checkpointing | none | **enabled** |\n",
    "| torch.compile | none | **enabled** |\n",
    "| accumulation | 1 | **2** |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# cell 17: distillation training loop (v10 - fused optimizer + accumulation)\n",
    "# =============================================================================\n",
    "def distill_v10(teacher, student, projector, train_loader, val_loader, cfg, device,\n",
    "               hw_stats, spike_stats):\n",
    "    \"\"\"\n",
    "    v10 distillation: 100M model with speedups.\n",
    "    \n",
    "    key settings (unchanged from v9):\n",
    "    - temperature = 2.0\n",
    "    - warmup = 50 steps\n",
    "    - hidden_align_weight = 0.0 (disabled)\n",
    "    \n",
    "    v10 speedups:\n",
    "    - gradient checkpointing (in model)\n",
    "    - torch.compile (in model creation)\n",
    "    - fused AdamW\n",
    "    - gradient accumulation\n",
    "    \"\"\"\n",
    "    training_logs = {\n",
    "        'loss_history': [],\n",
    "        'kl_loss_history': [],\n",
    "        'align_loss_history': [],\n",
    "        'ppl_history': [],\n",
    "        'lr_history': [],\n",
    "    }\n",
    "\n",
    "    # combine student and projector parameters (projector only trained if weight > 0)\n",
    "    if cfg.hidden_align_weight > 0:\n",
    "        all_params = list(student.parameters()) + list(projector.parameters())\n",
    "    else:\n",
    "        all_params = list(student.parameters())\n",
    "    \n",
    "    # v10: try fused AdamW for speedup (PyTorch 2.0+)\n",
    "    try:\n",
    "        optimizer = torch.optim.AdamW(all_params, lr=cfg.distill_lr, weight_decay=0.01, fused=True)\n",
    "        print(\"using fused AdamW\")\n",
    "    except TypeError:\n",
    "        optimizer = torch.optim.AdamW(all_params, lr=cfg.distill_lr, weight_decay=0.01)\n",
    "        print(\"fused AdamW not available, using standard\")\n",
    "    \n",
    "    scheduler = get_cosine_schedule_with_warmup(optimizer, cfg.warmup_steps, cfg.distill_steps)\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    hw_stats.start()\n",
    "    step = 0\n",
    "    accum_step = 0\n",
    "    best_val = float('inf')\n",
    "\n",
    "    # v10: gradient accumulation\n",
    "    accumulation_steps = cfg.accumulation_steps\n",
    "    effective_batch = cfg.batch_size * accumulation_steps\n",
    "    print(f\"gradient accumulation: {accumulation_steps} steps (effective batch = {effective_batch})\")\n",
    "\n",
    "    pbar = tqdm(total=cfg.distill_steps, desc='distilling (v10)')\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    while step < cfg.distill_steps:\n",
    "        for batch in train_loader:\n",
    "            if step >= cfg.distill_steps:\n",
    "                break\n",
    "\n",
    "            ids = batch[0].to(device, non_blocking=True)\n",
    "\n",
    "            with torch.cuda.amp.autocast():\n",
    "                # teacher forward (with hidden states only if needed)\n",
    "                with torch.no_grad():\n",
    "                    if cfg.hidden_align_weight > 0:\n",
    "                        t_out = teacher(ids, output_hidden_states=True)\n",
    "                        t_logits = t_out.logits\n",
    "                        t_hiddens = t_out.hidden_states\n",
    "                    else:\n",
    "                        t_logits = teacher(ids).logits\n",
    "\n",
    "                # student forward (with hidden states only if needed)\n",
    "                student.train()\n",
    "                if cfg.hidden_align_weight > 0:\n",
    "                    s_logits, s_hiddens = student(ids, return_hiddens=True)\n",
    "                else:\n",
    "                    s_logits = student(ids)\n",
    "\n",
    "                # kl divergence loss with T=2\n",
    "                T = cfg.temperature\n",
    "                s_log = F.log_softmax(s_logits / T, dim=-1)\n",
    "                t_prob = F.softmax(t_logits / T, dim=-1)\n",
    "                kl_loss = F.kl_div(\n",
    "                    s_log.view(-1, s_logits.size(-1)),\n",
    "                    t_prob.view(-1, t_logits.size(-1)),\n",
    "                    reduction='batchmean'\n",
    "                ) * (T ** 2)\n",
    "\n",
    "                # optional hidden-state alignment\n",
    "                if cfg.hidden_align_weight > 0:\n",
    "                    align_loss = compute_hidden_alignment_loss(\n",
    "                        t_hiddens, s_hiddens, projector,\n",
    "                        teacher_layers=cfg.teacher_n_layers,\n",
    "                        student_layers=cfg.n_layers\n",
    "                    )\n",
    "                    loss = kl_loss + cfg.hidden_align_weight * align_loss\n",
    "                else:\n",
    "                    align_loss = torch.tensor(0.0, device=device)\n",
    "                    loss = kl_loss\n",
    "\n",
    "                # v10: scale loss for gradient accumulation\n",
    "                loss = loss / accumulation_steps\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            accum_step += 1\n",
    "\n",
    "            # v10: only step optimizer after accumulation_steps\n",
    "            if accum_step % accumulation_steps == 0:\n",
    "                scaler.unscale_(optimizer)\n",
    "                gn = torch.nn.utils.clip_grad_norm_(all_params, cfg.max_grad_norm)\n",
    "\n",
    "                if torch.isfinite(gn):\n",
    "                    scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "                hw_stats.record_step(ids.size(0) * accumulation_steps, ids.size(1))\n",
    "                spike_stats.record(student, step)\n",
    "\n",
    "                current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "                # track losses (multiply back for logging)\n",
    "                training_logs['loss_history'].append({'step': step, 'loss': loss.item() * accumulation_steps})\n",
    "                training_logs['kl_loss_history'].append({'step': step, 'loss': kl_loss.item()})\n",
    "                training_logs['align_loss_history'].append({'step': step, 'loss': align_loss.item()})\n",
    "                training_logs['lr_history'].append({'step': step, 'lr': current_lr})\n",
    "\n",
    "                pbar.set_postfix(\n",
    "                    loss=f\"{loss.item() * accumulation_steps:.3f}\",\n",
    "                    kl=f\"{kl_loss.item():.3f}\",\n",
    "                    lr=f\"{current_lr:.1e}\"\n",
    "                )\n",
    "                pbar.update(1)\n",
    "                step += 1\n",
    "\n",
    "                if step % cfg.eval_interval == 0:\n",
    "                    val_loss = evaluate(student, val_loader, device)\n",
    "                    val_ppl = get_ppl(val_loss)\n",
    "                    training_logs['ppl_history'].append({'step': step, 'ppl': val_ppl})\n",
    "\n",
    "                    amps = student.get_amplitudes()\n",
    "                    amp_str = ', '.join([f\"L{i}:{amps[f'layer_{i}']['k']:.2f}\" for i in range(min(4, cfg.n_layers))])\n",
    "                    print(f\"\\n  step {step}: ppl={val_ppl:.1f}, amps=[{amp_str}...]\")\n",
    "\n",
    "                    if val_loss < best_val:\n",
    "                        best_val = val_loss\n",
    "                        torch.save({\n",
    "                            'student': student.state_dict(),\n",
    "                            'projector': projector.state_dict(),\n",
    "                        }, f'{OUTPUT_DIR}/checkpoints/v10_best.pt')\n",
    "\n",
    "    pbar.close()\n",
    "    return training_logs\n",
    "\n",
    "print(\"distillation function defined (v10 - with speedups)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# cell 18: run distillation\n",
    "# =============================================================================\n",
    "print(\"=\"*60)\n",
    "print(\"phase 1: distillation (v10 - 100M model with speedups)\")\n",
    "print(\"=\"*60)\n",
    "print(\"\")\n",
    "print(\"v10 architecture:\")\n",
    "print(f\"  d_model: 320 → {config.d_model} (+60%)\")\n",
    "print(f\"  n_layers: 5 → {config.n_layers} (+60%)\")\n",
    "print(f\"  params: ~22M → ~{student_params // 1_000_000}M\")\n",
    "print(\"\")\n",
    "print(\"v10 speedups:\")\n",
    "print(f\"  gradient checkpointing: {USE_GRADIENT_CHECKPOINTING}\")\n",
    "print(f\"  torch.compile: {compile_success}\")\n",
    "print(f\"  accumulation_steps: {config.accumulation_steps}\")\n",
    "print(f\"  fused optimizer: enabled\")\n",
    "print(\"\")\n",
    "print(\"settings (unchanged from v9):\")\n",
    "print(f\"  temperature: {config.temperature}\")\n",
    "print(f\"  warmup: {config.warmup_steps} steps\")\n",
    "print(f\"  hidden alignment: weight={config.hidden_align_weight} (disabled)\")\n",
    "print(\"\")\n",
    "\n",
    "hw_stats = HardwareStatsCollector()\n",
    "spike_stats = SpikeStatsCollector(config.n_layers)\n",
    "\n",
    "distill_logs = distill_v10(\n",
    "    teacher, student, projector, train_loader, val_loader,\n",
    "    config, DEVICE, hw_stats, spike_stats\n",
    ")\n",
    "\n",
    "print(\"\")\n",
    "print(f\"distillation complete!\")\n",
    "print(f\"throughput: {hw_stats.get_summary()['throughput_tokens_per_sec']:.0f} tokens/sec\")\n",
    "print(\"\")\n",
    "print(\"final amplitudes (first 4 layers):\")\n",
    "for i, (k, v) in enumerate(student.get_amplitudes().items()):\n",
    "    if i < 4:\n",
    "        print(f\"  {k}: k={v['k']:.4f}, v={v['v']:.4f}\")\n",
    "print(f\"  ... ({config.n_layers - 4} more layers)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. test-time training (ttt) with lora\n",
    "\n",
    "### 5.1 motivation\n",
    "\n",
    "test-time training adapts the model to new data distributions at inference time. lora (hu et al., 2022) provides efficient adaptation.\n",
    "\n",
    "### 5.2 note on current ttt\n",
    "\n",
    "current ttt trains on the same distribution (wikitext-2 val). future versions should implement \"triggered ttt\" on out-of-distribution data per paper section 7.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# cell 20: lora implementation (same as v9)\n",
    "# =============================================================================\n",
    "class LoRALinear(nn.Module):\n",
    "    \"\"\"lora adapter for linear layers.\"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, rank=8, alpha=16.0):\n",
    "        super().__init__()\n",
    "        self.scaling = alpha / rank\n",
    "        self.lora_A = nn.Parameter(torch.zeros(rank, in_features))\n",
    "        self.lora_B = nn.Parameter(torch.zeros(out_features, rank))\n",
    "        nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return (x @ self.lora_A.T @ self.lora_B.T) * self.scaling\n",
    "\n",
    "\n",
    "def apply_lora(model, rank=8, alpha=16.0, targets=['key_proj', 'value_proj']):\n",
    "    \"\"\"apply lora adapters to specified modules.\"\"\"\n",
    "    lora_modules = {}\n",
    "    for name, module in model.named_modules():\n",
    "        if any(t in name for t in targets) and isinstance(module, nn.Linear):\n",
    "            lora = LoRALinear(module.in_features, module.out_features, rank, alpha).to(next(module.parameters()).device)\n",
    "            lora_modules[name] = lora\n",
    "            orig_forward = module.forward\n",
    "            def make_forward(orig, lora_mod):\n",
    "                def forward(x):\n",
    "                    return orig(x) + lora_mod(x)\n",
    "                return forward\n",
    "            module.forward = make_forward(orig_forward, lora)\n",
    "    print(f\"lora: {len(lora_modules)} modules, rank={rank}\")\n",
    "    return lora_modules\n",
    "\n",
    "print(\"lora defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# cell 21: ttt with lora (same as v9)\n",
    "# =============================================================================\n",
    "print(\"=\"*60)\n",
    "print(\"phase 2: test-time training with lora\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for p in student.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "lora_modules = apply_lora(student, config.lora_rank, config.lora_alpha)\n",
    "lora_params = sum(p.numel() for m in lora_modules.values() for p in m.parameters())\n",
    "\n",
    "pre_ttt_loss = evaluate(student, val_loader, DEVICE)\n",
    "pre_ttt_ppl = get_ppl(pre_ttt_loss)\n",
    "print(f\"\\npre-ttt ppl: {pre_ttt_ppl:.2f}\")\n",
    "\n",
    "lora_opt = torch.optim.AdamW([p for m in lora_modules.values() for p in m.parameters()], lr=config.ttt_lr)\n",
    "ttt_logs = {'loss_history': []}\n",
    "student.train()\n",
    "\n",
    "for step, batch in enumerate(val_loader):\n",
    "    if step >= config.ttt_steps:\n",
    "        break\n",
    "    ids = batch[0].to(DEVICE)\n",
    "    with torch.cuda.amp.autocast():\n",
    "        loss = F.cross_entropy(student(ids)[:, :-1].reshape(-1, config.vocab_size), ids[:, 1:].reshape(-1))\n",
    "    lora_opt.zero_grad()\n",
    "    loss.backward()\n",
    "    lora_opt.step()\n",
    "    ttt_logs['loss_history'].append({'step': step, 'loss': loss.item()})\n",
    "    if step % 20 == 0:\n",
    "        print(f\"  ttt {step}: loss={loss.item():.4f}\")\n",
    "\n",
    "post_ttt_loss = evaluate(student, val_loader, DEVICE)\n",
    "post_ttt_ppl = get_ppl(post_ttt_loss)\n",
    "print(f\"\\npost-ttt ppl: {post_ttt_ppl:.2f}\")\n",
    "print(f\"ttt improvement: {pre_ttt_ppl - post_ttt_ppl:.1f} ppl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# cell 22: final evaluation\n",
    "# =============================================================================\n",
    "print(\"=\"*60)\n",
    "print(\"final evaluation\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "teacher_loss = evaluate(teacher, val_loader, DEVICE, is_gpt2=True)\n",
    "teacher_ppl = get_ppl(teacher_loss)\n",
    "student_loss = evaluate(student, val_loader, DEVICE)\n",
    "student_ppl = get_ppl(student_loss)\n",
    "\n",
    "# v10: VRAM logging\n",
    "vram_peak_gb = torch.cuda.max_memory_allocated() / 1e9 if torch.cuda.is_available() else 0\n",
    "\n",
    "print(f\"\")\n",
    "print(f\"{'model':<25} {'ppl':>10} {'params':>15}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'gpt-2 (teacher)':<25} {teacher_ppl:>10.2f} {teacher_params:>15,}\")\n",
    "print(f\"{'asnn-goose v10 (student)':<25} {student_ppl:>10.2f} {student_params:>15,}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'compression':<25} {compression_ratio:>10.1f}x\")\n",
    "print(f\"{'ppl gap':<25} {student_ppl - teacher_ppl:>10.2f}\")\n",
    "print(f\"{'spike density':<25} {student.get_avg_spike_density():>10.3f}\")\n",
    "print(f\"{'VRAM peak':<25} {vram_peak_gb:>10.2f}GB\")\n",
    "print(\"\")\n",
    "print(\"version comparison:\")\n",
    "print(f\"  v6: 627.3 PPL (baseline)\")\n",
    "print(f\"  v7: 1655 PPL (regression!)\")\n",
    "print(f\"  v8: 559 PPL (fixed)\")\n",
    "print(f\"  v9: 541.7 PPL (capacity increase)\")\n",
    "print(f\"  v10: {student_ppl:.2f} PPL (100M model)\")\n",
    "if student_ppl < 480:\n",
    "    print(f\"  v10 TARGET MET! PPL < 480\")\n",
    "elif student_ppl < 541.7:\n",
    "    print(f\"  v10 beats v9 by {541.7 - student_ppl:.1f} PPL\")\n",
    "else:\n",
    "    print(f\"  WARNING: v10 did not improve over v9\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# cell 23: visualization\n",
    "# =============================================================================\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# distillation loss\n",
    "d_steps = [l['step'] for l in distill_logs['loss_history']]\n",
    "d_losses = [l['loss'] for l in distill_logs['loss_history']]\n",
    "kl_losses = [l['loss'] for l in distill_logs['kl_loss_history']]\n",
    "axes[0,0].plot(d_steps, d_losses, label='total', alpha=0.8)\n",
    "axes[0,0].plot(d_steps, kl_losses, label='kl', alpha=0.7)\n",
    "axes[0,0].set_xlabel('step')\n",
    "axes[0,0].set_ylabel('loss')\n",
    "axes[0,0].set_title('distillation loss (v10)')\n",
    "axes[0,0].legend()\n",
    "\n",
    "# validation ppl\n",
    "p_steps = [l['step'] for l in distill_logs['ppl_history']]\n",
    "p_ppls = [l['ppl'] for l in distill_logs['ppl_history']]\n",
    "axes[0,1].plot(p_steps, p_ppls, 'orange', marker='o')\n",
    "axes[0,1].axhline(y=teacher_ppl, color='green', linestyle='--', label=f'teacher ({teacher_ppl:.1f})')\n",
    "axes[0,1].axhline(y=627.3, color='blue', linestyle=':', label='v6 (627.3)')\n",
    "axes[0,1].axhline(y=541.7, color='purple', linestyle=':', label='v9 (541.7)')\n",
    "axes[0,1].axhline(y=480, color='red', linestyle='--', label='v10 target (480)')\n",
    "axes[0,1].set_xlabel('step')\n",
    "axes[0,1].set_ylabel('ppl')\n",
    "axes[0,1].set_title('validation ppl')\n",
    "axes[0,1].legend()\n",
    "\n",
    "# lr schedule\n",
    "lr_steps = [l['step'] for l in distill_logs['lr_history']]\n",
    "lr_vals = [l['lr'] for l in distill_logs['lr_history']]\n",
    "axes[0,2].plot(lr_steps, lr_vals, 'purple')\n",
    "axes[0,2].axvline(x=config.warmup_steps, color='gray', linestyle='--', label=f'warmup ({config.warmup_steps})')\n",
    "axes[0,2].set_xlabel('step')\n",
    "axes[0,2].set_ylabel('lr')\n",
    "axes[0,2].set_title('learning rate')\n",
    "axes[0,2].legend()\n",
    "\n",
    "# spike density + amplitudes (first 4 layers)\n",
    "spike_summary = spike_stats.get_summary()\n",
    "layers = [f'layer_{i}' for i in range(min(4, config.n_layers))]\n",
    "k_dens = [spike_summary['per_layer'][l]['k_final'] for l in layers]\n",
    "v_dens = [spike_summary['per_layer'][l]['v_final'] for l in layers]\n",
    "k_amps = [spike_summary['per_layer'][l]['k_amp_final'] for l in layers]\n",
    "v_amps = [spike_summary['per_layer'][l]['v_amp_final'] for l in layers]\n",
    "\n",
    "x = np.arange(len(layers))\n",
    "axes[1,0].bar(x - 0.2, k_dens, 0.4, label='k density')\n",
    "axes[1,0].bar(x + 0.2, v_dens, 0.4, label='v density')\n",
    "ax2 = axes[1,0].twinx()\n",
    "ax2.plot(x, k_amps, 'r-o', label='k amp')\n",
    "ax2.plot(x, v_amps, 'b-s', label='v amp')\n",
    "axes[1,0].set_xlabel('layer')\n",
    "axes[1,0].set_ylabel('density')\n",
    "ax2.set_ylabel('amplitude')\n",
    "axes[1,0].set_title(f'spike density & amps (first 4/{config.n_layers} layers)')\n",
    "axes[1,0].legend(loc='upper left')\n",
    "ax2.legend(loc='upper right')\n",
    "\n",
    "# ttt loss\n",
    "t_steps = [l['step'] for l in ttt_logs['loss_history']]\n",
    "t_losses = [l['loss'] for l in ttt_logs['loss_history']]\n",
    "axes[1,1].plot(t_steps, t_losses, 'red')\n",
    "axes[1,1].set_xlabel('step')\n",
    "axes[1,1].set_ylabel('ce loss')\n",
    "axes[1,1].set_title('ttt with lora')\n",
    "\n",
    "# version comparison\n",
    "versions = ['v6', 'v7', 'v8', 'v9', 'v10']\n",
    "t_ppls = [44.6, 44.6, 44.6, 44.6, teacher_ppl]\n",
    "s_ppls = [627.3, 1655, 559, 541.7, student_ppl]\n",
    "x = np.arange(len(versions))\n",
    "axes[1,2].bar(x - 0.2, t_ppls, 0.4, label='teacher', alpha=0.7)\n",
    "axes[1,2].bar(x + 0.2, s_ppls, 0.4, label='student', alpha=0.7)\n",
    "axes[1,2].axhline(y=480, color='red', linestyle='--', label='v10 target', alpha=0.7)\n",
    "axes[1,2].set_xticks(x)\n",
    "axes[1,2].set_xticklabels(versions)\n",
    "axes[1,2].set_ylabel('ppl')\n",
    "axes[1,2].set_title('version comparison')\n",
    "axes[1,2].legend()\n",
    "axes[1,2].set_yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "figure_path = f'{OUTPUT_DIR}/figures/v10_training_{RUN_TIMESTAMP}.png'\n",
    "plt.savefig(figure_path, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"saved: {figure_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# cell 24: build results dict (DRAFT - validation_tests added in cell 26)\n",
    "# =============================================================================\n",
    "print(\"building results (draft - validation_tests added after cell 26)...\")\n",
    "\n",
    "with open(figure_path, 'rb') as f:\n",
    "    figure_base64 = base64.b64encode(f.read()).decode('utf-8')\n",
    "\n",
    "results = {\n",
    "    'version': 'v10',\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'run_id': RUN_TIMESTAMP,\n",
    "    'platform': PLATFORM,\n",
    "    'description': '100M model with speedups (512d, 8L)',\n",
    "\n",
    "    'v10_design': {\n",
    "        'principle': '100M model with training speedups',\n",
    "        'changes': {\n",
    "            'd_model': '320 → 512 (+60%)',\n",
    "            'n_layers': '5 → 8 (+60%)',\n",
    "            'params': '~22M → ~100M (+350%)',\n",
    "            'teacher_indices': '[2,5,7,10,12] → [1,2,4,5,7,8,10,11]',\n",
    "        },\n",
    "        'speedups': {\n",
    "            'gradient_checkpointing': USE_GRADIENT_CHECKPOINTING,\n",
    "            'torch_compile': compile_success,\n",
    "            'fused_optimizer': True,\n",
    "            'accumulation_steps': config.accumulation_steps,\n",
    "        },\n",
    "        'unchanged': [\n",
    "            'temperature: 2.0',\n",
    "            'hidden_align_weight: 0.0',\n",
    "            'warmup_steps: 50',\n",
    "            'distill_steps: 3000',\n",
    "        ],\n",
    "    },\n",
    "\n",
    "    'architecture': {\n",
    "        'teacher': {'name': 'gpt2', 'params': teacher_params},\n",
    "        'student': {\n",
    "            'name': 'asnn-goose-v10',\n",
    "            'd_model': config.d_model,\n",
    "            'n_layers': config.n_layers,\n",
    "            'params': student_params,\n",
    "        },\n",
    "        'projector_params': projector_params,\n",
    "        'compression_ratio': compression_ratio,\n",
    "        'vram_peak_gb': vram_peak_gb,\n",
    "    },\n",
    "\n",
    "    'training_config': {\n",
    "        'distill_steps': config.distill_steps,\n",
    "        'temperature': config.temperature,\n",
    "        'hidden_align_weight': config.hidden_align_weight,\n",
    "        'warmup_steps': config.warmup_steps,\n",
    "        'batch_size': config.batch_size,\n",
    "        'accumulation_steps': config.accumulation_steps,\n",
    "        'effective_batch': config.batch_size * config.accumulation_steps,\n",
    "        'distill_lr': config.distill_lr,\n",
    "        'max_grad_norm': config.max_grad_norm,\n",
    "    },\n",
    "\n",
    "    'results': {\n",
    "        'teacher_ppl': teacher_ppl,\n",
    "        'student_ppl': student_ppl,\n",
    "        'ppl_gap': student_ppl - teacher_ppl,\n",
    "        'spike_density': student.get_avg_spike_density(),\n",
    "        'amplitudes': student.get_amplitudes(),\n",
    "        'target_met': student_ppl < 480,\n",
    "    },\n",
    "\n",
    "    'training_curves': {\n",
    "        'loss_history': distill_logs['loss_history'],\n",
    "        'kl_loss_history': distill_logs['kl_loss_history'],\n",
    "        'align_loss_history': distill_logs['align_loss_history'],\n",
    "        'ppl_history': distill_logs['ppl_history'],\n",
    "        'lr_history': distill_logs['lr_history'],\n",
    "    },\n",
    "\n",
    "    'hardware_stats': hw_stats.get_summary(),\n",
    "    'spike_analysis': spike_stats.get_summary(),\n",
    "\n",
    "    'ttt': {\n",
    "        'lora_params': lora_params,\n",
    "        'pre_ppl': pre_ttt_ppl,\n",
    "        'post_ppl': post_ttt_ppl,\n",
    "        'improvement': pre_ttt_ppl - post_ttt_ppl,\n",
    "        'loss_history': ttt_logs['loss_history'],\n",
    "    },\n",
    "\n",
    "    'comparison': {\n",
    "        'v6': {'student_ppl': 627.3, 'note': 'baseline'},\n",
    "        'v7': {'student_ppl': 1655, 'note': 'regression (align=1.0, T=4)'},\n",
    "        'v8': {'student_ppl': 559, 'note': 'fixed defaults (align=0, T=2)'},\n",
    "        'v9': {'student_ppl': 541.7, 'note': 'capacity increase (320d, 5L)'},\n",
    "        'v10': {'student_ppl': student_ppl, 'note': '100M model (512d, 8L)'},\n",
    "    },\n",
    "\n",
    "    'figures': {\n",
    "        'training_plot': {\n",
    "            'filename': f'v10_training_{RUN_TIMESTAMP}.png',\n",
    "            'base64': figure_base64,\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    # validation_tests will be added in cell 26\n",
    "}\n",
    "\n",
    "print(\"results dict built (validation_tests pending)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# cell 25: validation tests (9 total)\n# =============================================================================\nprint(\"=\"*60)\nprint(\"validation tests (v10)\")\nprint(\"=\"*60)\n\ntest_results = {}\n\n# 1. teacher pre-trained\nprint(\"\\n[1] teacher pre-trained\")\ntest_results['teacher_pretrained'] = teacher_ppl < 50\nprint(f\"  teacher ppl: {teacher_ppl:.2f} - {'pass' if test_results['teacher_pretrained'] else 'fail'}\")\n\n# 2. student learned (v10 target: <480)\nprint(\"\\n[2] student learned language\")\ntest_results['student_learned'] = student_ppl < 627\nprint(f\"  student ppl: {student_ppl:.2f} - {'pass' if test_results['student_learned'] else 'fail'} (target: < 627)\")\n\n# 3. ternary activations\nprint(\"\\n[3] ternary activations\")\nstudent.eval()\nwith torch.no_grad():\n    test_ids = next(iter(val_loader))[0].to(DEVICE)\n    layer = student.layers[0]['rec']\n    x = student.embed(test_ids) + student.pos_embed(torch.arange(test_ids.size(1), device=DEVICE).unsqueeze(0))\n    x_norm = layer.ln(x)\n    prev_x = F.pad(x_norm[:, :-1, :], (0, 0, 1, 0))\n    xk = x_norm * layer.time_mix_k + prev_x * (1 - layer.time_mix_k)\n    k_spike = layer.k_spike(layer.key_proj(xk))\n    unique_vals = len(set([round(v, 4) for v in k_spike.unique().cpu().tolist()]))\n    test_results['ternary'] = unique_vals <= 3\n    print(f\"  unique values: {unique_vals} - {'pass' if test_results['ternary'] else 'fail'}\")\n\n# 4. gradient flow\nprint(\"\\n[4] gradient flow (STE)\")\n_spike = TrainableTernarySpike().to(DEVICE)\n_x = torch.randn(2, 16, 64, device=DEVICE, requires_grad=True)\n_spike(_x).sum().backward()\ntest_results['gradient'] = _x.grad is not None and _x.grad.abs().sum() > 0\nprint(f\"  {'pass' if test_results['gradient'] else 'fail'}\")\n\n# 5. spike density\nprint(\"\\n[5] spike density in range\")\ndensity = student.get_avg_spike_density()\ntest_results['density'] = 0.1 < density < 0.9\nprint(f\"  density: {density:.3f} - {'pass' if test_results['density'] else 'fail'}\")\n\n# 6. lora applied\nprint(\"\\n[6] lora applied\")\ntest_results['lora'] = len(lora_modules) > 0\nprint(f\"  modules: {len(lora_modules)} - {'pass' if test_results['lora'] else 'fail'}\")\n\n# 7. improvement over v6 baseline\nprint(\"\\n[7] beats v6 baseline\")\ntest_results['beats_v6'] = student_ppl < 627.3\nprint(f\"  v6: 627.3, v10: {student_ppl:.2f} - {'pass' if test_results['beats_v6'] else 'fail'}\")\n\n# 8. amplitudes learned\nprint(\"\\n[8] amplitudes learned\")\namps = student.get_amplitudes()\ntest_results['amplitudes_learned'] = any(\n    abs(amps[f'layer_{i}']['k'] - 1.0) > 0.05 or abs(amps[f'layer_{i}']['v'] - 1.0) > 0.05\n    for i in range(config.n_layers)\n)\n# v10 fix: avoid nested f-strings with escaped quotes (causes SyntaxError)\namp_strs = [f\"L{i}:{amps[f'layer_{i}']['k']:.3f}\" for i in range(min(4, config.n_layers))]\nprint(f\"  amplitudes (first 4): {amp_strs}\")\nprint(f\"  {'pass' if test_results['amplitudes_learned'] else 'fail'} - any amplitude != 1.0 by > 0.05\")\n\n# 9. amplitude health check\nprint(\"\\n[9] amplitude health\")\nall_healthy = True\nfor layer_idx, amp_dict in amps.items():\n    k_amp, v_amp = amp_dict['k'], amp_dict['v']\n    if not (0.3 < k_amp < 3.0) or not (0.3 < v_amp < 3.0):\n        print(f\"  WARNING: {layer_idx} unhealthy: k={k_amp:.3f}, v={v_amp:.3f}\")\n        all_healthy = False\ntest_results['amplitude_health'] = all_healthy\nprint(f\"  {'pass' if all_healthy else 'fail'} - all amplitudes in [0.3, 3.0]\")\n\nprint(\"\\n\" + \"=\"*60)\npassed = sum(1 for v in test_results.values() if v)\nprint(f\"results: {passed}/{len(test_results)} passed\")\nif student_ppl < 480:\n    print(f\"v10 TARGET MET: PPL {student_ppl:.2f} < 480\")\nelif student_ppl < 541.7:\n    print(f\"v10 improved from v9: {541.7 - student_ppl:.1f} PPL reduction\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# cell 26: FINAL save with validation_tests (v10 bug fix)\n",
    "# =============================================================================\n",
    "print(\"=\"*60)\n",
    "print(\"FINAL SAVE (includes validation_tests)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Add validation_tests to results\n",
    "results['validation_tests'] = test_results\n",
    "\n",
    "# Save final results.json with ALL data\n",
    "results_path = f'{OUTPUT_DIR}/results/results_{RUN_TIMESTAMP}.json'\n",
    "with open(results_path, 'w') as f:\n",
    "    json.dump(results, f, indent=2, default=str)\n",
    "\n",
    "print(f\"saved: {results_path}\")\n",
    "print(f\"size: {os.path.getsize(results_path) / 1024:.1f} KB\")\n",
    "print(f\"\")\n",
    "print(f\"validation_tests included: {list(test_results.keys())}\")\n",
    "\n",
    "# v10: auto-download AFTER final save\n",
    "print(\"\")\n",
    "print(\"auto-download\")\n",
    "if IS_COLAB:\n",
    "    try:\n",
    "        from google.colab import files\n",
    "        files.download(results_path)\n",
    "        files.download(figure_path)\n",
    "        print(\"downloads started!\")\n",
    "    except Exception as e:\n",
    "        print(f\"download failed: {e}\")\n",
    "elif IS_KAGGLE:\n",
    "    print(f\"kaggle: {results_path}\")\n",
    "else:\n",
    "    print(f\"local: {results_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. summary\n",
    "\n",
    "### 6.1 v10 design principle\n",
    "\n",
    "**100M model with training speedups**\n",
    "\n",
    "| attribute | v9 | v10 | change |\n",
    "|-----------|-----|-----|--------|\n",
    "| d_model | 320 | **512** | +60% |\n",
    "| n_layers | 5 | **8** | +60% |\n",
    "| params | ~22M | **~100M** | +350% |\n",
    "| VRAM | ~3.7GB | **~8-10GB** | +170% |\n",
    "| temperature | 2.0 | 2.0 | - |\n",
    "| hidden_align_weight | 0.0 | 0.0 | - |\n",
    "\n",
    "### 6.2 speedups\n",
    "\n",
    "| technique | implementation |\n",
    "|-----------|----------------|\n",
    "| gradient checkpointing | `checkpoint(..., use_reentrant=False)` |\n",
    "| torch.compile | `torch.compile(model, mode='reduce-overhead')` |\n",
    "| fused optimizer | `AdamW(..., fused=True)` |\n",
    "| gradient accumulation | `accumulation_steps=2` |\n",
    "\n",
    "### 6.3 version progression\n",
    "\n",
    "| version | teacher ppl | student ppl | key change |\n",
    "|---------|-------------|-------------|------------|\n",
    "| v6 | 44.6 | **627** | gpt-2 distillation |\n",
    "| v7 | 44.6 | 1655 | **regression** (align=1.0, T=4) |\n",
    "| v8 | 44.6 | **559** | fixed defaults |\n",
    "| v9 | 44.6 | **541.7** | capacity increase (320d, 5L) |\n",
    "| **v10** | ~45 | **target: <480** | **100M model + speedups** |\n",
    "\n",
    "### 6.4 validation tests (9 total)\n",
    "\n",
    "1. teacher pre-trained (PPL < 50)\n",
    "2. student learned (PPL < 627)\n",
    "3. ternary activations verified\n",
    "4. gradient flow via STE\n",
    "5. spike density in [0.1, 0.9]\n",
    "6. LoRA applied\n",
    "7. beats v6 baseline\n",
    "8. amplitudes learned\n",
    "9. amplitude health [0.3, 3.0]\n",
    "\n",
    "### 6.5 next steps\n",
    "\n",
    "if v10 succeeds (PPL < 480):\n",
    "1. v11: curriculum temperature (CTKD)\n",
    "2. v12: progressive training\n",
    "3. v13: patient training (5000 steps)\n",
    "\n",
    "---\n",
    "\n",
    "*asnn-goose v10 - eptesicus laboratories - lumis-next initiative*\n",
    "\n",
    "### references\n",
    "\n",
    "- hinton, g., vinyals, o., & dean, j. (2015). distilling the knowledge in a neural network.\n",
    "- radford, a., et al. (2019). language models are unsupervised multitask learners.\n",
    "- chen, t., et al. (2016). training deep nets with sublinear memory cost.\n",
    "- pytorch 2.0 documentation: torch.compile."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}