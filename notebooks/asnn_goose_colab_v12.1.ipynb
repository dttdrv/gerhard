{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# cell 1: environment setup (v11 - POCL + Channel-wise)\n",
    "# =============================================================================\n",
    "import os\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import json\n",
    "import base64\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Dict, List, Optional, Tuple, Any\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# v11: torch.compile flag (disable if causing issues)\n",
    "USE_TORCH_COMPILE = True\n",
    "USE_GRADIENT_CHECKPOINTING = True\n",
    "\n",
    "# generate timestamp for this run\n",
    "RUN_TIMESTAMP = datetime.now().strftime('%Y-%m-%d_%H%M%S')\n",
    "print(f\"run timestamp: {RUN_TIMESTAMP}\")\n",
    "\n",
    "# detect platform\n",
    "IS_KAGGLE = 'KAGGLE_KERNEL_RUN_TYPE' in os.environ\n",
    "IS_COLAB = 'COLAB_GPU' in os.environ or 'google.colab' in sys.modules\n",
    "PLATFORM = 'kaggle' if IS_KAGGLE else 'colab' if IS_COLAB else 'local'\n",
    "OUTPUT_DIR = '/kaggle/working/outputs' if IS_KAGGLE else 'outputs'\n",
    "\n",
    "for subdir in ['figures', 'checkpoints', 'logs', 'results']:\n",
    "    os.makedirs(f'{OUTPUT_DIR}/{subdir}', exist_ok=True)\n",
    "\n",
    "print(f\"platform: {PLATFORM}\")\n",
    "print(f\"output directory: {OUTPUT_DIR}\")\n",
    "print(f\"torch.compile: {'enabled' if USE_TORCH_COMPILE else 'disabled'}\")\n",
    "print(f\"gradient checkpointing: {'enabled' if USE_GRADIENT_CHECKPOINTING else 'disabled'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# cell 2: pytorch and hardware setup (v11 - POCL)\n",
    "# =============================================================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "SEED = 42\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"gpu: {gpu_name}\")\n",
    "    print(f\"memory: {gpu_memory:.1f} gb\")\n",
    "\n",
    "# v11: set float32 matmul precision for torch.compile\n",
    "if USE_TORCH_COMPILE and hasattr(torch, 'set_float32_matmul_precision'):\n",
    "    torch.set_float32_matmul_precision('high')\n",
    "    print(\"float32 matmul precision: high (for torch.compile)\")\n",
    "\n",
    "print(f\"device: {DEVICE}\")\n",
    "print(f\"pytorch: {torch.__version__}\")\n",
    "\n",
    "# check torch.compile availability\n",
    "TORCH_COMPILE_AVAILABLE = hasattr(torch, 'compile') and torch.__version__ >= '2.0'\n",
    "print(f\"torch.compile available: {TORCH_COMPILE_AVAILABLE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# cell 4: configuration (v12.1 - PROPER CTKD with GRL)\n# =============================================================================\n@dataclass\nclass Config:\n    # gpt-2 teacher (frozen, pre-trained)\n    teacher_name: str = \"gpt2\"\n\n    # student model architecture - v12.1: using v10 baseline (320d, 5L, ~22M)\n    d_model: int = 320      # v10 value (DO NOT reduce)\n    n_layers: int = 5       # v10 value (DO NOT reduce)\n    vocab_size: int = 50257\n    max_seq_len: int = 256\n\n    # distillation training\n    distill_steps: int = 3000\n    distill_lr: float = 3e-4\n    warmup_steps: int = 50        # minimal warmup for LR\n\n    # v12.1: gradient accumulation (same as v10)\n    accumulation_steps: int = 2   # effective batch = 8 * 2 = 16\n\n    # v12.1: HIDDEN ALIGNMENT DISABLED (caused regression in v7)\n    hidden_align_weight: float = 0.0   # DISABLED\n    teacher_d_model: int = 768         # gpt-2 hidden dim\n    teacher_n_layers: int = 12         # gpt-2 layers\n\n    # ==========================================================================\n    # v12.1: PROPER CTKD Configuration (with Gradient Reversal Layer)\n    # ==========================================================================\n    use_ctkd: bool = True                  # v12.1: ENABLED (proper CTKD with GRL)\n    \n    # Temperature bounds (conservative for LLMs - paper uses [1,21] for images)\n    tau_min: float = 1.0                   # Minimum temperature\n    tau_max: float = 5.0                   # Maximum temperature (5.0 is conservative)\n    tau_init: float = 2.0                  # Initial temperature\n    \n    # Lambda (adversarial strength) scheduling\n    lambda_max: float = 1.0                # Maximum adversarial strength\n    lambda_warmup_ratio: float = 0.2       # 20% warmup (\u03bb=0 for first 600 steps)\n    \n    # Legacy flags (disabled)\n    use_learnable_temperature: bool = False  # DEPRECATED: Use use_ctkd instead\n    use_channel_wise_spikes: bool = False    # DISABLED (structural symmetry issue)\n    use_progressive_stages: bool = False     # DISABLED\n    temperature: float = 2.0                 # Legacy, used if CTKD disabled\n    temperature_lr: float = 0.001            # Legacy, not used with CTKD\n\n    # lora for ttt\n    lora_rank: int = 8\n    lora_alpha: float = 16.0\n    ttt_lr: float = 1e-4\n    ttt_steps: int = 100\n\n    # spiking parameters\n    spike_alpha: float = 1.0\n\n    # general training\n    batch_size: int = 8\n    max_grad_norm: float = 1.0\n    eval_interval: int = 300  # reduced from 100 for speed\n\nconfig = Config()\n\nprint(f\"configuration (v12.1 - PROPER CTKD with Gradient Reversal Layer):\")\nprint(f\"  teacher: {config.teacher_name} (124m params)\")\nprint(f\"  student: d={config.d_model}, layers={config.n_layers}\")\nprint(f\"\")\nprint(f\"v12.1 INNOVATION - Proper CTKD (ArXiv 2211.16231):\")\nprint(f\"  use_ctkd: {config.use_ctkd} <-- v12.1 ENABLED\")\nprint(f\"\")\nprint(f\"  CTKD uses Gradient Reversal Layer (GRL) for ADVERSARIAL temperature:\")\nprint(f\"    - Student MINIMIZES KL loss (normal gradients)\")\nprint(f\"    - Temperature MAXIMIZES KL loss (reversed gradients)\")\nprint(f\"    - Result: Temperature finds OPTIMAL difficulty, not 'easy' solution\")\nprint(f\"\")\nprint(f\"  Temperature bounds (sigmoid, smooth gradients):\")\nprint(f\"    - \u03c4_min: {config.tau_min}\")\nprint(f\"    - \u03c4_max: {config.tau_max} (conservative for LLMs)\")\nprint(f\"    - \u03c4_init: {config.tau_init}\")\nprint(f\"\")\nprint(f\"  Lambda schedule (adversarial strength):\")\nprint(f\"    - \u03bb_max: {config.lambda_max}\")\nprint(f\"    - warmup: {config.lambda_warmup_ratio*100:.0f}% ({int(config.distill_steps * config.lambda_warmup_ratio)} steps)\")\nprint(f\"    - Schedule: cosine 0 \u2192 {config.lambda_max}\")\nprint(f\"\")\nprint(f\"v12.1 vs v12 (broken):\")\nprint(f\"  v12:   Both minimize \u2192 T finds 'easy' solution \u2192 runaway\")\nprint(f\"  v12.1: Min-max (GRL) \u2192 T finds 'hardest' difficulty \u2192 proper learning\")\nprint(f\"\")\nprint(f\"disabled features:\")\nprint(f\"  channel-wise spikes: {config.use_channel_wise_spikes} (structural symmetry issue)\")\nprint(f\"  progressive stages: {config.use_progressive_stages}\")\nprint(f\"  hidden alignment: {config.hidden_align_weight}\")\nprint(f\"\")\nprint(f\"training:\")\nprint(f\"  distillation: {config.distill_steps} steps\")\nprint(f\"  warmup (LR): {config.warmup_steps} steps\")\nprint(f\"  accumulation: {config.accumulation_steps} (effective batch = {config.batch_size * config.accumulation_steps})\")\nprint(f\"\")\nprint(f\"targets:\")\nprint(f\"  PPL: <500 (improve on v10's 514.5)\")\nprint(f\"  Temperature: should evolve meaningfully (not stuck at init or bounds)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# cell 6: v12.1 PROPER CTKD Implementation\n# =============================================================================\n# References:\n# - CTKD Paper: https://arxiv.org/abs/2211.16231\n# - GRL Origin: Ganin & Lempitsky (2015) https://arxiv.org/abs/1409.7495\n# - torch-gradient-reversal: https://pypi.org/project/torch-gradient-reversal/\n\n# -----------------------------------------------------------------------------\n# GradientReversalFunction (Custom Autograd)\n# -----------------------------------------------------------------------------\nclass GradientReversalFunction(torch.autograd.Function):\n    \"\"\"\n    Gradient Reversal Layer for adversarial training.\n    \n    Forward: Identity mapping f(x) = x\n    Backward: Negates gradient \u2202f/\u2202x = -\u03bb * grad\n    \n    This enables min-max optimization in a single backward pass:\n    - Student minimizes loss (normal gradients)\n    - Temperature maximizes loss (reversed gradients via GRL)\n    \n    Reference: Ganin & Lempitsky, \"Unsupervised Domain Adaptation by Backpropagation\"\n    \"\"\"\n    \n    @staticmethod\n    def forward(ctx, x, lambda_):\n        # Save lambda for backward pass\n        ctx.lambda_ = lambda_\n        # Forward is identity (must clone to avoid in-place issues)\n        return x.clone()\n    \n    @staticmethod\n    def backward(ctx, grad_output):\n        # Backward negates and scales gradient\n        # Returns: (grad for x, grad for lambda_)\n        # lambda_ is a hyperparameter, doesn't need gradient\n        return -ctx.lambda_ * grad_output, None\n\n\nclass GradientReversalLayer(nn.Module):\n    \"\"\"\n    Module wrapper for GradientReversalFunction.\n    \n    Usage:\n        grl = GradientReversalLayer()\n        grl.set_lambda(0.5)  # Set adversarial strength\n        y = grl(x)  # Forward: y = x, Backward: grad_x = -0.5 * grad_y\n    \"\"\"\n    \n    def __init__(self):\n        super().__init__()\n        self.lambda_ = 1.0\n    \n    def set_lambda(self, lambda_: float):\n        \"\"\"Set the adversarial strength (0 = no reversal, 1 = full reversal).\"\"\"\n        self.lambda_ = lambda_\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return GradientReversalFunction.apply(x, self.lambda_)\n\n\n# -----------------------------------------------------------------------------\n# Lambda Scheduler (Cosine with Warmup)\n# -----------------------------------------------------------------------------\ndef get_lambda(step: int, total_steps: int, lambda_max: float = 1.0, \n               warmup_ratio: float = 0.2) -> float:\n    \"\"\"\n    Cosine schedule for adversarial strength \u03bb.\n    \n    - During warmup (first warmup_ratio of training): \u03bb = 0\n      Temperature learns freely to find reasonable range\n    - After warmup: \u03bb increases from 0 to lambda_max via cosine\n      Gradually increases adversarial pressure\n    \n    Args:\n        step: Current training step\n        total_steps: Total number of training steps\n        lambda_max: Maximum \u03bb value (default 1.0 = full reversal)\n        warmup_ratio: Fraction of training for warmup (default 0.2 = 20%)\n    \n    Returns:\n        Current \u03bb value in [0, lambda_max]\n    \"\"\"\n    warmup_steps = int(total_steps * warmup_ratio)\n    \n    if step < warmup_steps:\n        return 0.0\n    \n    # Progress after warmup [0, 1]\n    progress = (step - warmup_steps) / max(total_steps - warmup_steps, 1)\n    # Cosine increase from 0 to lambda_max\n    lambda_ = lambda_max * (1 - math.cos(math.pi * progress)) / 2\n    return lambda_\n\n\n# -----------------------------------------------------------------------------\n# CTKDTemperature (Proper Implementation with GRL)\n# -----------------------------------------------------------------------------\nclass CTKDTemperature(nn.Module):\n    \"\"\"\n    Curriculum Temperature for Knowledge Distillation (CTKD).\n    \n    Key features:\n    1. Adversarial learning via Gradient Reversal Layer\n    2. Sigmoid bounding for smooth gradients at boundaries\n    3. Proper initialization via logit transform\n    \n    The temperature module tries to MAXIMIZE the KL loss (via GRL),\n    finding the \"hardest\" temperature for the student.\n    The student tries to MINIMIZE the KL loss.\n    This adversarial game leads to optimal curriculum difficulty.\n    \n    Reference: Li et al., \"Curriculum Temperature for Knowledge Distillation\", AAAI 2023\n    \"\"\"\n    \n    def __init__(self, tau_min: float = 1.0, tau_max: float = 5.0, init: float = 2.0):\n        \"\"\"\n        Args:\n            tau_min: Minimum temperature (default 1.0)\n            tau_max: Maximum temperature (default 5.0, conservative for LLMs)\n            init: Initial temperature (default 2.0)\n        \"\"\"\n        super().__init__()\n        self.tau_min = tau_min\n        self.tau_range = tau_max - tau_min\n        \n        # Initialize raw parameter so sigmoid outputs init value\n        # sigmoid(raw) = (init - tau_min) / tau_range\n        # raw = logit((init - tau_min) / tau_range)\n        init_normalized = (init - tau_min) / self.tau_range\n        init_normalized = max(0.01, min(0.99, init_normalized))  # Clamp for numerical stability\n        init_raw = math.log(init_normalized / (1 - init_normalized))  # logit function\n        \n        self.raw_temp = nn.Parameter(torch.tensor(init_raw, dtype=torch.float32))\n        self.grl = GradientReversalLayer()\n        \n        # Store config for logging\n        self.tau_min_val = tau_min\n        self.tau_max_val = tau_max\n        self.init_val = init\n    \n    def forward(self, lambda_: float) -> torch.Tensor:\n        \"\"\"\n        Compute temperature with GRL applied.\n        \n        Args:\n            lambda_: Current adversarial strength from scheduler\n        \n        Returns:\n            Temperature \u03c4 \u2208 [tau_min, tau_max]\n        \"\"\"\n        # Set GRL strength\n        self.grl.set_lambda(lambda_)\n        \n        # Apply GRL to raw parameter (this is where gradient reversal happens!)\n        raw_reversed = self.grl(self.raw_temp)\n        \n        # Sigmoid bounding (smooth, differentiable at boundaries)\n        tau = self.tau_min + self.tau_range * torch.sigmoid(raw_reversed)\n        \n        return tau\n    \n    def get_temperature(self) -> float:\n        \"\"\"Get current temperature without GRL (for logging/display).\"\"\"\n        with torch.no_grad():\n            tau = self.tau_min + self.tau_range * torch.sigmoid(self.raw_temp)\n            return tau.item()\n    \n    def get_raw_value(self) -> float:\n        \"\"\"Get raw (unbounded) parameter value (for debugging).\"\"\"\n        return self.raw_temp.item()\n\n\n# -----------------------------------------------------------------------------\n# Legacy Classes (kept for backward compatibility)\n# -----------------------------------------------------------------------------\nclass LearnableTemperature(nn.Module):\n    \"\"\"\n    DEPRECATED: Simple learnable temperature WITHOUT GRL.\n    Kept for backward compatibility. Use CTKDTemperature instead.\n    \n    WARNING: This class caused temperature runaway in v12!\n    \"\"\"\n    \n    def __init__(self, init: float = 2.0):\n        super().__init__()\n        self.log_temp = nn.Parameter(torch.log(torch.tensor(init)))\n    \n    def forward(self) -> torch.Tensor:\n        return torch.exp(self.log_temp).clamp(1.0, 10.0)\n    \n    def get_temperature(self) -> float:\n        return self.forward().item()\n\n\nclass ChannelWiseTernarySpike(nn.Module):\n    \"\"\"\n    Per-channel learnable alpha and amplitude for ternary spikes.\n    DISABLED in v12.1 due to structural symmetry issue with RWKV.\n    \"\"\"\n    \n    def __init__(self, d_model: int, alpha_init: float = 1.0):\n        super().__init__()\n        self.d_model = d_model\n        self.alpha = nn.Parameter(torch.ones(d_model) * alpha_init)\n        self.amplitude = nn.Parameter(torch.ones(d_model))\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x_abs_mean = x.abs().mean(dim=(0, 1), keepdim=True)\n        threshold = self.alpha * x_abs_mean\n        threshold = threshold.clamp(min=0.01, max=10.0)\n        \n        with torch.no_grad():\n            pos_mask = (x > threshold).float()\n            neg_mask = (x < -threshold).float()\n            spike_signs = pos_mask - neg_mask\n        \n        spikes = self.amplitude * spike_signs\n        return spikes + (x - x.detach())\n    \n    def get_amplitude(self) -> float:\n        return self.amplitude.mean().item()\n    \n    def get_stats(self) -> dict:\n        return {\n            'alpha_mean': self.alpha.mean().item(),\n            'alpha_std': self.alpha.std().item(),\n            'amplitude_mean': self.amplitude.mean().item(),\n            'amplitude_std': self.amplitude.std().item(),\n        }\n\n    def get_amplitude_stats(self) -> dict:\n        return {\n            'mean': self.amplitude.mean().item(),\n            'std': self.amplitude.std().item(),\n            'min': self.amplitude.min().item(),\n            'max': self.amplitude.max().item(),\n        }\n\n\nclass TrainableTernarySpike(nn.Module):\n    \"\"\"Original trainable ternary spike with scalar amplitude (from v8).\"\"\"\n\n    def __init__(self, alpha: float = 1.0):\n        super().__init__()\n        self.alpha = alpha\n        self.amplitude = nn.Parameter(torch.ones(1))\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        threshold = self.alpha * x.abs().mean(dim=-1, keepdim=True)\n        threshold = threshold.clamp(min=0.01, max=10.0)\n\n        with torch.no_grad():\n            pos_mask = (x > threshold).float()\n            neg_mask = (x < -threshold).float()\n            spike_signs = pos_mask - neg_mask\n\n        spikes = self.amplitude * spike_signs\n        return spikes + (x - x.detach())\n\n    def get_amplitude(self) -> float:\n        return self.amplitude.item()\n\n\ndef get_stage_params(step: int, total_steps: int = 3000) -> dict:\n    \"\"\"Progressive training stages (POCL) - kept for infrastructure.\"\"\"\n    if step < total_steps * 0.4:\n        return {'stage': 1, 'temp_target': 1.0, 'align_mult': 0.0, 'alpha': 0.9}\n    elif step < total_steps * 0.7:\n        return {'stage': 2, 'temp_target': 1.5, 'align_mult': 0.5, 'alpha': 0.7}\n    else:\n        return {'stage': 3, 'temp_target': 2.0, 'align_mult': 1.0, 'alpha': 0.5}\n\n\n# -----------------------------------------------------------------------------\n# Unit Tests for CTKD Components\n# -----------------------------------------------------------------------------\nprint(\"=\"*60)\nprint(\"v12.1 CTKD Component Tests\")\nprint(\"=\"*60)\n\n# Test 1: GRL Gradient Reversal\nprint(\"\\n[1] GRL Gradient Reversal Test\")\ngrl = GradientReversalLayer()\ngrl.set_lambda(1.0)\nx_test = torch.tensor([2.0], requires_grad=True)\ny_test = grl(x_test)\nloss_test = y_test.sum()\nloss_test.backward()\nexpected_grad = -1.0  # GRL should negate: 1 * -1.0 = -1.0\nactual_grad = x_test.grad.item()\ngrl_pass = abs(actual_grad - expected_grad) < 1e-6\nprint(f\"  Input grad without GRL would be: +1.0\")\nprint(f\"  Input grad with GRL (\u03bb=1.0): {actual_grad:.4f}\")\nprint(f\"  Expected: {expected_grad:.4f}\")\nprint(f\"  {'PASS' if grl_pass else 'FAIL'}\")\ndel x_test, y_test, loss_test\n\n# Test 2: Lambda Schedule\nprint(\"\\n[2] Lambda Schedule Test\")\ntotal = 3000\nwarmup = 0.2\n# During warmup\nlambda_0 = get_lambda(0, total, warmup_ratio=warmup)\nlambda_500 = get_lambda(500, total, warmup_ratio=warmup)\n# After warmup\nlambda_1500 = get_lambda(1500, total, warmup_ratio=warmup)\nlambda_2999 = get_lambda(2999, total, warmup_ratio=warmup)\n\nwarmup_pass = lambda_0 == 0.0 and lambda_500 == 0.0\nincrease_pass = 0 < lambda_1500 < lambda_2999 <= 1.0\nlambda_pass = warmup_pass and increase_pass\nprint(f\"  \u03bb(0) = {lambda_0:.4f} (should be 0.0)\")\nprint(f\"  \u03bb(500) = {lambda_500:.4f} (should be 0.0, still in warmup)\")\nprint(f\"  \u03bb(1500) = {lambda_1500:.4f} (should be > 0)\")\nprint(f\"  \u03bb(2999) = {lambda_2999:.4f} (should be \u2248 1.0)\")\nprint(f\"  {'PASS' if lambda_pass else 'FAIL'}\")\n\n# Test 3: Temperature Bounds\nprint(\"\\n[3] Temperature Bounds Test\")\ntemp_module = CTKDTemperature(tau_min=1.0, tau_max=5.0, init=2.0).to(DEVICE)\ninit_temp = temp_module.get_temperature()\n\n# Force extreme raw values\nwith torch.no_grad():\n    temp_module.raw_temp.fill_(-100)\n    tau_low = temp_module.get_temperature()\n    \n    temp_module.raw_temp.fill_(100)\n    tau_high = temp_module.get_temperature()\n    \n    # Reset to init\n    init_normalized = (2.0 - 1.0) / 4.0\n    init_raw = math.log(init_normalized / (1 - init_normalized))\n    temp_module.raw_temp.fill_(init_raw)\n\nbounds_pass = (1.0 <= tau_low <= 1.01) and (4.99 <= tau_high <= 5.0) and (1.9 <= init_temp <= 2.1)\nprint(f\"  Initial temp: {init_temp:.4f} (should be \u2248 2.0)\")\nprint(f\"  Min bound test: {tau_low:.4f} (should be \u2248 1.0)\")\nprint(f\"  Max bound test: {tau_high:.4f} (should be \u2248 5.0)\")\nprint(f\"  {'PASS' if bounds_pass else 'FAIL'}\")\n\n# Test 4: End-to-End Gradient Flow\nprint(\"\\n[4] End-to-End Gradient Flow Test\")\ntemp_module_test = CTKDTemperature(tau_min=1.0, tau_max=5.0, init=2.0).to(DEVICE)\nlambda_test = 0.5\n\n# Simulate forward pass\nT = temp_module_test(lambda_test)\nfake_kl_loss = T * 2.0  # Gradient \u2202L/\u2202T = 2.0\n\n# Without GRL: optimizer would DECREASE T to minimize loss\n# With GRL: optimizer should INCREASE T (because grad is reversed)\nfake_kl_loss.backward()\n\nraw_grad = temp_module_test.raw_temp.grad.item()\n# The gradient through sigmoid and GRL should be negative (reversed)\n# Original: \u2202L/\u2202raw > 0 would decrease raw\n# With GRL: \u2202L/\u2202raw < 0 (negated), so optimizer increases raw\ngrad_flow_pass = raw_grad < 0  # Should be negative due to GRL\nprint(f\"  Loss = T * 2.0, so \u2202L/\u2202T = 2.0 (positive)\")\nprint(f\"  Without GRL: raw_grad would be positive (decrease T)\")\nprint(f\"  With GRL (\u03bb=0.5): raw_grad = {raw_grad:.4f} (should be negative)\")\nprint(f\"  {'PASS' if grad_flow_pass else 'FAIL'}\")\ndel temp_module_test\n\n# Summary\nprint(\"\\n\" + \"=\"*60)\nall_pass = grl_pass and lambda_pass and bounds_pass and grad_flow_pass\nprint(f\"CTKD Component Tests: {'ALL PASS' if all_pass else 'SOME FAILED'}\")\nif not all_pass:\n    print(\"WARNING: Fix failing tests before running training!\")\nprint(\"=\"*60)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# cell 7: hardware and spike stats collectors (same as v9)\n",
    "# =============================================================================\n",
    "class HardwareStatsCollector:\n",
    "    \"\"\"collect gpu memory, timing, and throughput metrics.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.gpu_memory_history = []\n",
    "        self.step_times = []\n",
    "        self.tokens_processed = 0\n",
    "        self.start_time = None\n",
    "\n",
    "    def start(self):\n",
    "        self.start_time = time.time()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "    def record_step(self, batch_size: int, seq_len: int):\n",
    "        if torch.cuda.is_available():\n",
    "            self.gpu_memory_history.append(torch.cuda.memory_allocated() / 1e9)\n",
    "        self.tokens_processed += batch_size * seq_len\n",
    "        self.step_times.append(time.time())\n",
    "\n",
    "    def get_throughput(self) -> float:\n",
    "        if len(self.step_times) < 2:\n",
    "            return 0.0\n",
    "        elapsed = self.step_times[-1] - self.step_times[0]\n",
    "        return self.tokens_processed / elapsed if elapsed > 0 else 0.0\n",
    "\n",
    "    def get_summary(self) -> Dict[str, Any]:\n",
    "        elapsed = time.time() - self.start_time if self.start_time else 0\n",
    "        return {\n",
    "            'peak_gpu_memory_gb': max(self.gpu_memory_history) if self.gpu_memory_history else 0,\n",
    "            'avg_gpu_memory_gb': float(np.mean(self.gpu_memory_history)) if self.gpu_memory_history else 0,\n",
    "            'total_training_time_s': elapsed,\n",
    "            'total_training_time_min': elapsed / 60,\n",
    "            'tokens_processed': self.tokens_processed,\n",
    "            'throughput_tokens_per_sec': self.get_throughput(),\n",
    "        }\n",
    "\n",
    "\n",
    "class SpikeStatsCollector:\n",
    "    \"\"\"collect per-layer spike density and amplitude evolution.\"\"\"\n",
    "\n",
    "    def __init__(self, n_layers: int):\n",
    "        self.n_layers = n_layers\n",
    "        self.density_history = {i: {'k': [], 'v': []} for i in range(n_layers)}\n",
    "        self.amplitude_history = {i: {'k': [], 'v': []} for i in range(n_layers)}\n",
    "        self.step_densities = []\n",
    "\n",
    "    def record(self, student, step: int):\n",
    "        stats = student.get_spike_stats()\n",
    "        all_densities = []\n",
    "        for i in range(self.n_layers):\n",
    "            layer_key = f'layer_{i}'\n",
    "            if layer_key in stats:\n",
    "                k_density = stats[layer_key].get('k', 0)\n",
    "                v_density = stats[layer_key].get('v', 0)\n",
    "                k_amp = stats[layer_key].get('k_amp', 1.0)\n",
    "                v_amp = stats[layer_key].get('v_amp', 1.0)\n",
    "\n",
    "                self.density_history[i]['k'].append(k_density)\n",
    "                self.density_history[i]['v'].append(v_density)\n",
    "                self.amplitude_history[i]['k'].append(k_amp)\n",
    "                self.amplitude_history[i]['v'].append(v_amp)\n",
    "                all_densities.extend([k_density, v_density])\n",
    "\n",
    "        if all_densities:\n",
    "            self.step_densities.append({'step': step, 'density': float(np.mean(all_densities))})\n",
    "\n",
    "    def get_summary(self) -> Dict[str, Any]:\n",
    "        per_layer = {}\n",
    "        all_k, all_v = [], []\n",
    "        all_k_amp, all_v_amp = [], []\n",
    "\n",
    "        for i in range(self.n_layers):\n",
    "            k_vals = self.density_history[i]['k']\n",
    "            v_vals = self.density_history[i]['v']\n",
    "            k_amps = self.amplitude_history[i]['k']\n",
    "            v_amps = self.amplitude_history[i]['v']\n",
    "\n",
    "            per_layer[f'layer_{i}'] = {\n",
    "                'k_mean': float(np.mean(k_vals)) if k_vals else 0,\n",
    "                'k_std': float(np.std(k_vals)) if k_vals else 0,\n",
    "                'k_final': float(k_vals[-1]) if k_vals else 0,\n",
    "                'v_mean': float(np.mean(v_vals)) if v_vals else 0,\n",
    "                'v_std': float(np.std(v_vals)) if v_vals else 0,\n",
    "                'v_final': float(v_vals[-1]) if v_vals else 0,\n",
    "                'k_amp_final': float(k_amps[-1]) if k_amps else 1.0,\n",
    "                'v_amp_final': float(v_amps[-1]) if v_amps else 1.0,\n",
    "            }\n",
    "            all_k.extend(k_vals)\n",
    "            all_v.extend(v_vals)\n",
    "            if k_amps: all_k_amp.append(k_amps[-1])\n",
    "            if v_amps: all_v_amp.append(v_amps[-1])\n",
    "\n",
    "        return {\n",
    "            'per_layer': per_layer,\n",
    "            'overall_k_density': float(np.mean(all_k)) if all_k else 0,\n",
    "            'overall_v_density': float(np.mean(all_v)) if all_v else 0,\n",
    "            'overall_density': float(np.mean(all_k + all_v)) if (all_k or all_v) else 0,\n",
    "            'amplitudes': {'k': all_k_amp, 'v': all_v_amp},\n",
    "            'density_history': self.step_densities,\n",
    "        }\n",
    "\n",
    "print(\"collectors defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# cell 8: spiking goose model (v11 - channel-wise spikes + gradient checkpointing)\n# =============================================================================\nclass SpikingGooseRecurrentLayer(nn.Module):\n    \"\"\"\n    RWKV-style recurrence with trainable ternary spiking.\n    \n    v11: Supports channel-wise ternary spikes (when use_channel_wise=True)\n    \"\"\"\n\n    def __init__(self, d_model, layer_idx=0, n_layers=4, spike_alpha=1.0, \n                 use_channel_wise: bool = False):\n        super().__init__()\n        self.d_model = d_model\n        self.layer_idx = layer_idx\n        self.use_channel_wise = use_channel_wise\n        self.ln = nn.LayerNorm(d_model)\n\n        ratio = layer_idx / max(n_layers - 1, 1)\n        self.time_mix_k = nn.Parameter(torch.ones(d_model) * (1 - ratio))\n        self.time_mix_v = nn.Parameter(torch.ones(d_model) * (1 - ratio))\n        self.time_mix_r = nn.Parameter(torch.ones(d_model) * (1 - ratio))\n        self.decay_weight = nn.Parameter(torch.zeros(d_model) - 0.5)\n\n        self.key_proj = nn.Linear(d_model, d_model, bias=False)\n        self.value_proj = nn.Linear(d_model, d_model, bias=False)\n        self.receptance_proj = nn.Linear(d_model, d_model, bias=False)\n        self.output_proj = nn.Linear(d_model, d_model, bias=False)\n\n        # v11: Use channel-wise spikes if enabled\n        if use_channel_wise:\n            self.k_spike = ChannelWiseTernarySpike(d_model, alpha_init=spike_alpha)\n            self.v_spike = ChannelWiseTernarySpike(d_model, alpha_init=spike_alpha)\n        else:\n            self.k_spike = TrainableTernarySpike(alpha=spike_alpha)\n            self.v_spike = TrainableTernarySpike(alpha=spike_alpha)\n\n        self.register_buffer('running_k_density', torch.tensor(0.0))\n        self.register_buffer('running_v_density', torch.tensor(0.0))\n        self._init_weights()\n\n    def _init_weights(self):\n        std = 0.1 / math.sqrt(self.d_model)\n        for m in [self.key_proj, self.value_proj, self.receptance_proj, self.output_proj]:\n            nn.init.normal_(m.weight, std=std)\n\n    def forward(self, x):\n        B, T, D = x.shape\n        x_norm = self.ln(x)\n        prev_x = F.pad(x_norm[:, :-1, :], (0, 0, 1, 0))\n\n        xk = x_norm * self.time_mix_k + prev_x * (1 - self.time_mix_k)\n        xv = x_norm * self.time_mix_v + prev_x * (1 - self.time_mix_v)\n        xr = x_norm * self.time_mix_r + prev_x * (1 - self.time_mix_r)\n\n        k_pre = self.key_proj(xk)\n        v_pre = self.value_proj(xv)\n\n        k = self.k_spike(k_pre)\n        v = self.v_spike(v_pre)\n        r = torch.sigmoid(self.receptance_proj(xr))\n\n        kv = k * v\n        decay = torch.sigmoid(self.decay_weight)\n        t_idx = torch.arange(T, device=x.device, dtype=x.dtype)\n        decay_powers = decay.unsqueeze(0) ** t_idx.unsqueeze(1)\n\n        kv_weighted = kv / (decay_powers.unsqueeze(0) + 1e-8)\n        S = torch.cumsum(kv_weighted, dim=1) * decay_powers.unsqueeze(0)\n\n        if self.training:\n            with torch.no_grad():\n                self.running_k_density = 0.99 * self.running_k_density + 0.01 * (k != 0).float().mean()\n                self.running_v_density = 0.99 * self.running_v_density + 0.01 * (v != 0).float().mean()\n\n        return x + r * self.output_proj(S)\n\n    def get_spike_density(self):\n        return {\n            'k': self.running_k_density.item(),\n            'v': self.running_v_density.item(),\n            'k_amp': self.k_spike.get_amplitude(),\n            'v_amp': self.v_spike.get_amplitude(),\n        }\n    \n    def get_channel_wise_stats(self) -> dict:\n        \"\"\"Get channel-wise spike statistics (only available if use_channel_wise=True).\"\"\"\n        if self.use_channel_wise:\n            return {\n                'k': self.k_spike.get_stats(),\n                'v': self.v_spike.get_stats(),\n            }\n        return None\n\n\nclass GooseFFN(nn.Module):\n    def __init__(self, d_model, expand=4):\n        super().__init__()\n        self.ln = nn.LayerNorm(d_model)\n        self.w1 = nn.Linear(d_model, d_model * expand, bias=False)\n        self.w2 = nn.Linear(d_model * expand, d_model, bias=False)\n\n    def forward(self, x):\n        return x + self.w2(F.silu(self.w1(self.ln(x))))\n\n\nclass StudentSpikingGoose(nn.Module):\n    \"\"\"\n    Spiking student model with trainable ternary activations.\n    \n    v11: Supports channel-wise ternary spikes + gradient checkpointing.\n    \"\"\"\n\n    def __init__(self, cfg, use_checkpointing=True):\n        super().__init__()\n        self.cfg = cfg\n        self.use_checkpointing = use_checkpointing and USE_GRADIENT_CHECKPOINTING\n        \n        # v11: Check for channel-wise spikes flag\n        use_channel_wise = getattr(cfg, 'use_channel_wise_spikes', False)\n        \n        self.embed = nn.Embedding(cfg.vocab_size, cfg.d_model)\n        self.pos_embed = nn.Embedding(cfg.max_seq_len, cfg.d_model)\n\n        self.layers = nn.ModuleList([\n            nn.ModuleDict({\n                'rec': SpikingGooseRecurrentLayer(\n                    cfg.d_model, i, cfg.n_layers, cfg.spike_alpha,\n                    use_channel_wise=use_channel_wise\n                ),\n                'ffn': GooseFFN(cfg.d_model),\n            })\n            for i in range(cfg.n_layers)\n        ])\n\n        self.ln_out = nn.LayerNorm(cfg.d_model)\n        self.head = nn.Linear(cfg.d_model, cfg.vocab_size, bias=False)\n        self.head.weight = self.embed.weight\n\n        nn.init.normal_(self.embed.weight, std=0.02)\n        nn.init.normal_(self.pos_embed.weight, std=0.02)\n\n    def _layer_forward(self, layer, x):\n        \"\"\"helper for gradient checkpointing - processes one layer.\"\"\"\n        x = layer['rec'](x)\n        x = layer['ffn'](x)\n        return x\n\n    def forward(self, input_ids, return_hiddens=False):\n        \"\"\"forward pass with optional hidden state return for alignment.\"\"\"\n        B, T = input_ids.shape\n        pos = torch.arange(T, device=input_ids.device).unsqueeze(0)\n        x = self.embed(input_ids) + self.pos_embed(pos)\n\n        hiddens = [x] if return_hiddens else None\n\n        for layer in self.layers:\n            if self.use_checkpointing and self.training:\n                # v10: gradient checkpointing with use_reentrant=False (recommended)\n                x = checkpoint(self._layer_forward, layer, x, use_reentrant=False)\n            else:\n                x = self._layer_forward(layer, x)\n            \n            if return_hiddens:\n                hiddens.append(x)\n\n        logits = self.head(self.ln_out(x))\n\n        if return_hiddens:\n            return logits, hiddens\n        return logits\n\n    def get_spike_stats(self):\n        return {f'layer_{i}': layer['rec'].get_spike_density() for i, layer in enumerate(self.layers)}\n\n    def get_avg_spike_density(self):\n        densities = []\n        for layer in self.layers:\n            d = layer['rec'].get_spike_density()\n            densities.extend([d['k'], d['v']])\n        return float(np.mean(densities)) if densities else 0.0\n\n    def get_amplitudes(self):\n        return {f'layer_{i}': {'k': layer['rec'].k_spike.get_amplitude(), 'v': layer['rec'].v_spike.get_amplitude()}\n                for i, layer in enumerate(self.layers)}\n    \n    def get_channel_amplitude_variance(self) -> float:\n        \"\"\"Get total variance of channel-wise amplitudes (for regularization).\"\"\"\n        total_var = 0.0\n        for layer in self.layers:\n            rec = layer['rec']\n            if hasattr(rec.k_spike, 'amplitude') and rec.k_spike.amplitude.numel() > 1:\n                total_var += rec.k_spike.amplitude.var().item()\n                total_var += rec.v_spike.amplitude.var().item()\n        return total_var\n\nprint(\"student model defined (v11: channel-wise spikes + gradient checkpointing)\")\nprint(f\"  gradient checkpointing: {USE_GRADIENT_CHECKPOINTING}\")\nprint(f\"  channel-wise spikes: {config.use_channel_wise_spikes}\")\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n# cell 9: hidden-state alignment (v11 - 320d \u00d7 5L, but DISABLED)\n# =============================================================================\nclass HiddenStateProjector(nn.Module):\n    \"\"\"\n    Project student hidden states to teacher dimension for alignment.\n\n    student: (B, T, 512) -> (B, T, 768)  # v11: 512 dim (same as v11)\n\n    Maps 8 student layers to selected teacher layers.\n\n    NOTE: This is kept for infrastructure but DISABLED in v11 (weight=0.0).\n    \"\"\"\n\n    def __init__(self, student_dim: int, teacher_dim: int, n_student_layers: int):\n        super().__init__()\n        self.projectors = nn.ModuleList([\n            nn.Linear(student_dim, teacher_dim, bias=False)\n            for _ in range(n_student_layers)\n        ])\n        for proj in self.projectors:\n            nn.init.normal_(proj.weight, std=0.02)\n\n    def forward(self, student_hidden: torch.Tensor, layer_idx: int) -> torch.Tensor:\n        return self.projectors[layer_idx](student_hidden)\n\n\ndef compute_hidden_alignment_loss(\n    teacher_hiddens: List[torch.Tensor],\n    student_hiddens: List[torch.Tensor],\n    projector: HiddenStateProjector,\n    teacher_layers: int = 12,\n    student_layers: int = 8\n) -> torch.Tensor:\n    \"\"\"\n    Compute MSE loss between projected student and teacher hidden states.\n\n    v11: Uses 8 student layers, maps to every 1-2 teacher layers.\n    NOTE: Disabled in v11 (hidden_align_weight=0.0), kept for future experiments.\n    \"\"\"\n    # v11: Map 8 student layers to 8 teacher layers\n    # teacher_indices[i] = which teacher layer student layer i maps to\n    teacher_indices = [1, 2, 4, 5, 7, 8, 10, 11]  # v11 mapping (8 student layers)\n\n    total_loss = 0.0\n    for s_idx, t_idx in enumerate(teacher_indices):\n        if s_idx >= len(student_hiddens) - 1:  # student_hiddens includes embedding\n            break\n        if t_idx >= len(teacher_hiddens):\n            break\n\n        s_hidden = student_hiddens[s_idx + 1]  # +1 because [0] is embedding\n        t_hidden = teacher_hiddens[t_idx]\n\n        s_proj = projector(s_hidden, s_idx)\n        total_loss = total_loss + F.mse_loss(s_proj, t_hidden)\n\n    return total_loss / len(teacher_indices)\n\n\n# Create projector (even if disabled, keeps infrastructure)\nprojector = HiddenStateProjector(\n    student_dim=config.d_model,\n    teacher_dim=config.teacher_d_model,\n    n_student_layers=config.n_layers\n).to(DEVICE)\n\nprojector_params = sum(p.numel() for p in projector.parameters())\nprint(f\"hidden-state projector: {projector_params:,} params\")\nprint(f\"  student dim: {config.d_model} (v11: same as v11)\")\nprint(f\"  teacher dim: {config.teacher_d_model}\")\nprint(f\"  student layers: {config.n_layers}\")\nprint(f\"  hidden_align_weight: {config.hidden_align_weight}\")\nif config.hidden_align_weight == 0.0:\n    print(f\"  STATUS: DISABLED (keeping infrastructure for future experiments)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# cell 10: cosine lr with warmup (same as v9)\n",
    "# =============================================================================\n",
    "def get_cosine_schedule_with_warmup(\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    warmup_steps: int,\n",
    "    total_steps: int,\n",
    ") -> torch.optim.lr_scheduler.LambdaLR:\n",
    "    \"\"\"\n",
    "    linear warmup then cosine decay to 0.\n",
    "    \"\"\"\n",
    "    def lr_lambda(step: int) -> float:\n",
    "        if step < warmup_steps:\n",
    "            return step / max(warmup_steps, 1)\n",
    "        else:\n",
    "            progress = (step - warmup_steps) / max(total_steps - warmup_steps, 1)\n",
    "            return 0.5 * (1.0 + math.cos(math.pi * progress))\n",
    "\n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "\n",
    "print(f\"cosine lr: {config.warmup_steps} warmup, {config.distill_steps} total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n# cell 11: load gpt-2 teacher (same as v9)\n# =============================================================================\nfrom transformers import GPT2LMHeadModel, GPT2Tokenizer\n\nprint(\"loading gpt-2 teacher...\")\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\ntokenizer.pad_token = tokenizer.eos_token\n\nteacher = GPT2LMHeadModel.from_pretrained('gpt2').to(DEVICE)\nteacher.config.use_cache = False  # Disable KV caching (not needed for distillation)\n\n# Compile teacher for faster inference (PyTorch 2.0+)\ntry:\n    teacher = torch.compile(teacher, mode='reduce-overhead')\n    print('teacher compiled with torch.compile')\nexcept Exception as e:\n    print(f'torch.compile not available: {e}')\nteacher.eval()\nfor p in teacher.parameters():\n    p.requires_grad = False\n\nteacher_params = sum(p.numel() for p in teacher.parameters())\nprint(f\"teacher: gpt-2 ({teacher_params:,} params)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# cell 13: data loading (v11 - efficient DataLoader)\n",
    "# =============================================================================\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"loading wikitext-2...\")\n",
    "dataset = load_dataset('wikitext', 'wikitext-2-raw-v1')\n",
    "\n",
    "def pre_tokenize(texts, max_len):\n",
    "    all_tokens = []\n",
    "    for text in tqdm(texts, desc=\"tokenizing\", leave=False):\n",
    "        if text.strip():\n",
    "            all_tokens.extend(tokenizer.encode(text, max_length=max_len*2, truncation=True))\n",
    "    chunks = [all_tokens[i:i+max_len] for i in range(0, len(all_tokens)-max_len+1, max_len//2) if len(all_tokens[i:i+max_len]) == max_len]\n",
    "    print(f\"created {len(chunks)} sequences\")\n",
    "    return torch.tensor(chunks, dtype=torch.long)\n",
    "\n",
    "train_tokens = pre_tokenize(dataset['train']['text'], config.max_seq_len)\n",
    "val_tokens = pre_tokenize(dataset['validation']['text'], config.max_seq_len)\n",
    "\n",
    "# v11: efficient DataLoader with workers and prefetch\n",
    "# Note: num_workers=0 for Kaggle/Colab compatibility, but prefetch still helps\n",
    "dataloader_kwargs = {\n",
    "    'batch_size': config.batch_size,\n",
    "    'pin_memory': True,\n",
    "    'num_workers': 0 if IS_KAGGLE or IS_COLAB else 2,  # workers disabled on cloud platforms\n",
    "    'prefetch_factor': None if IS_KAGGLE or IS_COLAB else 2,\n",
    "    'persistent_workers': False if IS_KAGGLE or IS_COLAB else True,\n",
    "}\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(train_tokens), shuffle=True, **dataloader_kwargs)\n",
    "val_loader = DataLoader(TensorDataset(val_tokens), shuffle=False, **dataloader_kwargs)\n",
    "\n",
    "print(f\"train: {len(train_loader)} batches, val: {len(val_loader)} batches\")\n",
    "print(f\"DataLoader: num_workers={dataloader_kwargs['num_workers']}, pin_memory={dataloader_kwargs['pin_memory']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# cell 14: create student model and projector (v11 - with compile)\n",
    "# =============================================================================\n",
    "print(\"creating student model (v11 - v11 baseline + POCL)...\")\n",
    "\n",
    "student = StudentSpikingGoose(config, use_checkpointing=USE_GRADIENT_CHECKPOINTING).to(DEVICE)\n",
    "student_params = sum(p.numel() for p in student.parameters())\n",
    "\n",
    "# v11: create projector (even if not used, for infrastructure preservation)\n",
    "projector = HiddenStateProjector(\n",
    "    student_dim=config.d_model,\n",
    "    teacher_dim=config.teacher_d_model,\n",
    "    n_student_layers=config.n_layers\n",
    ").to(DEVICE)\n",
    "projector_params = sum(p.numel() for p in projector.parameters())\n",
    "\n",
    "compression_ratio = teacher_params / student_params\n",
    "\n",
    "print(f\"student: asnn-goose v11 ({student_params:,} params)\")\n",
    "print(f\"projector: ({projector_params:,} params)\")\n",
    "print(f\"compression ratio: {compression_ratio:.1f}x\")\n",
    "print(f\"\")\n",
    "print(f\"v11 architecture:\")\n",
    "print(f\"  d_model: {config.d_model}\")\n",
    "print(f\"  n_layers: {config.n_layers}\")\n",
    "print(f\"  params: ~{student_params // 1_000_000}M\")\n",
    "print(f\"\")\n",
    "\n",
    "# v11: compile model if available and enabled\n",
    "compile_success = False\n",
    "if USE_TORCH_COMPILE and TORCH_COMPILE_AVAILABLE:\n",
    "    try:\n",
    "        print(\"compiling student model with torch.compile...\")\n",
    "        # Use the compile() method as recommended by PyTorch docs\n",
    "        student = torch.compile(student, mode='reduce-overhead')\n",
    "        compile_success = True\n",
    "        print(\"compilation successful!\")\n",
    "    except Exception as e:\n",
    "        print(f\"torch.compile failed: {e}\")\n",
    "        print(\"continuing without compilation\")\n",
    "else:\n",
    "    print(f\"torch.compile skipped (USE_TORCH_COMPILE={USE_TORCH_COMPILE}, available={TORCH_COMPILE_AVAILABLE})\")\n",
    "\n",
    "print(f\"\")\n",
    "print(f\"speedups active:\")\n",
    "print(f\"  gradient checkpointing: {USE_GRADIENT_CHECKPOINTING}\")\n",
    "print(f\"  torch.compile: {compile_success}\")\n",
    "print(f\"  accumulation_steps: {config.accumulation_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n# cell 15: evaluation functions (same as v9)\n# =============================================================================\n@torch.no_grad()\ndef evaluate(model, loader, device, is_gpt2=False):\n    model.eval()\n    total_loss, total_tokens = 0, 0\n    with torch.inference_mode():\n      for batch in loader:\n        ids = batch[0].to(device, non_blocking=True)\n        with torch.cuda.amp.autocast():\n            logits = model(ids).logits if is_gpt2 else model(ids)\n        loss = F.cross_entropy(logits[:, :-1].reshape(-1, logits.size(-1)), ids[:, 1:].reshape(-1), reduction='sum')\n        total_loss += loss.item()\n        total_tokens += ids[:, 1:].numel()\n    return total_loss / total_tokens\n\ndef get_ppl(loss):\n    return math.exp(min(loss, 10))\n\nprint(\"evaluation functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# cell 17: distillation training loop (v12.1 - PROPER CTKD with GRL)\n# =============================================================================\ndef distill_v11(teacher, student, projector, train_loader, val_loader, cfg, device,\n               hw_stats, spike_stats):\n    \"\"\"\n    v12.1 distillation with PROPER CTKD (Gradient Reversal Layer).\n    \n    Key differences from v12 (broken):\n    - Uses GRL for adversarial min-max optimization\n    - Temperature tries to MAXIMIZE loss (via reversed gradients)\n    - Student tries to MINIMIZE loss (normal gradients)\n    - No manual temperature regularization needed (GRL handles it)\n    - Lambda scheduler controls adversarial strength\n    \n    References:\n    - CTKD: https://arxiv.org/abs/2211.16231\n    - GRL: https://arxiv.org/abs/1409.7495\n    \"\"\"\n    training_logs = {\n        'loss_history': [],\n        'kl_loss_history': [],\n        'align_loss_history': [],\n        'ppl_history': [],\n        'lr_history': [],\n        'temp_history': [],\n        'lambda_history': [],  # v12.1: track lambda evolution\n        'stage_history': [],\n    }\n\n    # =========================================================================\n    # v12.1: PROPER CTKD Temperature (with Gradient Reversal Layer)\n    # =========================================================================\n    if cfg.use_ctkd:\n        temp_module = CTKDTemperature(\n            tau_min=cfg.tau_min,\n            tau_max=cfg.tau_max,\n            init=cfg.tau_init\n        ).to(device)\n        print(f\"v12.1: PROPER CTKD with Gradient Reversal Layer\")\n        print(f\"       Temperature bounds: [{cfg.tau_min}, {cfg.tau_max}]\")\n        print(f\"       Initial temp: {cfg.tau_init}\")\n        print(f\"       Lambda warmup: {cfg.lambda_warmup_ratio*100:.0f}% ({int(cfg.distill_steps * cfg.lambda_warmup_ratio)} steps)\")\n        print(f\"       Lambda max: {cfg.lambda_max}\")\n        print(f\"\")\n        print(f\"       Adversarial game:\")\n        print(f\"       - Student gradients: NORMAL (minimize KL loss)\")\n        print(f\"       - Temperature gradients: REVERSED (maximize KL loss)\")\n    elif cfg.use_learnable_temperature:\n        # Legacy (broken) - kept for comparison\n        temp_module = LearnableTemperature(init=cfg.temperature).to(device)\n        print(f\"WARNING: Using legacy LearnableTemperature (caused runaway in v12!)\")\n    else:\n        temp_module = None\n        print(f\"using fixed temperature: {cfg.temperature}\")\n\n    # =========================================================================\n    # Setup optimizer\n    # =========================================================================\n    param_groups = [\n        {'params': list(student.parameters()), 'lr': cfg.distill_lr}\n    ]\n    \n    # Add projector params if alignment is enabled\n    if cfg.hidden_align_weight > 0:\n        param_groups.append({'params': list(projector.parameters()), 'lr': cfg.distill_lr})\n        print(f\"hidden alignment enabled: weight={cfg.hidden_align_weight}\")\n    \n    # v12.1: Add CTKD temperature params (same LR as model - GRL handles adversarial)\n    if temp_module is not None:\n        param_groups.append({'params': list(temp_module.parameters()), 'lr': cfg.distill_lr})\n        print(f\"temperature added to optimizer (same LR as model - GRL controls adversarial)\")\n    \n    # Collect all params for gradient clipping\n    all_params = []\n    for group in param_groups:\n        all_params.extend(group['params'])\n    \n    # Fused AdamW for speedup\n    try:\n        optimizer = torch.optim.AdamW(param_groups, weight_decay=0.01, fused=True)\n        print(\"using fused AdamW\")\n    except TypeError:\n        optimizer = torch.optim.AdamW(param_groups, weight_decay=0.01)\n        print(\"fused AdamW not available\")\n    \n    scheduler = get_cosine_schedule_with_warmup(optimizer, cfg.warmup_steps, cfg.distill_steps)\n    scaler = torch.cuda.amp.GradScaler()\n\n    hw_stats.start()\n    step = 0\n    accum_step = 0\n    best_val = float('inf')\n    current_stage = 1\n\n    accumulation_steps = cfg.accumulation_steps\n    effective_batch = cfg.batch_size * accumulation_steps\n    print(f\"gradient accumulation: {accumulation_steps} steps (effective batch = {effective_batch})\")\n\n    pbar = tqdm(total=cfg.distill_steps, desc='distilling (v12.1 - CTKD+GRL)')\n\n    optimizer.zero_grad(set_to_none=True)\n\n    while step < cfg.distill_steps:\n        for batch in train_loader:\n            if step >= cfg.distill_steps:\n                break\n\n            ids = batch[0].to(device, non_blocking=True)\n\n            # =========================================================================\n            # v12.1: Compute lambda (adversarial strength) from scheduler\n            # =========================================================================\n            if cfg.use_ctkd:\n                current_lambda = get_lambda(\n                    step, cfg.distill_steps,\n                    lambda_max=cfg.lambda_max,\n                    warmup_ratio=cfg.lambda_warmup_ratio\n                )\n            else:\n                current_lambda = 0.0\n\n            # Get stage params (for legacy infrastructure)\n            if cfg.use_progressive_stages:\n                stage_params = get_stage_params(step, cfg.distill_steps)\n                stage = stage_params['stage']\n                align_mult = stage_params['align_mult']\n                kd_alpha = stage_params.get('alpha', 0.9)\n            else:\n                stage = 1\n                align_mult = 1.0\n                kd_alpha = 1.0\n            \n            if stage != current_stage:\n                print(f\"\\n  [stage transition] step {step}: stage {current_stage} -> stage {stage}\")\n                current_stage = stage\n\n            with torch.cuda.amp.autocast():\n                # =========================================================================\n                # Teacher forward\n                # =========================================================================\n                with torch.no_grad():\n                    if cfg.hidden_align_weight > 0:\n                        t_out = teacher(ids, output_hidden_states=True)\n                        t_logits = t_out.logits\n                        t_hiddens = t_out.hidden_states\n                    else:\n                        t_logits = teacher(ids).logits\n\n                # =========================================================================\n                # Student forward\n                # =========================================================================\n                student.train()\n                if cfg.hidden_align_weight > 0:\n                    s_logits, s_hiddens = student(ids, return_hiddens=True)\n                else:\n                    s_logits = student(ids)\n\n                # =========================================================================\n                # v12.1: Get temperature with GRL applied\n                # =========================================================================\n                if cfg.use_ctkd and temp_module is not None:\n                    # Pass current lambda to temperature module\n                    # GRL inside temp_module will reverse gradients by -lambda\n                    T = temp_module(current_lambda)\n                elif temp_module is not None:\n                    # Legacy (broken) approach\n                    T = temp_module()\n                else:\n                    T = cfg.temperature\n\n                # =========================================================================\n                # KL divergence loss with temperature scaling\n                # T\u00b2 scaling is CRITICAL for proper gradient magnitude\n                # =========================================================================\n                s_log = F.log_softmax(s_logits / T, dim=-1)\n                t_prob = F.softmax(t_logits / T, dim=-1)\n                kl_loss = F.kl_div(\n                    s_log.view(-1, s_logits.size(-1)),\n                    t_prob.view(-1, t_logits.size(-1)),\n                    reduction='batchmean'\n                ) * (T ** 2)\n\n                # =========================================================================\n                # Hidden-state alignment (usually disabled)\n                # =========================================================================\n                effective_align_weight = cfg.hidden_align_weight * align_mult\n                \n                if effective_align_weight > 0:\n                    align_loss = compute_hidden_alignment_loss(\n                        t_hiddens, s_hiddens, projector,\n                        teacher_layers=cfg.teacher_n_layers,\n                        student_layers=cfg.n_layers\n                    )\n                else:\n                    align_loss = torch.tensor(0.0, device=device)\n\n                # =========================================================================\n                # v12.1: NO manual temperature regularization needed!\n                # GRL handles the adversarial optimization automatically.\n                # The temperature will find optimal difficulty through min-max game.\n                # =========================================================================\n                \n                # Total loss\n                kd_weight = kd_alpha if cfg.use_progressive_stages else 1.0\n                loss = kd_weight * kl_loss + effective_align_weight * align_loss\n\n                # Scale for gradient accumulation\n                loss = loss / accumulation_steps\n\n            scaler.scale(loss).backward()\n            accum_step += 1\n\n            if accum_step % accumulation_steps == 0:\n                scaler.unscale_(optimizer)\n                gn = torch.nn.utils.clip_grad_norm_(all_params, cfg.max_grad_norm)\n\n                if torch.isfinite(gn):\n                    scaler.step(optimizer)\n                scaler.update()\n                scheduler.step()\n                optimizer.zero_grad(set_to_none=True)\n\n                hw_stats.record_step(ids.size(0) * accumulation_steps, ids.size(1))\n                spike_stats.record(student, step)\n\n                current_lr = optimizer.param_groups[0]['lr']\n                current_temp = temp_module.get_temperature() if temp_module is not None else cfg.temperature\n\n                # Track logs\n                training_logs['loss_history'].append({'step': step, 'loss': loss.item() * accumulation_steps})\n                training_logs['kl_loss_history'].append({'step': step, 'loss': kl_loss.item()})\n                training_logs['align_loss_history'].append({'step': step, 'loss': align_loss.item() if isinstance(align_loss, torch.Tensor) else align_loss})\n                training_logs['lr_history'].append({'step': step, 'lr': current_lr})\n                training_logs['temp_history'].append({'step': step, 'temperature': current_temp})\n                training_logs['lambda_history'].append({'step': step, 'lambda': current_lambda})  # v12.1\n                training_logs['stage_history'].append({'step': step, 'stage': stage})\n\n                # v12.1: Show lambda in progress bar\n                pbar.set_postfix(\n                    loss=f\"{loss.item() * accumulation_steps:.3f}\",\n                    kl=f\"{kl_loss.item():.3f}\",\n                    T=f\"{current_temp:.2f}\",\n                    lam=f\"{current_lambda:.2f}\",  # v12.1: show lambda\n                    lr=f\"{current_lr:.1e}\"\n                )\n                pbar.update(1)\n                step += 1\n\n                if step % cfg.eval_interval == 0:\n                    val_loss = evaluate(student, val_loader, device)\n                    val_ppl = get_ppl(val_loss)\n                    training_logs['ppl_history'].append({'step': step, 'ppl': val_ppl})\n\n                    amps = student.get_amplitudes()\n                    amp_str = ', '.join([f\"L{i}:{amps[f'layer_{i}']['k']:.2f}\" for i in range(min(4, cfg.n_layers))])\n                    \n                    # v12.1: Show lambda and temperature evolution\n                    lambda_str = f\", \u03bb={current_lambda:.2f}\" if cfg.use_ctkd else \"\"\n                    print(f\"\\n  step {step}: ppl={val_ppl:.1f}, T={current_temp:.2f}{lambda_str}, amps=[{amp_str}...]\")\n\n                    if val_loss < best_val:\n                        best_val = val_loss\n                        save_dict = {\n                            'student': student.state_dict(),\n                            'projector': projector.state_dict(),\n                        }\n                        if temp_module is not None:\n                            save_dict['temp_module'] = temp_module.state_dict()\n                        torch.save(save_dict, f'{OUTPUT_DIR}/checkpoints/v12_1_best.pt')\n\n    pbar.close()\n    return training_logs\n\nprint(\"distillation function defined (v12.1 - PROPER CTKD with GRL)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# cell 18: run distillation (v12.1 - PROPER CTKD with GRL)\n# =============================================================================\nprint(\"=\"*60)\nprint(\"starting distillation (v12.1 - PROPER CTKD with GRL)\")\nprint(\"=\"*60)\nprint(f\"  architecture: {config.d_model}d \u00d7 {config.n_layers}L (~22M params)\")\nprint(f\"\")\nprint(f\"v12.1 CTKD Configuration:\")\nprint(f\"  use_ctkd: {config.use_ctkd}\")\nprint(f\"  temperature bounds: [{config.tau_min}, {config.tau_max}]\")\nprint(f\"  initial temperature: {config.tau_init}\")\nprint(f\"  lambda_max: {config.lambda_max}\")\nprint(f\"  lambda warmup: {config.lambda_warmup_ratio*100:.0f}% ({int(config.distill_steps * config.lambda_warmup_ratio)} steps)\")\nprint(f\"\")\nprint(f\"v12.1 KEY DIFFERENCES from v12 (broken):\")\nprint(f\"  - Uses Gradient Reversal Layer for ADVERSARIAL min-max game\")\nprint(f\"  - Temperature tries to MAXIMIZE loss (find hardest difficulty)\")\nprint(f\"  - Student tries to MINIMIZE loss (learn from challenge)\")\nprint(f\"  - NO manual regularization needed (GRL handles it)\")\nprint(f\"\")\nprint(f\"  target: PPL < 500 (improve on v10's 514.5)\")\nprint(\"\")\n\n# Instantiate collectors (required for distill_v11)\nhw_stats = HardwareStatsCollector()\nspike_stats = SpikeStatsCollector(config.n_layers)\n\ndistill_logs = distill_v11(\n    teacher, student, projector,\n    train_loader, val_loader,\n    config, DEVICE,\n    hw_stats, spike_stats\n)\n\nprint(\"\\ndistillation complete (v12.1 - PROPER CTKD with GRL)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# cell 20: lora implementation (same as v9)\n",
    "# =============================================================================\n",
    "class LoRALinear(nn.Module):\n",
    "    \"\"\"lora adapter for linear layers.\"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, rank=8, alpha=16.0):\n",
    "        super().__init__()\n",
    "        self.scaling = alpha / rank\n",
    "        self.lora_A = nn.Parameter(torch.zeros(rank, in_features))\n",
    "        self.lora_B = nn.Parameter(torch.zeros(out_features, rank))\n",
    "        nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return (x @ self.lora_A.T @ self.lora_B.T) * self.scaling\n",
    "\n",
    "\n",
    "def apply_lora(model, rank=8, alpha=16.0, targets=['key_proj', 'value_proj']):\n",
    "    \"\"\"apply lora adapters to specified modules.\"\"\"\n",
    "    lora_modules = {}\n",
    "    for name, module in model.named_modules():\n",
    "        if any(t in name for t in targets) and isinstance(module, nn.Linear):\n",
    "            lora = LoRALinear(module.in_features, module.out_features, rank, alpha).to(next(module.parameters()).device)\n",
    "            lora_modules[name] = lora\n",
    "            orig_forward = module.forward\n",
    "            def make_forward(orig, lora_mod):\n",
    "                def forward(x):\n",
    "                    return orig(x) + lora_mod(x)\n",
    "                return forward\n",
    "            module.forward = make_forward(orig_forward, lora)\n",
    "    print(f\"lora: {len(lora_modules)} modules, rank={rank}\")\n",
    "    return lora_modules\n",
    "\n",
    "print(\"lora defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# cell 21: ttt with lora (same as v9)\n",
    "# =============================================================================\n",
    "print(\"=\"*60)\n",
    "print(\"phase 2: test-time training with lora\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for p in student.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "lora_modules = apply_lora(student, config.lora_rank, config.lora_alpha)\n",
    "lora_params = sum(p.numel() for m in lora_modules.values() for p in m.parameters())\n",
    "\n",
    "pre_ttt_loss = evaluate(student, val_loader, DEVICE)\n",
    "pre_ttt_ppl = get_ppl(pre_ttt_loss)\n",
    "print(f\"\\npre-ttt ppl: {pre_ttt_ppl:.2f}\")\n",
    "\n",
    "lora_opt = torch.optim.AdamW([p for m in lora_modules.values() for p in m.parameters()], lr=config.ttt_lr)\n",
    "ttt_logs = {'loss_history': []}\n",
    "student.train()\n",
    "\n",
    "for step, batch in enumerate(val_loader):\n",
    "    if step >= config.ttt_steps:\n",
    "        break\n",
    "    ids = batch[0].to(DEVICE)\n",
    "    with torch.cuda.amp.autocast():\n",
    "        loss = F.cross_entropy(student(ids)[:, :-1].reshape(-1, config.vocab_size), ids[:, 1:].reshape(-1))\n",
    "    lora_opt.zero_grad()\n",
    "    loss.backward()\n",
    "    lora_opt.step()\n",
    "    ttt_logs['loss_history'].append({'step': step, 'loss': loss.item()})\n",
    "    if step % 20 == 0:\n",
    "        print(f\"  ttt {step}: loss={loss.item():.4f}\")\n",
    "\n",
    "post_ttt_loss = evaluate(student, val_loader, DEVICE)\n",
    "post_ttt_ppl = get_ppl(post_ttt_loss)\n",
    "print(f\"\\npost-ttt ppl: {post_ttt_ppl:.2f}\")\n",
    "print(f\"ttt improvement: {pre_ttt_ppl - post_ttt_ppl:.1f} ppl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# cell 22: final evaluation (v12.1 - Proper CTKD with GRL)\n# =============================================================================\nprint(\"=\"*60)\nprint(\"final evaluation (v12.1 - CTKD with Gradient Reversal Layer)\")\nprint(\"=\"*60)\n\nteacher_loss = evaluate(teacher, val_loader, DEVICE, is_gpt2=True)\nteacher_ppl = get_ppl(teacher_loss)\nstudent_loss = evaluate(student, val_loader, DEVICE)\nstudent_ppl = get_ppl(student_loss)\n\n# v12.1: Get final temperature and lambda from CTKD\nfinal_temp = distill_logs['temp_history'][-1]['temperature'] if distill_logs['temp_history'] else config.tau_init\nfinal_lambda = distill_logs['temp_history'][-1].get('lambda', config.lambda_max) if distill_logs['temp_history'] else config.lambda_max\n\n# VRAM logging\nvram_peak_gb = torch.cuda.max_memory_allocated() / 1e9 if torch.cuda.is_available() else 0\n\nprint(f\"\")\nprint(f\"{'model':<30} {'ppl':>10} {'params':>15}\")\nprint(\"-\" * 55)\nprint(f\"{'gpt-2 (teacher)':<30} {teacher_ppl:>10.2f} {teacher_params:>15,}\")\nprint(f\"{'asnn-goose v12.1 (student)':<30} {student_ppl:>10.2f} {student_params:>15,}\")\nprint(\"-\" * 55)\nprint(f\"{'compression':<30} {compression_ratio:>10.1f}x\")\nprint(f\"{'ppl gap':<30} {student_ppl - teacher_ppl:>10.2f}\")\nprint(f\"{'spike density':<30} {student.get_avg_spike_density():>10.3f}\")\nprint(f\"{'VRAM peak':<30} {vram_peak_gb:>10.2f}GB\")\nprint(f\"{'final temperature':<30} {final_temp:>10.2f}\")\nprint(f\"{'final lambda (GRL)':<30} {final_lambda:>10.3f}\")\nprint(\"\")\nprint(\"CTKD Implementation:\")\nprint(f\"  tau range: [{config.tau_min:.1f}, {config.tau_max:.1f}]\")\nprint(f\"  lambda warmup ratio: {config.lambda_warmup_ratio:.0%}\")\nprint(f\"  GRL: Gradient Reversal Layer for adversarial min-max\")\nprint(\"\")\nprint(\"version comparison:\")\nprint(f\"  v6: 627.3 PPL (baseline)\")\nprint(f\"  v7: 1655 PPL (regression!)\")\nprint(f\"  v8: 559 PPL (fixed)\")\nprint(f\"  v9: 541.7 PPL (capacity increase)\")\nprint(f\"  v10: 514.5 PPL (320d/5L baseline)\")\nprint(f\"  v11: 512.67 PPL (channel-wise, WITH reg)\")\nprint(f\"  v11.1: 512.04 PPL (channel-wise, NO reg)\")\nprint(f\"  v12: FAILED (temp runaway without GRL)\")\nprint(f\"  v12.1: {student_ppl:.2f} PPL (CTKD+GRL, T={final_temp:.2f}, \u03bb={final_lambda:.3f})\")\nif student_ppl < 500:\n    print(f\"  v12.1 TARGET MET! PPL < 500\")\nelif student_ppl < 512.04:\n    print(f\"  v12.1 beats v11.1 by {512.04 - student_ppl:.1f} PPL\")\nelse:\n    print(f\"  WARNING: v12.1 did not improve over v11.1\")\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n# cell 23: visualization\n# =============================================================================\nfig, axes = plt.subplots(2, 3, figsize=(15, 10))\n\n# distillation loss\nd_steps = [l['step'] for l in distill_logs['loss_history']]\nd_losses = [l['loss'] for l in distill_logs['loss_history']]\nkl_losses = [l['loss'] for l in distill_logs['kl_loss_history']]\naxes[0,0].plot(d_steps, d_losses, label='total', alpha=0.8)\naxes[0,0].plot(d_steps, kl_losses, label='kl', alpha=0.7)\naxes[0,0].set_xlabel('step')\naxes[0,0].set_ylabel('loss')\naxes[0,0].set_title('distillation loss (v11)')\naxes[0,0].legend()\n\n# validation ppl\np_steps = [l['step'] for l in distill_logs['ppl_history']]\np_ppls = [l['ppl'] for l in distill_logs['ppl_history']]\naxes[0,1].plot(p_steps, p_ppls, 'orange', marker='o')\naxes[0,1].axhline(y=teacher_ppl, color='green', linestyle='--', label=f'teacher ({teacher_ppl:.1f})')\naxes[0,1].axhline(y=627.3, color='blue', linestyle=':', label='v6 (627.3)')\naxes[0,1].axhline(y=541.7, color='purple', linestyle=':', label='v9 (541.7)')\naxes[0,1].axhline(y=480, color='red', linestyle='--', label='v11 target (480)')\naxes[0,1].set_xlabel('step')\naxes[0,1].set_ylabel('ppl')\naxes[0,1].set_title('validation ppl')\naxes[0,1].legend()\n\n# lr schedule\nlr_steps = [l['step'] for l in distill_logs['lr_history']]\nlr_vals = [l['lr'] for l in distill_logs['lr_history']]\naxes[0,2].plot(lr_steps, lr_vals, 'purple')\naxes[0,2].axvline(x=config.warmup_steps, color='gray', linestyle='--', label=f'warmup ({config.warmup_steps})')\naxes[0,2].set_xlabel('step')\naxes[0,2].set_ylabel('lr')\naxes[0,2].set_title('learning rate')\naxes[0,2].legend()\n\n# spike density + amplitudes (first 4 layers)\nspike_summary = spike_stats.get_summary()\nlayers = [f'layer_{i}' for i in range(min(4, config.n_layers))]\nk_dens = [spike_summary['per_layer'][l]['k_final'] for l in layers]\nv_dens = [spike_summary['per_layer'][l]['v_final'] for l in layers]\nk_amps = [spike_summary['per_layer'][l]['k_amp_final'] for l in layers]\nv_amps = [spike_summary['per_layer'][l]['v_amp_final'] for l in layers]\n\nx = np.arange(len(layers))\naxes[1,0].bar(x - 0.2, k_dens, 0.4, label='k density')\naxes[1,0].bar(x + 0.2, v_dens, 0.4, label='v density')\nax2 = axes[1,0].twinx()\nax2.plot(x, k_amps, 'r-o', label='k amp')\nax2.plot(x, v_amps, 'b-s', label='v amp')\naxes[1,0].set_xlabel('layer')\naxes[1,0].set_ylabel('density')\nax2.set_ylabel('amplitude')\naxes[1,0].set_title(f'spike density & amps (first 4/{config.n_layers} layers)')\naxes[1,0].legend(loc='upper left')\nax2.legend(loc='upper right')\n\n# ttt loss\nt_steps = [l['step'] for l in ttt_logs['loss_history']]\nt_losses = [l['loss'] for l in ttt_logs['loss_history']]\naxes[1,1].plot(t_steps, t_losses, 'red')\naxes[1,1].set_xlabel('step')\naxes[1,1].set_ylabel('ce loss')\naxes[1,1].set_title('ttt with lora')\n\n# version comparison\nversions = ['v6', 'v7', 'v8', 'v9', 'v11']\nt_ppls = [44.6, 44.6, 44.6, 44.6, teacher_ppl]\ns_ppls = [627.3, 1655, 559, 541.7, student_ppl]\nx = np.arange(len(versions))\naxes[1,2].bar(x - 0.2, t_ppls, 0.4, label='teacher', alpha=0.7)\naxes[1,2].bar(x + 0.2, s_ppls, 0.4, label='student', alpha=0.7)\naxes[1,2].axhline(y=480, color='red', linestyle='--', label='v11 target', alpha=0.7)\naxes[1,2].set_xticks(x)\naxes[1,2].set_xticklabels(versions)\naxes[1,2].set_ylabel('ppl')\naxes[1,2].set_title('version comparison')\naxes[1,2].legend()\naxes[1,2].set_yscale('log')\n\nplt.tight_layout()\nfigure_path = f'{OUTPUT_DIR}/figures/v11_training_{RUN_TIMESTAMP}.png'\nplt.savefig(figure_path, dpi=300, bbox_inches='tight')\nplt.show()\nprint(f\"saved: {figure_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# cell 24: build results dict (v12.1 - Proper CTKD with GRL)\n# =============================================================================\nprint(\"building results (v12.1 - CTKD with Gradient Reversal Layer)...\")\n\nwith open(figure_path, 'rb') as f:\n    figure_base64 = base64.b64encode(f.read()).decode('utf-8')\n\n# v12.1: Extract final lambda\nfinal_lambda = distill_logs['temp_history'][-1].get('lambda', config.lambda_max) if distill_logs['temp_history'] else config.lambda_max\n\nresults = {\n    'version': 'v12.1',\n    'timestamp': datetime.now().isoformat(),\n    'run_id': RUN_TIMESTAMP,\n    'platform': PLATFORM,\n    'description': 'CTKD with Gradient Reversal Layer - adversarial temperature learning',\n\n    'v12.1_design': {\n        'principle': 'CTKD: Adversarial min-max optimization via GRL',\n        'innovation': 'Gradient Reversal Layer makes temperature MAXIMIZE KL while student MINIMIZES',\n        'rationale': 'Proper CTKD (ArXiv 2211.16231) requires adversarial training, not simple regularization',\n        'why_v12_failed': 'v12 used simple regularization - optimizer pushed T to max for easy KL',\n        'techniques': {\n            'ctkd_with_grl': 'ENABLED - Gradient Reversal Layer for adversarial min-max (v12.1 KEY)',\n            'lambda_scheduling': f'Cosine warmup 0->{config.lambda_max} with {config.lambda_warmup_ratio:.0%} warmup',\n            'sigmoid_bounding': f'T bounded to [{config.tau_min}, {config.tau_max}] via sigmoid (smooth gradients)',\n            'no_manual_reg': 'GRL eliminates need for manual temperature regularization',\n            'progressive_stages': 'DISABLED',\n            'channel_wise_spikes': 'DISABLED (structural symmetry issue)',\n        },\n        'grl_mechanism': {\n            'forward_pass': 'Identity: GRL(x) = x',\n            'backward_pass': 'Negation: dGRL/dx = -lambda',\n            'effect': 'Temperature gradients reversed -> T maximizes KL loss',\n        },\n        'temperature_config': {\n            'tau_min': config.tau_min,\n            'tau_max': config.tau_max,\n            'tau_init': config.tau_init,\n            'lambda_max': config.lambda_max,\n            'lambda_warmup_ratio': config.lambda_warmup_ratio,\n        },\n        'architecture': {\n            'd_model': 320,\n            'n_layers': 5,\n            'params': '~22M',\n        },\n        'speedups': {\n            'gradient_checkpointing': USE_GRADIENT_CHECKPOINTING,\n            'torch_compile': compile_success,\n            'fused_optimizer': True,\n            'accumulation_steps': config.accumulation_steps,\n        },\n        'unchanged': [\n            'hidden_align_weight: 0.0',\n            'warmup_steps: 50',\n            'distill_steps: 3000',\n        ],\n    },\n\n    'architecture': {\n        'teacher': {'name': 'gpt2', 'params': teacher_params},\n        'student': {\n            'name': 'asnn-goose-v12.1',\n            'd_model': config.d_model,\n            'n_layers': config.n_layers,\n            'params': student_params,\n        },\n        'projector_params': projector_params,\n        'compression_ratio': compression_ratio,\n        'vram_peak_gb': vram_peak_gb,\n    },\n\n    'training_config': {\n        'distill_steps': config.distill_steps,\n        'tau_min': config.tau_min,\n        'tau_max': config.tau_max,\n        'tau_init': config.tau_init,\n        'final_temperature': final_temp,\n        'lambda_max': config.lambda_max,\n        'lambda_warmup_ratio': config.lambda_warmup_ratio,\n        'final_lambda': final_lambda,\n        'hidden_align_weight': config.hidden_align_weight,\n        'warmup_steps': config.warmup_steps,\n        'batch_size': config.batch_size,\n        'accumulation_steps': config.accumulation_steps,\n        'effective_batch': config.batch_size * config.accumulation_steps,\n        'distill_lr': config.distill_lr,\n        'max_grad_norm': config.max_grad_norm,\n    },\n\n    'results': {\n        'teacher_ppl': teacher_ppl,\n        'student_ppl': student_ppl,\n        'ppl_gap': student_ppl - teacher_ppl,\n        'spike_density': student.get_avg_spike_density(),\n        'amplitudes': student.get_amplitudes(),\n        'final_temperature': final_temp,\n        'final_lambda': final_lambda,\n        'target_met': student_ppl < 500,\n    },\n\n    'training_curves': {\n        'loss_history': distill_logs['loss_history'],\n        'kl_loss_history': distill_logs['kl_loss_history'],\n        'align_loss_history': distill_logs['align_loss_history'],\n        'ppl_history': distill_logs['ppl_history'],\n        'lr_history': distill_logs['lr_history'],\n        'temp_history': distill_logs['temp_history'],  # v12.1: includes temperature AND lambda\n    },\n\n    'hardware_stats': hw_stats.get_summary(),\n    'spike_analysis': spike_stats.get_summary(),\n\n    'ttt': {\n        'lora_params': lora_params,\n        'pre_ppl': pre_ttt_ppl,\n        'post_ppl': post_ttt_ppl,\n        'improvement': pre_ttt_ppl - post_ttt_ppl,\n        'loss_history': ttt_logs['loss_history'],\n    },\n\n    'comparison': {\n        'v6': {'student_ppl': 627.3, 'note': 'baseline'},\n        'v7': {'student_ppl': 1655, 'note': 'regression (align=1.0, T=4)'},\n        'v8': {'student_ppl': 559, 'note': 'fixed defaults (align=0, T=2)'},\n        'v9': {'student_ppl': 541.7, 'note': 'capacity increase (320d, 5L)'},\n        'v10': {'student_ppl': 514.5, 'note': '320d/5L baseline'},\n        'v11': {'student_ppl': 512.67, 'note': 'channel-wise WITH reg (bug)'},\n        'v11.1': {'student_ppl': 512.04, 'note': 'channel-wise NO reg (symmetry issue)'},\n        'v12': {'student_ppl': 'FAILED', 'note': 'temp runaway without GRL'},\n        'v12.1': {'student_ppl': student_ppl, 'note': f'CTKD+GRL (T={final_temp:.2f}, L={final_lambda:.3f})'},\n    },\n\n    'figures': {\n        'training_plot': {\n            'filename': f'v12.1_training_{RUN_TIMESTAMP}.png',\n            'base64': figure_base64,\n        }\n    },\n\n    # validation_tests will be added in cell 26\n}\n\nprint(\"results dict built (validation_tests pending)\")\nprint(f\"  version: v12.1 (CTKD with GRL)\")\nprint(f\"  final_temperature: {final_temp:.2f}\")\nprint(f\"  final_lambda: {final_lambda:.3f}\")\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# cell 25: validation tests (v12.1 - 10 tests with proper CTKD)\n# =============================================================================\nprint(\"=\"*60)\nprint(\"validation tests (v12.1 - CTKD with Gradient Reversal Layer)\")\nprint(\"=\"*60)\n\ntest_results = {}\n\n# 1. teacher pre-trained\nprint(\"\\n[1] teacher pre-trained\")\ntest_results['teacher_pretrained'] = teacher_ppl < 50\nprint(f\"  teacher ppl: {teacher_ppl:.2f} - {'pass' if test_results['teacher_pretrained'] else 'fail'}\")\n\n# 2. student learned (v12.1 target: <500)\nprint(\"\\n[2] student learned language\")\ntest_results['student_learned'] = student_ppl < 627\nprint(f\"  student ppl: {student_ppl:.2f} - {'pass' if test_results['student_learned'] else 'fail'} (target: < 627)\")\n\n# 3. ternary activations (scalar amplitudes)\nprint(\"\\n[3] ternary activations\")\nstudent.eval()\nwith torch.no_grad():\n    test_ids = next(iter(val_loader))[0].to(DEVICE)\n    layer = student.layers[0]['rec']\n    x = student.embed(test_ids) + student.pos_embed(torch.arange(test_ids.size(1), device=DEVICE).unsqueeze(0))\n    x_norm = layer.ln(x)\n    prev_x = F.pad(x_norm[:, :-1, :], (0, 0, 1, 0))\n    xk = x_norm * layer.time_mix_k + prev_x * (1 - layer.time_mix_k)\n    k_spike = layer.k_spike(layer.key_proj(xk))\n\n    # For scalar amplitudes, values should be {-amp, 0, +amp} = 3 unique\n    # Round to 4 decimal places to handle floating point\n    unique_vals = len(set([round(v, 4) for v in k_spike.unique().cpu().tolist()]))\n    test_results['ternary'] = unique_vals <= 3\n    print(f\"  unique values: {unique_vals} - {'pass' if test_results['ternary'] else 'fail'}\")\n\n# 4. gradient flow\nprint(\"\\n[4] gradient flow (STE)\")\n_spike = TrainableTernarySpike().to(DEVICE)\n_x = torch.randn(2, 16, 64, device=DEVICE, requires_grad=True)\n_spike(_x).sum().backward()\ntest_results['gradient'] = _x.grad is not None and _x.grad.abs().sum() > 0\nprint(f\"  {'pass' if test_results['gradient'] else 'fail'}\")\n\n# 5. spike density\nprint(\"\\n[5] spike density in range\")\ndensity = student.get_avg_spike_density()\ntest_results['density'] = 0.1 < density < 0.9\nprint(f\"  density: {density:.3f} - {'pass' if test_results['density'] else 'fail'}\")\n\n# 6. lora applied\nprint(\"\\n[6] lora applied\")\ntest_results['lora'] = len(lora_modules) > 0\nprint(f\"  modules: {len(lora_modules)} - {'pass' if test_results['lora'] else 'fail'}\")\n\n# 7. improvement over v6 baseline\nprint(\"\\n[7] beats v6 baseline\")\ntest_results['beats_v6'] = student_ppl < 627.3\nprint(f\"  v6: 627.3, v12.1: {student_ppl:.2f} - {'pass' if test_results['beats_v6'] else 'fail'}\")\n\n# =============================================================================\n# [8] v12.1: Amplitudes learned (scalar, not channel-wise)\n# v12.1 uses TrainableTernarySpike (scalar amplitude) since channel-wise disabled\n# =============================================================================\nprint(\"\\n[8] amplitudes learned (scalar)\")\namps = student.get_amplitudes()\namp_vals = []\nfor layer_idx, amp_dict in amps.items():\n    amp_vals.extend([amp_dict['k'], amp_dict['v']])\n\n# Scalar amplitudes should deviate from 1.0 (initial value)\namp_deviation = max(abs(a - 1.0) for a in amp_vals)\ntest_results['amplitudes_learned'] = amp_deviation > 0.01  # Should move from 1.0\namp_strs = [f\"L{i}:{amps[f'layer_{i}']['k']:.3f}\" for i in range(config.n_layers)]\nprint(f\"  amplitudes: {amp_strs}\")\nprint(f\"  max deviation from 1.0: {amp_deviation:.4f}\")\nprint(f\"  {'pass' if test_results['amplitudes_learned'] else 'fail'} - amplitudes should deviate from 1.0\")\n\n# 9. amplitude health check\nprint(\"\\n[9] amplitude health\")\nall_healthy = True\nfor layer_idx, amp_dict in amps.items():\n    k_amp, v_amp = amp_dict['k'], amp_dict['v']\n    if not (0.3 < k_amp < 3.0) or not (0.3 < v_amp < 3.0):\n        print(f\"  WARNING: {layer_idx} unhealthy: k={k_amp:.3f}, v={v_amp:.3f}\")\n        all_healthy = False\ntest_results['amplitude_health'] = all_healthy\nprint(f\"  {'pass' if all_healthy else 'fail'} - all amplitudes in [0.3, 3.0]\")\n\n# =============================================================================\n# [10] v12.1 KEY TEST: CTKD with Gradient Reversal Layer\n# This is THE key test for v12.1 - verifies proper adversarial temperature learning\n# =============================================================================\nprint(\"\\n[10] CTKD with GRL (v12.1 KEY TEST)\")\nif config.use_ctkd and 'temp_history' in distill_logs:\n    temps = [h['temperature'] for h in distill_logs['temp_history']]\n    lambdas = [h.get('lambda', 0) for h in distill_logs['temp_history']]\n\n    if len(temps) > 100:\n        # ===== CTKD-SPECIFIC CHECKS =====\n\n        # 1. Lambda schedule worked (should go from 0 to ~lambda_max)\n        warmup_steps = int(config.distill_steps * config.lambda_warmup_ratio)\n        warmup_lambdas = lambdas[:min(warmup_steps // 10, len(lambdas))]  # Samples during warmup\n        late_lambdas = lambdas[int(len(lambdas) * 0.8):]  # Last 20%\n\n        lambda_was_zero_early = len(warmup_lambdas) > 0 and all(l < 0.01 for l in warmup_lambdas)\n        lambda_increased = len(late_lambdas) > 0 and np.mean(late_lambdas) > 0.5\n\n        print(f\"  lambda schedule:\")\n        print(f\"    early (warmup): {np.mean(warmup_lambdas) if warmup_lambdas else 0:.4f} (should be ~0)\")\n        print(f\"    late: {np.mean(late_lambdas) if late_lambdas else 0:.4f} (should be ~{config.lambda_max})\")\n        print(f\"    warmup correct: {lambda_was_zero_early}\")\n        print(f\"    increased: {lambda_increased}\")\n\n        # 2. Temperature stayed in bounds [tau_min, tau_max]\n        min_temp, max_temp = min(temps), max(temps)\n        temp_in_bounds = min_temp >= config.tau_min - 0.01 and max_temp <= config.tau_max + 0.01\n        print(f\"  temperature bounds: [{min_temp:.3f}, {max_temp:.3f}]\")\n        print(f\"    expected: [{config.tau_min}, {config.tau_max}]\")\n        print(f\"    in bounds: {temp_in_bounds}\")\n\n        # 3. Temperature did NOT runaway to max (unlike v12)\n        final_temp_val = temps[-1]\n        not_at_max = final_temp_val < config.tau_max - 0.1  # Not stuck at upper bound\n        not_at_min = final_temp_val > config.tau_min + 0.1  # Not stuck at lower bound\n        print(f\"  final temperature: {final_temp_val:.3f}\")\n        print(f\"    not at max ({config.tau_max}): {not_at_max}\")\n        print(f\"    not at min ({config.tau_min}): {not_at_min}\")\n\n        # 4. Temperature evolved meaningfully (changed during training)\n        temp_range = max_temp - min_temp\n        temp_evolved = temp_range > 0.05  # Should have some movement\n        print(f\"  temperature evolution range: {temp_range:.4f}\")\n        print(f\"    evolved: {temp_evolved}\")\n\n        # 5. Temperature stabilized (low variance in last 20%)\n        late_temps = temps[int(len(temps) * 0.8):]\n        temp_variance = np.std(late_temps)\n        temp_stable = temp_variance < 0.5  # Tighter bound for CTKD\n        print(f\"  late temperature variance: {temp_variance:.4f}\")\n        print(f\"    stable: {temp_stable}\")\n\n        # OVERALL CTKD TEST\n        # Pass requires:\n        # - Lambda schedule worked (was 0 during warmup, increased after)\n        # - Temperature stayed in sigmoid bounds\n        # - Temperature didn't runaway (GRL prevented max seeking)\n        # - Temperature evolved and stabilized\n        ctkd_working = (\n            lambda_increased and  # Lambda actually increased\n            temp_in_bounds and    # Sigmoid bounding worked\n            not_at_max and        # GRL prevented runaway\n            temp_stable           # Training converged\n        )\n\n        test_results['ctkd_temperature'] = ctkd_working\n        print(f\"\\n  CTKD RESULT: {'PASS' if ctkd_working else 'FAIL'}\")\n\n        if not lambda_increased:\n            print(f\"  ISSUE: Lambda did not increase (GRL not active?)\")\n        if not temp_in_bounds:\n            print(f\"  ISSUE: Temperature escaped sigmoid bounds\")\n        if not not_at_max:\n            print(f\"  ISSUE: Temperature stuck at max (GRL failed to prevent runaway)\")\n        if not temp_stable:\n            print(f\"  ISSUE: Temperature still oscillating (training not converged)\")\n\n    else:\n        test_results['ctkd_temperature'] = False\n        print(f\"  not enough data points ({len(temps)}) - fail\")\nelse:\n    test_results['ctkd_temperature'] = False  # FAIL if CTKD disabled in v12.1\n    print(f\"  CTKD disabled - FAIL (v12.1 requires this feature)\")\n\nprint(\"\\n\" + \"=\"*60)\npassed = sum(1 for v in test_results.values() if v)\nprint(f\"results: {passed}/{len(test_results)} passed\")\nif student_ppl < 500:\n    print(f\"v12.1 TARGET MET! PPL {student_ppl:.2f} < 500\")\nelif student_ppl < 512.04:\n    print(f\"v12.1 improved from v11.1: {512.04 - student_ppl:.1f} PPL reduction\")\nelse:\n    print(f\"v12.1 did NOT improve from v11.1 (512.04)\")\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# cell 26: FINAL save with validation_tests (v11 bug fix)\n",
    "# =============================================================================\n",
    "print(\"=\"*60)\n",
    "print(\"FINAL SAVE (includes validation_tests)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Add validation_tests to results\n",
    "results['validation_tests'] = test_results\n",
    "\n",
    "# Save final results.json with ALL data\n",
    "results_path = f'{OUTPUT_DIR}/results/results_{RUN_TIMESTAMP}.json'\n",
    "with open(results_path, 'w') as f:\n",
    "    json.dump(results, f, indent=2, default=str)\n",
    "\n",
    "print(f\"saved: {results_path}\")\n",
    "print(f\"size: {os.path.getsize(results_path) / 1024:.1f} KB\")\n",
    "print(f\"\")\n",
    "print(f\"validation_tests included: {list(test_results.keys())}\")\n",
    "\n",
    "# v11: auto-download AFTER final save\n",
    "print(\"\")\n",
    "print(\"auto-download\")\n",
    "if IS_COLAB:\n",
    "    try:\n",
    "        from google.colab import files\n",
    "        files.download(results_path)\n",
    "        files.download(figure_path)\n",
    "        print(\"downloads started!\")\n",
    "    except Exception as e:\n",
    "        print(f\"download failed: {e}\")\n",
    "elif IS_KAGGLE:\n",
    "    print(f\"kaggle: {results_path}\")\n",
    "else:\n",
    "    print(f\"local: {results_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}