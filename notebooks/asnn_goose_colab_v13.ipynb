{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# cell 1: environment setup (v11 - POCL + Channel-wise)\n",
    "# =============================================================================\n",
    "import os\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import json\n",
    "import base64\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Dict, List, Optional, Tuple, Any\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# v11: torch.compile flag (disable if causing issues)\n",
    "USE_TORCH_COMPILE = True\n",
    "USE_GRADIENT_CHECKPOINTING = True\n",
    "\n",
    "# generate timestamp for this run\n",
    "RUN_TIMESTAMP = datetime.now().strftime('%Y-%m-%d_%H%M%S')\n",
    "print(f\"run timestamp: {RUN_TIMESTAMP}\")\n",
    "\n",
    "# detect platform\n",
    "IS_KAGGLE = 'KAGGLE_KERNEL_RUN_TYPE' in os.environ\n",
    "IS_COLAB = 'COLAB_GPU' in os.environ or 'google.colab' in sys.modules\n",
    "PLATFORM = 'kaggle' if IS_KAGGLE else 'colab' if IS_COLAB else 'local'\n",
    "OUTPUT_DIR = '/kaggle/working/outputs' if IS_KAGGLE else 'outputs'\n",
    "\n",
    "for subdir in ['figures', 'checkpoints', 'logs', 'results']:\n",
    "    os.makedirs(f'{OUTPUT_DIR}/{subdir}', exist_ok=True)\n",
    "\n",
    "print(f\"platform: {PLATFORM}\")\n",
    "print(f\"output directory: {OUTPUT_DIR}\")\n",
    "print(f\"torch.compile: {'enabled' if USE_TORCH_COMPILE else 'disabled'}\")\n",
    "print(f\"gradient checkpointing: {'enabled' if USE_GRADIENT_CHECKPOINTING else 'disabled'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# cell 2: pytorch and hardware setup (v11 - POCL)\n",
    "# =============================================================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "SEED = 42\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"gpu: {gpu_name}\")\n",
    "    print(f\"memory: {gpu_memory:.1f} gb\")\n",
    "\n",
    "# v11: set float32 matmul precision for torch.compile\n",
    "if USE_TORCH_COMPILE and hasattr(torch, 'set_float32_matmul_precision'):\n",
    "    torch.set_float32_matmul_precision('high')\n",
    "    print(\"float32 matmul precision: high (for torch.compile)\")\n",
    "\n",
    "print(f\"device: {DEVICE}\")\n",
    "print(f\"pytorch: {torch.__version__}\")\n",
    "\n",
    "# check torch.compile availability\n",
    "TORCH_COMPILE_AVAILABLE = hasattr(torch, 'compile') and torch.__version__ >= '2.0'\n",
    "print(f\"torch.compile available: {TORCH_COMPILE_AVAILABLE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n# cell 4: configuration (v13.1 - Extended Training with CTKD)\n# =============================================================================\n@dataclass\nclass Config:\n    # gpt-2 teacher (frozen, pre-trained)\n    teacher_name: str = \"gpt2\"\n\n    # student model architecture - v13: same as v13 (320d, 5L, ~22M)\n    d_model: int = 320      # v10 value (DO NOT reduce)\n    n_layers: int = 5       # v10 value (DO NOT reduce)\n    vocab_size: int = 50257\n    max_seq_len: int = 256\n\n    # ==========================================================================\n    # v13: Extended Training (5000 steps)\n    # ==========================================================================\n    distill_steps: int = 5000  # Extended from 3000         # v13: extended from 3000\n    distill_lr: float = 3e-4\n    warmup_steps: int = 100           # v13: increased for 5000 steps\n    min_lr: float = 1e-6              # v13: lower final LR\n\n    # v13: gradient accumulation (same as v10/v13)\n    accumulation_steps: int = 2       # effective batch = 8 * 2 = 16\n\n    # ==========================================================================\n    # v13: Early Stopping\n    # ==========================================================================\n    use_early_stopping: bool = True\n    early_stopping_patience: int = 500  # steps without improvement\n    min_ppl_delta: float = 1.0          # minimum PPL improvement to count\n\n    # ==========================================================================\n    # v13: POCL (Progressive Overload Curriculum Learning)\n    # ==========================================================================\n    use_pocl: bool = False  # v13.1: DISABLED (caused regression)               # v13: ENABLED\n    pocl_stages: int = 3                # 3-stage curriculum\n    pocl_temp_schedule: tuple = (1.0, 1.5, 2.0)  # Rising temperature per stage\n    pocl_pretrain_steps: int = 100      # Brief pre-training before difficulty scoring\n\n    # ==========================================================================\n    # v13: CTKD DISABLED (using fixed POCL schedule instead)\n    # ==========================================================================\n    use_ctkd: bool = True   # v13.1: RE-ENABLED (proven to work)              # v13: DISABLED (using POCL fixed schedule)\n    \n    # Temperature bounds (only used if CTKD enabled)\n    tau_min: float = 1.0\n    tau_max: float = 5.0\n    tau_init: float = 2.0\n    \n    # Lambda scheduling (only used if CTKD enabled)\n    lambda_max: float = 1.0\n    lambda_warmup_ratio: float = 0.2\n\n    # Legacy flags (all disabled for v13)\n    use_learnable_temperature: bool = False\n    use_channel_wise_spikes: bool = False\n    use_progressive_stages: bool = False\n    temperature: float = 2.0            # Fallback if nothing else enabled\n\n    # Hidden alignment DISABLED (caused regression in v7)\n    hidden_align_weight: float = 0.0\n    teacher_d_model: int = 768\n    teacher_n_layers: int = 12\n    temperature_lr: float = 0.001\n\n    # lora for ttt\n    lora_rank: int = 8\n    lora_alpha: float = 16.0\n    ttt_lr: float = 1e-4\n    ttt_steps: int = 100\n\n    # spiking parameters\n    spike_alpha: float = 1.0\n\n    # general training\n    batch_size: int = 8\n    max_grad_norm: float = 1.0\n    eval_interval: int = 300\n\nconfig = Config()\n\nprint(f\"configuration (v13.1 - Extended Training with CTKD):\")\nprint(f\"  teacher: {config.teacher_name} (124m params)\")\nprint(f\"  student: d={config.d_model}, layers={config.n_layers}\")\nprint(f\"\")\nprint(f\"v13 INNOVATION #1 - POCL (Progressive Overload Curriculum):\")\nprint(f\"  use_pocl: {config.use_pocl}\")\nprint(f\"  stages: {config.pocl_stages}\")\nprint(f\"  temperature schedule: {config.pocl_temp_schedule}\")\nprint(f\"  pre-train steps: {config.pocl_pretrain_steps}\")\nprint(f\"\")\nprint(f\"  POCL Strategy:\")\nprint(f\"    Stage 1: Easy 33% data,  T={config.pocl_temp_schedule[0]}\")\nprint(f\"    Stage 2: Med  66% data,  T={config.pocl_temp_schedule[1]}\")\nprint(f\"    Stage 3: All 100% data,  T={config.pocl_temp_schedule[2]}\")\nprint(f\"\")\nprint(f\"v13 INNOVATION #2 - Extended Training:\")\nprint(f\"  distill_steps: {config.distill_steps} (was 3000 in v13)\")\nprint(f\"  warmup_steps: {config.warmup_steps}\")\nprint(f\"  min_lr: {config.min_lr}\")\nprint(f\"\")\nprint(f\"v13 INNOVATION #3 - Early Stopping:\")\nprint(f\"  use_early_stopping: {config.use_early_stopping}\")\nprint(f\"  patience: {config.early_stopping_patience} steps\")\nprint(f\"  min_delta: {config.min_ppl_delta} PPL\")\nprint(f\"\")\nprint(f\"disabled features:\")\nprint(f\"  CTKD: {config.use_ctkd} (using fixed POCL schedule)\")\nprint(f\"  channel-wise spikes: {config.use_channel_wise_spikes}\")\nprint(f\"  hidden alignment: {config.hidden_align_weight}\")\nprint(f\"\")\nprint(f\"training:\")\nprint(f\"  accumulation: {config.accumulation_steps} (effective batch = {config.batch_size * config.accumulation_steps})\")\nprint(f\"\")\nprint(f\"targets:\")\nprint(f\"  PPL: <420 (improve on v13 445.61)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n# cell 6: v13 PROPER CTKD Implementation\n# =============================================================================\n# References:\n# - CTKD Paper: https://arxiv.org/abs/2211.16231\n# - GRL Origin: Ganin & Lempitsky (2015) https://arxiv.org/abs/1409.7495\n# - torch-gradient-reversal: https://pypi.org/project/torch-gradient-reversal/\n\n# -----------------------------------------------------------------------------\n# GradientReversalFunction (Custom Autograd)\n# -----------------------------------------------------------------------------\nclass GradientReversalFunction(torch.autograd.Function):\n    \"\"\"\n    Gradient Reversal Layer for adversarial training.\n    \n    Forward: Identity mapping f(x) = x\n    Backward: Negates gradient \u2202f/\u2202x = -\u03bb * grad\n    \n    This enables min-max optimization in a single backward pass:\n    - Student minimizes loss (normal gradients)\n    - Temperature maximizes loss (reversed gradients via GRL)\n    \n    Reference: Ganin & Lempitsky, \"Unsupervised Domain Adaptation by Backpropagation\"\n    \"\"\"\n    \n    @staticmethod\n    def forward(ctx, x, lambda_):\n        # Save lambda for backward pass\n        ctx.lambda_ = lambda_\n        # Forward is identity (must clone to avoid in-place issues)\n        return x.clone()\n    \n    @staticmethod\n    def backward(ctx, grad_output):\n        # Backward negates and scales gradient\n        # Returns: (grad for x, grad for lambda_)\n        # lambda_ is a hyperparameter, doesn't need gradient\n        return -ctx.lambda_ * grad_output, None\n\n\nclass GradientReversalLayer(nn.Module):\n    \"\"\"\n    Module wrapper for GradientReversalFunction.\n    \n    Usage:\n        grl = GradientReversalLayer()\n        grl.set_lambda(0.5)  # Set adversarial strength\n        y = grl(x)  # Forward: y = x, Backward: grad_x = -0.5 * grad_y\n    \"\"\"\n    \n    def __init__(self):\n        super().__init__()\n        self.lambda_ = 1.0\n    \n    def set_lambda(self, lambda_: float):\n        \"\"\"Set the adversarial strength (0 = no reversal, 1 = full reversal).\"\"\"\n        self.lambda_ = lambda_\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return GradientReversalFunction.apply(x, self.lambda_)\n\n\n# -----------------------------------------------------------------------------\n# Lambda Scheduler (Cosine with Warmup)\n# -----------------------------------------------------------------------------\ndef get_lambda(step: int, total_steps: int, lambda_max: float = 1.0, \n               warmup_ratio: float = 0.2) -> float:\n    \"\"\"\n    Cosine schedule for adversarial strength \u03bb.\n    \n    - During warmup (first warmup_ratio of training): \u03bb = 0\n      Temperature learns freely to find reasonable range\n    - After warmup: \u03bb increases from 0 to lambda_max via cosine\n      Gradually increases adversarial pressure\n    \n    Args:\n        step: Current training step\n        total_steps: Total number of training steps\n        lambda_max: Maximum \u03bb value (default 1.0 = full reversal)\n        warmup_ratio: Fraction of training for warmup (default 0.2 = 20%)\n    \n    Returns:\n        Current \u03bb value in [0, lambda_max]\n    \"\"\"\n    warmup_steps = int(total_steps * warmup_ratio)\n    \n    if step < warmup_steps:\n        return 0.0\n    \n    # Progress after warmup [0, 1]\n    progress = (step - warmup_steps) / max(total_steps - warmup_steps, 1)\n    # Cosine increase from 0 to lambda_max\n    lambda_ = lambda_max * (1 - math.cos(math.pi * progress)) / 2\n    return lambda_\n\n\n# -----------------------------------------------------------------------------\n# CTKDTemperature (Proper Implementation with GRL)\n# -----------------------------------------------------------------------------\nclass CTKDTemperature(nn.Module):\n    \"\"\"\n    Curriculum Temperature for Knowledge Distillation (CTKD).\n    \n    Key features:\n    1. Adversarial learning via Gradient Reversal Layer\n    2. Sigmoid bounding for smooth gradients at boundaries\n    3. Proper initialization via logit transform\n    \n    The temperature module tries to MAXIMIZE the KL loss (via GRL),\n    finding the \"hardest\" temperature for the student.\n    The student tries to MINIMIZE the KL loss.\n    This adversarial game leads to optimal curriculum difficulty.\n    \n    Reference: Li et al., \"Curriculum Temperature for Knowledge Distillation\", AAAI 2023\n    \"\"\"\n    \n    def __init__(self, tau_min: float = 1.0, tau_max: float = 5.0, init: float = 2.0):\n        \"\"\"\n        Args:\n            tau_min: Minimum temperature (default 1.0)\n            tau_max: Maximum temperature (default 5.0, conservative for LLMs)\n            init: Initial temperature (default 2.0)\n        \"\"\"\n        super().__init__()\n        self.tau_min = tau_min\n        self.tau_range = tau_max - tau_min\n        \n        # Initialize raw parameter so sigmoid outputs init value\n        # sigmoid(raw) = (init - tau_min) / tau_range\n        # raw = logit((init - tau_min) / tau_range)\n        init_normalized = (init - tau_min) / self.tau_range\n        init_normalized = max(0.01, min(0.99, init_normalized))  # Clamp for numerical stability\n        init_raw = math.log(init_normalized / (1 - init_normalized))  # logit function\n        \n        self.raw_temp = nn.Parameter(torch.tensor(init_raw, dtype=torch.float32))\n        self.grl = GradientReversalLayer()\n        \n        # Store config for logging\n        self.tau_min_val = tau_min\n        self.tau_max_val = tau_max\n        self.init_val = init\n    \n    def forward(self, lambda_: float) -> torch.Tensor:\n        \"\"\"\n        Compute temperature with GRL applied.\n        \n        Args:\n            lambda_: Current adversarial strength from scheduler\n        \n        Returns:\n            Temperature \u03c4 \u2208 [tau_min, tau_max]\n        \"\"\"\n        # Set GRL strength\n        self.grl.set_lambda(lambda_)\n        \n        # Apply GRL to raw parameter (this is where gradient reversal happens!)\n        raw_reversed = self.grl(self.raw_temp)\n        \n        # Sigmoid bounding (smooth, differentiable at boundaries)\n        tau = self.tau_min + self.tau_range * torch.sigmoid(raw_reversed)\n        \n        return tau\n    \n    def get_temperature(self) -> float:\n        \"\"\"Get current temperature without GRL (for logging/display).\"\"\"\n        with torch.no_grad():\n            tau = self.tau_min + self.tau_range * torch.sigmoid(self.raw_temp)\n            return tau.item()\n    \n    def get_raw_value(self) -> float:\n        \"\"\"Get raw (unbounded) parameter value (for debugging).\"\"\"\n        return self.raw_temp.item()\n\n\n# -----------------------------------------------------------------------------\n# Legacy Classes (kept for backward compatibility)\n# -----------------------------------------------------------------------------\nclass LearnableTemperature(nn.Module):\n    \"\"\"\n    DEPRECATED: Simple learnable temperature WITHOUT GRL.\n    Kept for backward compatibility. Use CTKDTemperature instead.\n    \n    WARNING: This class caused temperature runaway in v12!\n    \"\"\"\n    \n    def __init__(self, init: float = 2.0):\n        super().__init__()\n        self.log_temp = nn.Parameter(torch.log(torch.tensor(init)))\n    \n    def forward(self) -> torch.Tensor:\n        return torch.exp(self.log_temp).clamp(1.0, 10.0)\n    \n    def get_temperature(self) -> float:\n        return self.forward().item()\n\n\nclass ChannelWiseTernarySpike(nn.Module):\n    \"\"\"\n    Per-channel learnable alpha and amplitude for ternary spikes.\n    DISABLED in v13 due to structural symmetry issue with RWKV.\n    \"\"\"\n    \n    def __init__(self, d_model: int, alpha_init: float = 1.0):\n        super().__init__()\n        self.d_model = d_model\n        self.alpha = nn.Parameter(torch.ones(d_model) * alpha_init)\n        self.amplitude = nn.Parameter(torch.ones(d_model))\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x_abs_mean = x.abs().mean(dim=(0, 1), keepdim=True)\n        threshold = self.alpha * x_abs_mean\n        threshold = threshold.clamp(min=0.01, max=10.0)\n        \n        with torch.no_grad():\n            pos_mask = (x > threshold).float()\n            neg_mask = (x < -threshold).float()\n            spike_signs = pos_mask - neg_mask\n        \n        spikes = self.amplitude * spike_signs\n        return spikes + (x - x.detach())\n    \n    def get_amplitude(self) -> float:\n        return self.amplitude.mean().item()\n    \n    def get_stats(self) -> dict:\n        return {\n            'alpha_mean': self.alpha.mean().item(),\n            'alpha_std': self.alpha.std().item(),\n            'amplitude_mean': self.amplitude.mean().item(),\n            'amplitude_std': self.amplitude.std().item(),\n        }\n\n    def get_amplitude_stats(self) -> dict:\n        return {\n            'mean': self.amplitude.mean().item(),\n            'std': self.amplitude.std().item(),\n            'min': self.amplitude.min().item(),\n            'max': self.amplitude.max().item(),\n        }\n\n\nclass TrainableTernarySpike(nn.Module):\n    \"\"\"Original trainable ternary spike with scalar amplitude (from v8).\"\"\"\n\n    def __init__(self, alpha: float = 1.0):\n        super().__init__()\n        self.alpha = alpha\n        self.amplitude = nn.Parameter(torch.ones(1))\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        threshold = self.alpha * x.abs().mean(dim=-1, keepdim=True)\n        threshold = threshold.clamp(min=0.01, max=10.0)\n\n        with torch.no_grad():\n            pos_mask = (x > threshold).float()\n            neg_mask = (x < -threshold).float()\n            spike_signs = pos_mask - neg_mask\n\n        spikes = self.amplitude * spike_signs\n        return spikes + (x - x.detach())\n\n    def get_amplitude(self) -> float:\n        return self.amplitude.item()\n\n\ndef get_stage_params(step: int, total_steps: int = 3000) -> dict:\n    \"\"\"Progressive training stages (POCL) - kept for infrastructure.\"\"\"\n    if step < total_steps * 0.4:\n        return {'stage': 1, 'temp_target': 1.0, 'align_mult': 0.0, 'alpha': 0.9}\n    elif step < total_steps * 0.7:\n        return {'stage': 2, 'temp_target': 1.5, 'align_mult': 0.5, 'alpha': 0.7}\n    else:\n        return {'stage': 3, 'temp_target': 2.0, 'align_mult': 1.0, 'alpha': 0.5}\n\n\n# -----------------------------------------------------------------------------\n# Unit Tests for CTKD Components\n# -----------------------------------------------------------------------------\nprint(\"=\"*60)\nprint(\"v13 CTKD Component Tests\")\nprint(\"=\"*60)\n\n# Test 1: GRL Gradient Reversal\nprint(\"\\n[1] GRL Gradient Reversal Test\")\ngrl = GradientReversalLayer()\ngrl.set_lambda(1.0)\nx_test = torch.tensor([2.0], requires_grad=True)\ny_test = grl(x_test)\nloss_test = y_test.sum()\nloss_test.backward()\nexpected_grad = -1.0  # GRL should negate: 1 * -1.0 = -1.0\nactual_grad = x_test.grad.item()\ngrl_pass = abs(actual_grad - expected_grad) < 1e-6\nprint(f\"  Input grad without GRL would be: +1.0\")\nprint(f\"  Input grad with GRL (\u03bb=1.0): {actual_grad:.4f}\")\nprint(f\"  Expected: {expected_grad:.4f}\")\nprint(f\"  {'PASS' if grl_pass else 'FAIL'}\")\ndel x_test, y_test, loss_test\n\n# Test 2: Lambda Schedule\nprint(\"\\n[2] Lambda Schedule Test\")\ntotal = 3000\nwarmup = 0.2\n# During warmup\nlambda_0 = get_lambda(0, total, warmup_ratio=warmup)\nlambda_500 = get_lambda(500, total, warmup_ratio=warmup)\n# After warmup\nlambda_1500 = get_lambda(1500, total, warmup_ratio=warmup)\nlambda_2999 = get_lambda(2999, total, warmup_ratio=warmup)\n\nwarmup_pass = lambda_0 == 0.0 and lambda_500 == 0.0\nincrease_pass = 0 < lambda_1500 < lambda_2999 <= 1.0\nlambda_pass = warmup_pass and increase_pass\nprint(f\"  \u03bb(0) = {lambda_0:.4f} (should be 0.0)\")\nprint(f\"  \u03bb(500) = {lambda_500:.4f} (should be 0.0, still in warmup)\")\nprint(f\"  \u03bb(1500) = {lambda_1500:.4f} (should be > 0)\")\nprint(f\"  \u03bb(2999) = {lambda_2999:.4f} (should be \u2248 1.0)\")\nprint(f\"  {'PASS' if lambda_pass else 'FAIL'}\")\n\n# Test 3: Temperature Bounds\nprint(\"\\n[3] Temperature Bounds Test\")\ntemp_module = CTKDTemperature(tau_min=1.0, tau_max=5.0, init=2.0).to(DEVICE)\ninit_temp = temp_module.get_temperature()\n\n# Force extreme raw values\nwith torch.no_grad():\n    temp_module.raw_temp.fill_(-100)\n    tau_low = temp_module.get_temperature()\n    \n    temp_module.raw_temp.fill_(100)\n    tau_high = temp_module.get_temperature()\n    \n    # Reset to init\n    init_normalized = (2.0 - 1.0) / 4.0\n    init_raw = math.log(init_normalized / (1 - init_normalized))\n    temp_module.raw_temp.fill_(init_raw)\n\nbounds_pass = (1.0 <= tau_low <= 1.01) and (4.99 <= tau_high <= 5.0) and (1.9 <= init_temp <= 2.1)\nprint(f\"  Initial temp: {init_temp:.4f} (should be \u2248 2.0)\")\nprint(f\"  Min bound test: {tau_low:.4f} (should be \u2248 1.0)\")\nprint(f\"  Max bound test: {tau_high:.4f} (should be \u2248 5.0)\")\nprint(f\"  {'PASS' if bounds_pass else 'FAIL'}\")\n\n# Test 4: End-to-End Gradient Flow\nprint(\"\\n[4] End-to-End Gradient Flow Test\")\ntemp_module_test = CTKDTemperature(tau_min=1.0, tau_max=5.0, init=2.0).to(DEVICE)\nlambda_test = 0.5\n\n# Simulate forward pass\nT = temp_module_test(lambda_test)\nfake_kl_loss = T * 2.0  # Gradient \u2202L/\u2202T = 2.0\n\n# Without GRL: optimizer would DECREASE T to minimize loss\n# With GRL: optimizer should INCREASE T (because grad is reversed)\nfake_kl_loss.backward()\n\nraw_grad = temp_module_test.raw_temp.grad.item()\n# The gradient through sigmoid and GRL should be negative (reversed)\n# Original: \u2202L/\u2202raw > 0 would decrease raw\n# With GRL: \u2202L/\u2202raw < 0 (negated), so optimizer increases raw\ngrad_flow_pass = raw_grad < 0  # Should be negative due to GRL\nprint(f\"  Loss = T * 2.0, so \u2202L/\u2202T = 2.0 (positive)\")\nprint(f\"  Without GRL: raw_grad would be positive (decrease T)\")\nprint(f\"  With GRL (\u03bb=0.5): raw_grad = {raw_grad:.4f} (should be negative)\")\nprint(f\"  {'PASS' if grad_flow_pass else 'FAIL'}\")\ndel temp_module_test\n\n# Summary\nprint(\"\\n\" + \"=\"*60)\nall_pass = grl_pass and lambda_pass and bounds_pass and grad_flow_pass\nprint(f\"CTKD Component Tests: {'ALL PASS' if all_pass else 'SOME FAILED'}\")\nif not all_pass:\n    print(\"WARNING: Fix failing tests before running training!\")\nprint(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n# cell 7: v13 POCL (Progressive Overload Curriculum Learning)\n# =============================================================================\n# Reference: \"POCL: Progressive Overload Curriculum Learning\" (2025)\n# arXiv:2506.05695\n\n# -----------------------------------------------------------------------------\n# Sample Difficulty Scoring\n# -----------------------------------------------------------------------------\ndef compute_sample_difficulty(student, teacher, dataloader, device, max_batches=50):\n    \"\"\"\n    Compute difficulty scores for each sample using student-teacher divergence.\n\n    Difficulty = average (CE loss + KL divergence) per sample.\n    Higher score = harder sample for the student.\n\n    Uses a small pre-trained student to get meaningful gradients.\n\n    Args:\n        student: Student model (should be briefly pre-trained)\n        teacher: Teacher model (frozen)\n        dataloader: Training data loader\n        device: Compute device\n        max_batches: Limit batches for efficiency\n\n    Returns:\n        Dict with sample indices and difficulty scores\n    \"\"\"\n    student.eval()\n    teacher.eval()\n\n    all_difficulties = []\n    all_indices = []\n    sample_idx = 0\n\n    with torch.no_grad():\n        for batch_idx, batch in enumerate(dataloader):\n            if batch_idx >= max_batches:\n                break\n\n            ids = batch[0].to(device, non_blocking=True)\n            batch_size = ids.size(0)\n\n            # Get logits\n            s_logits = student(ids)\n            t_logits = teacher(ids).logits\n\n            # Per-sample difficulty (average over sequence)\n            # 1. Cross-entropy with teacher as target\n            s_probs = F.softmax(s_logits, dim=-1)\n            t_probs = F.softmax(t_logits, dim=-1)\n\n            # KL divergence per sample\n            kl_div = F.kl_div(\n                F.log_softmax(s_logits, dim=-1),\n                t_probs,\n                reduction='none'\n            ).sum(dim=-1).mean(dim=-1)  # [batch_size]\n\n            # Cross-entropy per sample (using teacher hard targets)\n            t_tokens = t_logits.argmax(dim=-1)\n            ce_loss = F.cross_entropy(\n                s_logits.view(-1, s_logits.size(-1)),\n                t_tokens.view(-1),\n                reduction='none'\n            ).view(batch_size, -1).mean(dim=-1)  # [batch_size]\n\n            # Combined difficulty\n            difficulty = kl_div + ce_loss  # [batch_size]\n\n            all_difficulties.extend(difficulty.cpu().tolist())\n            all_indices.extend(range(sample_idx, sample_idx + batch_size))\n            sample_idx += batch_size\n\n            if batch_idx % 10 == 0:\n                print(f\"  Scoring batch {batch_idx+1}/{max_batches}...\")\n\n    student.train()\n\n    return {\n        'indices': all_indices,\n        'difficulties': all_difficulties,\n        'num_samples': len(all_indices)\n    }\n\n\n# -----------------------------------------------------------------------------\n# Data Partitioning by Difficulty\n# -----------------------------------------------------------------------------\ndef partition_by_difficulty(difficulties_dict, n_stages=3):\n    \"\"\"\n    Partition data into stages by difficulty (easy -> hard).\n\n    Stage 1: Easiest 33%\n    Stage 2: Easiest 66% (includes stage 1)\n    Stage 3: All 100% (includes stages 1+2)\n\n    Args:\n        difficulties_dict: Output from compute_sample_difficulty()\n        n_stages: Number of stages (default 3)\n\n    Returns:\n        List of index lists, one per stage (cumulative)\n    \"\"\"\n    indices = difficulties_dict['indices']\n    difficulties = difficulties_dict['difficulties']\n\n    # Sort by difficulty (ascending = easy first)\n    sorted_pairs = sorted(zip(indices, difficulties), key=lambda x: x[1])\n    sorted_indices = [idx for idx, _ in sorted_pairs]\n\n    n = len(sorted_indices)\n    stage_indices = []\n\n    for stage in range(n_stages):\n        # Cumulative: stage 1 = 33%, stage 2 = 66%, stage 3 = 100%\n        end_idx = int(n * (stage + 1) / n_stages)\n        stage_indices.append(sorted_indices[:end_idx])\n\n    return stage_indices\n\n\n# -----------------------------------------------------------------------------\n# Brief Pre-training for Difficulty Scoring\n# -----------------------------------------------------------------------------\ndef pretrain_for_difficulty_scoring(student, teacher, train_loader, cfg, device, steps=100):\n    \"\"\"\n    Brief pre-training so difficulty scores are meaningful.\n\n    Without pre-training, student predictions are random garbage,\n    making all samples appear equally difficult.\n\n    Args:\n        student: Student model\n        teacher: Teacher model (frozen)\n        train_loader: Training data loader\n        cfg: Config object\n        device: Compute device\n        steps: Number of pre-training steps\n\n    Returns:\n        Student model (modified in-place)\n    \"\"\"\n    print(f\"Pre-training student for {steps} steps (for difficulty scoring)...\")\n\n    optimizer = torch.optim.AdamW(student.parameters(), lr=cfg.distill_lr, weight_decay=0.01)\n    scaler = torch.cuda.amp.GradScaler()\n\n    student.train()\n    teacher.eval()\n\n    step = 0\n    pbar = tqdm(total=steps, desc='Pre-training')\n\n    for batch in train_loader:\n        if step >= steps:\n            break\n\n        ids = batch[0].to(device, non_blocking=True)\n\n        with torch.cuda.amp.autocast():\n            with torch.no_grad():\n                t_logits = teacher(ids).logits\n\n            s_logits = student(ids)\n\n            # Simple KL loss (no temperature complexity)\n            T = 2.0\n            s_log = F.log_softmax(s_logits / T, dim=-1)\n            t_prob = F.softmax(t_logits / T, dim=-1)\n            loss = F.kl_div(\n                s_log.view(-1, s_logits.size(-1)),\n                t_prob.view(-1, t_logits.size(-1)),\n                reduction='batchmean'\n            ) * (T ** 2)\n\n        optimizer.zero_grad()\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n\n        step += 1\n        pbar.update(1)\n        if step % 20 == 0:\n            pbar.set_postfix(loss=f\"{loss.item():.3f}\")\n\n    pbar.close()\n    print(f\"Pre-training complete. Final loss: {loss.item():.3f}\")\n\n    return student\n\n\n# -----------------------------------------------------------------------------\n# Get Stage Temperature (Fixed Schedule)\n# -----------------------------------------------------------------------------\ndef get_pocl_temperature(step, total_steps, temp_schedule, n_stages=3):\n    \"\"\"\n    Get temperature for current POCL stage.\n\n    Args:\n        step: Current training step\n        total_steps: Total training steps\n        temp_schedule: Tuple of temperatures per stage (e.g., (1.0, 1.5, 2.0))\n        n_stages: Number of stages\n\n    Returns:\n        Temperature for current stage\n    \"\"\"\n    current_stage = get_pocl_stage(step, total_steps, n_stages)\n    return temp_schedule[current_stage]\n\n\ndef get_pocl_stage(step, total_steps, n_stages=3):\n    \"\"\"\n    Get current POCL stage (0-indexed).\n\n    Uses rounded boundaries to ensure even distribution:\n    - 5000 steps, 3 stages: boundaries at 1667, 3333\n    - Stage 0: steps 0-1666\n    - Stage 1: steps 1667-3332\n    - Stage 2: steps 3333-4999\n    \"\"\"\n    for i in range(n_stages - 1):\n        boundary = round((i + 1) * total_steps / n_stages)\n        if step < boundary:\n            return i\n    return n_stages - 1\n\n\n# -----------------------------------------------------------------------------\n# POCL Unit Tests\n# -----------------------------------------------------------------------------\nprint(\"=\"*60)\nprint(\"v13 POCL Component Tests\")\nprint(\"=\"*60)\n\n# Test 1: Temperature Schedule\nprint(\"\\n[1] Temperature Schedule Test\")\ntemp_schedule = (1.0, 1.5, 2.0)\ntotal = 5000\n\nt_start = get_pocl_temperature(0, total, temp_schedule)\nt_stage1_end = get_pocl_temperature(1666, total, temp_schedule)  # End of stage 1\nt_stage2_start = get_pocl_temperature(1667, total, temp_schedule)  # Start of stage 2\nt_stage2_end = get_pocl_temperature(3332, total, temp_schedule)\nt_stage3 = get_pocl_temperature(4000, total, temp_schedule)\n\ntemp_pass = (t_start == 1.0 and t_stage1_end == 1.0 and t_stage2_start == 1.5 and t_stage3 == 2.0)\nprint(f\"  T(0) = {t_start} (should be 1.0)\")\nprint(f\"  T(1666) = {t_stage1_end} (should be 1.0, end of stage 1)\")\nprint(f\"  T(1667) = {t_stage2_start} (should be 1.5, start of stage 2)\")\nprint(f\"  T(4000) = {t_stage3} (should be 2.0, stage 3)\")\nprint(f\"  {'PASS' if temp_pass else 'FAIL'}\")\n\n# Test 2: Stage Boundaries\nprint(\"\\n[2] Stage Boundaries Test\")\nstages = [get_pocl_stage(s, total) for s in [0, 1666, 1667, 3332, 3333, 4999]]\nstage_pass = stages == [0, 0, 1, 1, 2, 2]\nprint(f\"  Stages at [0, 1666, 1667, 3332, 3333, 4999]: {stages}\")\nprint(f\"  Expected: [0, 0, 1, 1, 2, 2]\")\nprint(f\"  {'PASS' if stage_pass else 'FAIL'}\")\n\n# Test 3: Partition by Difficulty (mock)\nprint(\"\\n[3] Partition by Difficulty Test (mock data)\")\nmock_difficulties = {\n    'indices': list(range(9)),\n    'difficulties': [0.5, 1.5, 0.3, 2.0, 0.8, 1.2, 2.5, 0.1, 1.8],  # Easy: 7,2,0,4 | Med: 5,1 | Hard: 8,3,6\n    'num_samples': 9\n}\npartitions = partition_by_difficulty(mock_difficulties, n_stages=3)\n# After sorting: [7(0.1), 2(0.3), 0(0.5), 4(0.8), 5(1.2), 1(1.5), 8(1.8), 3(2.0), 6(2.5)]\n# Stage 1 (33%): indices [7, 2, 0] -> 3 samples\n# Stage 2 (66%): indices [7, 2, 0, 4, 5, 1] -> 6 samples\n# Stage 3 (100%): all 9 samples\npartition_pass = (len(partitions[0]) == 3 and len(partitions[1]) == 6 and len(partitions[2]) == 9)\nprint(f\"  Stage 1 samples: {len(partitions[0])} (should be 3)\")\nprint(f\"  Stage 2 samples: {len(partitions[1])} (should be 6)\")\nprint(f\"  Stage 3 samples: {len(partitions[2])} (should be 9)\")\nprint(f\"  Cumulative check: {partitions[0][0] in partitions[1] and partitions[1][0] in partitions[2]}\")\nprint(f\"  {'PASS' if partition_pass else 'FAIL'}\")\n\n# Summary\nprint(\"\\n\" + \"=\"*60)\nall_pass = temp_pass and stage_pass and partition_pass\nprint(f\"POCL Component Tests: {'ALL PASS' if all_pass else 'SOME FAILED'}\")\nif not all_pass:\n    print(\"WARNING: Fix failing tests before running training!\")\nprint(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# cell 7: hardware and spike stats collectors (same as v9)\n",
    "# =============================================================================\n",
    "class HardwareStatsCollector:\n",
    "    \"\"\"collect gpu memory, timing, and throughput metrics.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.gpu_memory_history = []\n",
    "        self.step_times = []\n",
    "        self.tokens_processed = 0\n",
    "        self.start_time = None\n",
    "\n",
    "    def start(self):\n",
    "        self.start_time = time.time()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "    def record_step(self, batch_size: int, seq_len: int):\n",
    "        if torch.cuda.is_available():\n",
    "            self.gpu_memory_history.append(torch.cuda.memory_allocated() / 1e9)\n",
    "        self.tokens_processed += batch_size * seq_len\n",
    "        self.step_times.append(time.time())\n",
    "\n",
    "    def get_throughput(self) -> float:\n",
    "        if len(self.step_times) < 2:\n",
    "            return 0.0\n",
    "        elapsed = self.step_times[-1] - self.step_times[0]\n",
    "        return self.tokens_processed / elapsed if elapsed > 0 else 0.0\n",
    "\n",
    "    def get_summary(self) -> Dict[str, Any]:\n",
    "        elapsed = time.time() - self.start_time if self.start_time else 0\n",
    "        return {\n",
    "            'peak_gpu_memory_gb': max(self.gpu_memory_history) if self.gpu_memory_history else 0,\n",
    "            'avg_gpu_memory_gb': float(np.mean(self.gpu_memory_history)) if self.gpu_memory_history else 0,\n",
    "            'total_training_time_s': elapsed,\n",
    "            'total_training_time_min': elapsed / 60,\n",
    "            'tokens_processed': self.tokens_processed,\n",
    "            'throughput_tokens_per_sec': self.get_throughput(),\n",
    "        }\n",
    "\n",
    "\n",
    "class SpikeStatsCollector:\n",
    "    \"\"\"collect per-layer spike density and amplitude evolution.\"\"\"\n",
    "\n",
    "    def __init__(self, n_layers: int):\n",
    "        self.n_layers = n_layers\n",
    "        self.density_history = {i: {'k': [], 'v': []} for i in range(n_layers)}\n",
    "        self.amplitude_history = {i: {'k': [], 'v': []} for i in range(n_layers)}\n",
    "        self.step_densities = []\n",
    "\n",
    "    def record(self, student, step: int):\n",
    "        stats = student.get_spike_stats()\n",
    "        all_densities = []\n",
    "        for i in range(self.n_layers):\n",
    "            layer_key = f'layer_{i}'\n",
    "            if layer_key in stats:\n",
    "                k_density = stats[layer_key].get('k', 0)\n",
    "                v_density = stats[layer_key].get('v', 0)\n",
    "                k_amp = stats[layer_key].get('k_amp', 1.0)\n",
    "                v_amp = stats[layer_key].get('v_amp', 1.0)\n",
    "\n",
    "                self.density_history[i]['k'].append(k_density)\n",
    "                self.density_history[i]['v'].append(v_density)\n",
    "                self.amplitude_history[i]['k'].append(k_amp)\n",
    "                self.amplitude_history[i]['v'].append(v_amp)\n",
    "                all_densities.extend([k_density, v_density])\n",
    "\n",
    "        if all_densities:\n",
    "            self.step_densities.append({'step': step, 'density': float(np.mean(all_densities))})\n",
    "\n",
    "    def get_summary(self) -> Dict[str, Any]:\n",
    "        per_layer = {}\n",
    "        all_k, all_v = [], []\n",
    "        all_k_amp, all_v_amp = [], []\n",
    "\n",
    "        for i in range(self.n_layers):\n",
    "            k_vals = self.density_history[i]['k']\n",
    "            v_vals = self.density_history[i]['v']\n",
    "            k_amps = self.amplitude_history[i]['k']\n",
    "            v_amps = self.amplitude_history[i]['v']\n",
    "\n",
    "            per_layer[f'layer_{i}'] = {\n",
    "                'k_mean': float(np.mean(k_vals)) if k_vals else 0,\n",
    "                'k_std': float(np.std(k_vals)) if k_vals else 0,\n",
    "                'k_final': float(k_vals[-1]) if k_vals else 0,\n",
    "                'v_mean': float(np.mean(v_vals)) if v_vals else 0,\n",
    "                'v_std': float(np.std(v_vals)) if v_vals else 0,\n",
    "                'v_final': float(v_vals[-1]) if v_vals else 0,\n",
    "                'k_amp_final': float(k_amps[-1]) if k_amps else 1.0,\n",
    "                'v_amp_final': float(v_amps[-1]) if v_amps else 1.0,\n",
    "            }\n",
    "            all_k.extend(k_vals)\n",
    "            all_v.extend(v_vals)\n",
    "            if k_amps: all_k_amp.append(k_amps[-1])\n",
    "            if v_amps: all_v_amp.append(v_amps[-1])\n",
    "\n",
    "        return {\n",
    "            'per_layer': per_layer,\n",
    "            'overall_k_density': float(np.mean(all_k)) if all_k else 0,\n",
    "            'overall_v_density': float(np.mean(all_v)) if all_v else 0,\n",
    "            'overall_density': float(np.mean(all_k + all_v)) if (all_k or all_v) else 0,\n",
    "            'amplitudes': {'k': all_k_amp, 'v': all_v_amp},\n",
    "            'density_history': self.step_densities,\n",
    "        }\n",
    "\n",
    "print(\"collectors defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# cell 8: spiking goose model (v11 - channel-wise spikes + gradient checkpointing)\n# =============================================================================\nclass SpikingGooseRecurrentLayer(nn.Module):\n    \"\"\"\n    RWKV-style recurrence with trainable ternary spiking.\n    \n    v11: Supports channel-wise ternary spikes (when use_channel_wise=True)\n    \"\"\"\n\n    def __init__(self, d_model, layer_idx=0, n_layers=4, spike_alpha=1.0, \n                 use_channel_wise: bool = False):\n        super().__init__()\n        self.d_model = d_model\n        self.layer_idx = layer_idx\n        self.use_channel_wise = use_channel_wise\n        self.ln = nn.LayerNorm(d_model)\n\n        ratio = layer_idx / max(n_layers - 1, 1)\n        self.time_mix_k = nn.Parameter(torch.ones(d_model) * (1 - ratio))\n        self.time_mix_v = nn.Parameter(torch.ones(d_model) * (1 - ratio))\n        self.time_mix_r = nn.Parameter(torch.ones(d_model) * (1 - ratio))\n        self.decay_weight = nn.Parameter(torch.zeros(d_model) - 0.5)\n\n        self.key_proj = nn.Linear(d_model, d_model, bias=False)\n        self.value_proj = nn.Linear(d_model, d_model, bias=False)\n        self.receptance_proj = nn.Linear(d_model, d_model, bias=False)\n        self.output_proj = nn.Linear(d_model, d_model, bias=False)\n\n        # v11: Use channel-wise spikes if enabled\n        if use_channel_wise:\n            self.k_spike = ChannelWiseTernarySpike(d_model, alpha_init=spike_alpha)\n            self.v_spike = ChannelWiseTernarySpike(d_model, alpha_init=spike_alpha)\n        else:\n            self.k_spike = TrainableTernarySpike(alpha=spike_alpha)\n            self.v_spike = TrainableTernarySpike(alpha=spike_alpha)\n\n        self.register_buffer('running_k_density', torch.tensor(0.0))\n        self.register_buffer('running_v_density', torch.tensor(0.0))\n        self._init_weights()\n\n    def _init_weights(self):\n        std = 0.1 / math.sqrt(self.d_model)\n        for m in [self.key_proj, self.value_proj, self.receptance_proj, self.output_proj]:\n            nn.init.normal_(m.weight, std=std)\n\n    def forward(self, x):\n        B, T, D = x.shape\n        x_norm = self.ln(x)\n        prev_x = F.pad(x_norm[:, :-1, :], (0, 0, 1, 0))\n\n        xk = x_norm * self.time_mix_k + prev_x * (1 - self.time_mix_k)\n        xv = x_norm * self.time_mix_v + prev_x * (1 - self.time_mix_v)\n        xr = x_norm * self.time_mix_r + prev_x * (1 - self.time_mix_r)\n\n        k_pre = self.key_proj(xk)\n        v_pre = self.value_proj(xv)\n\n        k = self.k_spike(k_pre)\n        v = self.v_spike(v_pre)\n        r = torch.sigmoid(self.receptance_proj(xr))\n\n        kv = k * v\n        decay = torch.sigmoid(self.decay_weight)\n        t_idx = torch.arange(T, device=x.device, dtype=x.dtype)\n        decay_powers = decay.unsqueeze(0) ** t_idx.unsqueeze(1)\n\n        kv_weighted = kv / (decay_powers.unsqueeze(0) + 1e-8)\n        S = torch.cumsum(kv_weighted, dim=1) * decay_powers.unsqueeze(0)\n\n        if self.training:\n            with torch.no_grad():\n                self.running_k_density = 0.99 * self.running_k_density + 0.01 * (k != 0).float().mean()\n                self.running_v_density = 0.99 * self.running_v_density + 0.01 * (v != 0).float().mean()\n\n        return x + r * self.output_proj(S)\n\n    def get_spike_density(self):\n        return {\n            'k': self.running_k_density.item(),\n            'v': self.running_v_density.item(),\n            'k_amp': self.k_spike.get_amplitude(),\n            'v_amp': self.v_spike.get_amplitude(),\n        }\n    \n    def get_channel_wise_stats(self) -> dict:\n        \"\"\"Get channel-wise spike statistics (only available if use_channel_wise=True).\"\"\"\n        if self.use_channel_wise:\n            return {\n                'k': self.k_spike.get_stats(),\n                'v': self.v_spike.get_stats(),\n            }\n        return None\n\n\nclass GooseFFN(nn.Module):\n    def __init__(self, d_model, expand=4):\n        super().__init__()\n        self.ln = nn.LayerNorm(d_model)\n        self.w1 = nn.Linear(d_model, d_model * expand, bias=False)\n        self.w2 = nn.Linear(d_model * expand, d_model, bias=False)\n\n    def forward(self, x):\n        return x + self.w2(F.silu(self.w1(self.ln(x))))\n\n\nclass StudentSpikingGoose(nn.Module):\n    \"\"\"\n    Spiking student model with trainable ternary activations.\n    \n    v11: Supports channel-wise ternary spikes + gradient checkpointing.\n    \"\"\"\n\n    def __init__(self, cfg, use_checkpointing=True):\n        super().__init__()\n        self.cfg = cfg\n        self.use_checkpointing = use_checkpointing and USE_GRADIENT_CHECKPOINTING\n        \n        # v11: Check for channel-wise spikes flag\n        use_channel_wise = getattr(cfg, 'use_channel_wise_spikes', False)\n        \n        self.embed = nn.Embedding(cfg.vocab_size, cfg.d_model)\n        self.pos_embed = nn.Embedding(cfg.max_seq_len, cfg.d_model)\n\n        self.layers = nn.ModuleList([\n            nn.ModuleDict({\n                'rec': SpikingGooseRecurrentLayer(\n                    cfg.d_model, i, cfg.n_layers, cfg.spike_alpha,\n                    use_channel_wise=use_channel_wise\n                ),\n                'ffn': GooseFFN(cfg.d_model),\n            })\n            for i in range(cfg.n_layers)\n        ])\n\n        self.ln_out = nn.LayerNorm(cfg.d_model)\n        self.head = nn.Linear(cfg.d_model, cfg.vocab_size, bias=False)\n        self.head.weight = self.embed.weight\n\n        nn.init.normal_(self.embed.weight, std=0.02)\n        nn.init.normal_(self.pos_embed.weight, std=0.02)\n\n    def _layer_forward(self, layer, x):\n        \"\"\"helper for gradient checkpointing - processes one layer.\"\"\"\n        x = layer['rec'](x)\n        x = layer['ffn'](x)\n        return x\n\n    def forward(self, input_ids, return_hiddens=False):\n        \"\"\"forward pass with optional hidden state return for alignment.\"\"\"\n        B, T = input_ids.shape\n        pos = torch.arange(T, device=input_ids.device).unsqueeze(0)\n        x = self.embed(input_ids) + self.pos_embed(pos)\n\n        hiddens = [x] if return_hiddens else None\n\n        for layer in self.layers:\n            if self.use_checkpointing and self.training:\n                # v10: gradient checkpointing with use_reentrant=False (recommended)\n                x = checkpoint(self._layer_forward, layer, x, use_reentrant=False)\n            else:\n                x = self._layer_forward(layer, x)\n            \n            if return_hiddens:\n                hiddens.append(x)\n\n        logits = self.head(self.ln_out(x))\n\n        if return_hiddens:\n            return logits, hiddens\n        return logits\n\n    def get_spike_stats(self):\n        return {f'layer_{i}': layer['rec'].get_spike_density() for i, layer in enumerate(self.layers)}\n\n    def get_avg_spike_density(self):\n        densities = []\n        for layer in self.layers:\n            d = layer['rec'].get_spike_density()\n            densities.extend([d['k'], d['v']])\n        return float(np.mean(densities)) if densities else 0.0\n\n    def get_amplitudes(self):\n        return {f'layer_{i}': {'k': layer['rec'].k_spike.get_amplitude(), 'v': layer['rec'].v_spike.get_amplitude()}\n                for i, layer in enumerate(self.layers)}\n    \n    def get_channel_amplitude_variance(self) -> float:\n        \"\"\"Get total variance of channel-wise amplitudes (for regularization).\"\"\"\n        total_var = 0.0\n        for layer in self.layers:\n            rec = layer['rec']\n            if hasattr(rec.k_spike, 'amplitude') and rec.k_spike.amplitude.numel() > 1:\n                total_var += rec.k_spike.amplitude.var().item()\n                total_var += rec.v_spike.amplitude.var().item()\n        return total_var\n\nprint(\"student model defined (v11: channel-wise spikes + gradient checkpointing)\")\nprint(f\"  gradient checkpointing: {USE_GRADIENT_CHECKPOINTING}\")\nprint(f\"  channel-wise spikes: {config.use_channel_wise_spikes}\")\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n# cell 9: hidden-state alignment (v11 - 320d \u00d7 5L, but DISABLED)\n# =============================================================================\nclass HiddenStateProjector(nn.Module):\n    \"\"\"\n    Project student hidden states to teacher dimension for alignment.\n\n    student: (B, T, 512) -> (B, T, 768)  # v11: 512 dim (same as v11)\n\n    Maps 8 student layers to selected teacher layers.\n\n    NOTE: This is kept for infrastructure but DISABLED in v11 (weight=0.0).\n    \"\"\"\n\n    def __init__(self, student_dim: int, teacher_dim: int, n_student_layers: int):\n        super().__init__()\n        self.projectors = nn.ModuleList([\n            nn.Linear(student_dim, teacher_dim, bias=False)\n            for _ in range(n_student_layers)\n        ])\n        for proj in self.projectors:\n            nn.init.normal_(proj.weight, std=0.02)\n\n    def forward(self, student_hidden: torch.Tensor, layer_idx: int) -> torch.Tensor:\n        return self.projectors[layer_idx](student_hidden)\n\n\ndef compute_hidden_alignment_loss(\n    teacher_hiddens: List[torch.Tensor],\n    student_hiddens: List[torch.Tensor],\n    projector: HiddenStateProjector,\n    teacher_layers: int = 12,\n    student_layers: int = 8\n) -> torch.Tensor:\n    \"\"\"\n    Compute MSE loss between projected student and teacher hidden states.\n\n    v11: Uses 8 student layers, maps to every 1-2 teacher layers.\n    NOTE: Disabled in v11 (hidden_align_weight=0.0), kept for future experiments.\n    \"\"\"\n    # v11: Map 8 student layers to 8 teacher layers\n    # teacher_indices[i] = which teacher layer student layer i maps to\n    teacher_indices = [1, 2, 4, 5, 7, 8, 10, 11]  # v11 mapping (8 student layers)\n\n    total_loss = 0.0\n    for s_idx, t_idx in enumerate(teacher_indices):\n        if s_idx >= len(student_hiddens) - 1:  # student_hiddens includes embedding\n            break\n        if t_idx >= len(teacher_hiddens):\n            break\n\n        s_hidden = student_hiddens[s_idx + 1]  # +1 because [0] is embedding\n        t_hidden = teacher_hiddens[t_idx]\n\n        s_proj = projector(s_hidden, s_idx)\n        total_loss = total_loss + F.mse_loss(s_proj, t_hidden)\n\n    return total_loss / len(teacher_indices)\n\n\n# Create projector (even if disabled, keeps infrastructure)\nprojector = HiddenStateProjector(\n    student_dim=config.d_model,\n    teacher_dim=config.teacher_d_model,\n    n_student_layers=config.n_layers\n).to(DEVICE)\n\nprojector_params = sum(p.numel() for p in projector.parameters())\nprint(f\"hidden-state projector: {projector_params:,} params\")\nprint(f\"  student dim: {config.d_model} (v11: same as v11)\")\nprint(f\"  teacher dim: {config.teacher_d_model}\")\nprint(f\"  student layers: {config.n_layers}\")\nprint(f\"  hidden_align_weight: {config.hidden_align_weight}\")\nif config.hidden_align_weight == 0.0:\n    print(f\"  STATUS: DISABLED (keeping infrastructure for future experiments)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# cell 10: cosine lr with warmup (same as v9)\n",
    "# =============================================================================\n",
    "def get_cosine_schedule_with_warmup(\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    warmup_steps: int,\n",
    "    total_steps: int,\n",
    ") -> torch.optim.lr_scheduler.LambdaLR:\n",
    "    \"\"\"\n",
    "    linear warmup then cosine decay to 0.\n",
    "    \"\"\"\n",
    "    def lr_lambda(step: int) -> float:\n",
    "        if step < warmup_steps:\n",
    "            return step / max(warmup_steps, 1)\n",
    "        else:\n",
    "            progress = (step - warmup_steps) / max(total_steps - warmup_steps, 1)\n",
    "            return 0.5 * (1.0 + math.cos(math.pi * progress))\n",
    "\n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "\n",
    "print(f\"cosine lr: {config.warmup_steps} warmup, {config.distill_steps} total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n# cell 11: load gpt-2 teacher (same as v9)\n# =============================================================================\nfrom transformers import GPT2LMHeadModel, GPT2Tokenizer\n\nprint(\"loading gpt-2 teacher...\")\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\ntokenizer.pad_token = tokenizer.eos_token\n\nteacher = GPT2LMHeadModel.from_pretrained('gpt2').to(DEVICE)\nteacher.config.use_cache = False  # Disable KV caching (not needed for distillation)\n\n# Compile teacher for faster inference (PyTorch 2.0+)\ntry:\n    teacher = torch.compile(teacher, mode='reduce-overhead')\n    print('teacher compiled with torch.compile')\nexcept Exception as e:\n    print(f'torch.compile not available: {e}')\nteacher.eval()\nfor p in teacher.parameters():\n    p.requires_grad = False\n\nteacher_params = sum(p.numel() for p in teacher.parameters())\nprint(f\"teacher: gpt-2 ({teacher_params:,} params)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# cell 13: data loading (v11 - efficient DataLoader)\n",
    "# =============================================================================\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"loading wikitext-2...\")\n",
    "dataset = load_dataset('wikitext', 'wikitext-2-raw-v1')\n",
    "\n",
    "def pre_tokenize(texts, max_len):\n",
    "    all_tokens = []\n",
    "    for text in tqdm(texts, desc=\"tokenizing\", leave=False):\n",
    "        if text.strip():\n",
    "            all_tokens.extend(tokenizer.encode(text, max_length=max_len*2, truncation=True))\n",
    "    chunks = [all_tokens[i:i+max_len] for i in range(0, len(all_tokens)-max_len+1, max_len//2) if len(all_tokens[i:i+max_len]) == max_len]\n",
    "    print(f\"created {len(chunks)} sequences\")\n",
    "    return torch.tensor(chunks, dtype=torch.long)\n",
    "\n",
    "train_tokens = pre_tokenize(dataset['train']['text'], config.max_seq_len)\n",
    "val_tokens = pre_tokenize(dataset['validation']['text'], config.max_seq_len)\n",
    "\n",
    "# v11: efficient DataLoader with workers and prefetch\n",
    "# Note: num_workers=0 for Kaggle/Colab compatibility, but prefetch still helps\n",
    "dataloader_kwargs = {\n",
    "    'batch_size': config.batch_size,\n",
    "    'pin_memory': True,\n",
    "    'num_workers': 0 if IS_KAGGLE or IS_COLAB else 2,  # workers disabled on cloud platforms\n",
    "    'prefetch_factor': None if IS_KAGGLE or IS_COLAB else 2,\n",
    "    'persistent_workers': False if IS_KAGGLE or IS_COLAB else True,\n",
    "}\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(train_tokens), shuffle=True, **dataloader_kwargs)\n",
    "val_loader = DataLoader(TensorDataset(val_tokens), shuffle=False, **dataloader_kwargs)\n",
    "\n",
    "print(f\"train: {len(train_loader)} batches, val: {len(val_loader)} batches\")\n",
    "print(f\"DataLoader: num_workers={dataloader_kwargs['num_workers']}, pin_memory={dataloader_kwargs['pin_memory']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# cell 14: create student model and projector (v11 - with compile)\n",
    "# =============================================================================\n",
    "print(\"creating student model (v11 - v11 baseline + POCL)...\")\n",
    "\n",
    "student = StudentSpikingGoose(config, use_checkpointing=USE_GRADIENT_CHECKPOINTING).to(DEVICE)\n",
    "student_params = sum(p.numel() for p in student.parameters())\n",
    "\n",
    "# v11: create projector (even if not used, for infrastructure preservation)\n",
    "projector = HiddenStateProjector(\n",
    "    student_dim=config.d_model,\n",
    "    teacher_dim=config.teacher_d_model,\n",
    "    n_student_layers=config.n_layers\n",
    ").to(DEVICE)\n",
    "projector_params = sum(p.numel() for p in projector.parameters())\n",
    "\n",
    "compression_ratio = teacher_params / student_params\n",
    "\n",
    "print(f\"student: asnn-goose v11 ({student_params:,} params)\")\n",
    "print(f\"projector: ({projector_params:,} params)\")\n",
    "print(f\"compression ratio: {compression_ratio:.1f}x\")\n",
    "print(f\"\")\n",
    "print(f\"v11 architecture:\")\n",
    "print(f\"  d_model: {config.d_model}\")\n",
    "print(f\"  n_layers: {config.n_layers}\")\n",
    "print(f\"  params: ~{student_params // 1_000_000}M\")\n",
    "print(f\"\")\n",
    "\n",
    "# v11: compile model if available and enabled\n",
    "compile_success = False\n",
    "if USE_TORCH_COMPILE and TORCH_COMPILE_AVAILABLE:\n",
    "    try:\n",
    "        print(\"compiling student model with torch.compile...\")\n",
    "        # Use the compile() method as recommended by PyTorch docs\n",
    "        student = torch.compile(student, mode='reduce-overhead')\n",
    "        compile_success = True\n",
    "        print(\"compilation successful!\")\n",
    "    except Exception as e:\n",
    "        print(f\"torch.compile failed: {e}\")\n",
    "        print(\"continuing without compilation\")\n",
    "else:\n",
    "    print(f\"torch.compile skipped (USE_TORCH_COMPILE={USE_TORCH_COMPILE}, available={TORCH_COMPILE_AVAILABLE})\")\n",
    "\n",
    "print(f\"\")\n",
    "print(f\"speedups active:\")\n",
    "print(f\"  gradient checkpointing: {USE_GRADIENT_CHECKPOINTING}\")\n",
    "print(f\"  torch.compile: {compile_success}\")\n",
    "print(f\"  accumulation_steps: {config.accumulation_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n# cell 15: evaluation functions (same as v9)\n# =============================================================================\n@torch.no_grad()\ndef evaluate(model, loader, device, is_gpt2=False):\n    model.eval()\n    total_loss, total_tokens = 0, 0\n    with torch.inference_mode():\n      for batch in loader:\n        ids = batch[0].to(device, non_blocking=True)\n        with torch.cuda.amp.autocast():\n            logits = model(ids).logits if is_gpt2 else model(ids)\n        loss = F.cross_entropy(logits[:, :-1].reshape(-1, logits.size(-1)), ids[:, 1:].reshape(-1), reduction='sum')\n        total_loss += loss.item()\n        total_tokens += ids[:, 1:].numel()\n    return total_loss / total_tokens\n\ndef get_ppl(loss):\n    return math.exp(min(loss, 10))\n\nprint(\"evaluation functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n# cell 17: distillation training loop (v13.1 - CTKD + Extended Training)\n# =============================================================================\ndef distill_v13(teacher, student, projector, train_loader, val_loader, cfg, device,\n                hw_stats, spike_stats):\n    \"\"\"\n    v13.1 distillation with CTKD (proven) + Extended Training + Early Stopping.\n\n    Based on v12.1 success, with added:\n    - Extended training (5000 steps)\n    - Early stopping (patience=500)\n\n    References:\n    - CTKD: https://arxiv.org/abs/2211.16231\n    - GRL: https://arxiv.org/abs/1409.7495\n    \"\"\"\n    training_logs = {\n        'loss_history': [],\n        'kl_loss_history': [],\n        'align_loss_history': [],\n        'ppl_history': [],\n        'lr_history': [],\n        'temp_history': [],\n        'lambda_history': [],\n        'stage_history': [],\n        'stage_transitions': [],\n        'early_stopped': False,\n        'early_stop_step': None,\n    }\n\n    # =========================================================================\n    # v13.1: CTKD Temperature (same as v12.1)\n    # =========================================================================\n    if cfg.use_ctkd:\n        temp_module = CTKDTemperature(\n            tau_min=cfg.tau_min,\n            tau_max=cfg.tau_max,\n            init=cfg.tau_init\n        ).to(device)\n        print(f\"v13.1: CTKD with Gradient Reversal Layer (proven from v12.1)\")\n        print(f\"       Temperature bounds: [{cfg.tau_min}, {cfg.tau_max}]\")\n        print(f\"       Initial temp: {cfg.tau_init}\")\n        print(f\"       Lambda warmup: {cfg.lambda_warmup_ratio*100:.0f}%\")\n    else:\n        temp_module = None\n        print(f\"Using fixed temperature: {cfg.temperature}\")\n\n    # =========================================================================\n    # v13.1: Early Stopping Setup\n    # =========================================================================\n    best_ppl = float('inf')\n    best_step = 0\n    no_improve_steps = 0\n\n    if cfg.use_early_stopping:\n        print(f\"\")\n        print(f\"v13.1: Early Stopping\")\n        print(f\"       Patience: {cfg.early_stopping_patience} steps\")\n        print(f\"       Min delta: {cfg.min_ppl_delta} PPL\")\n\n    # =========================================================================\n    # Setup optimizer\n    # =========================================================================\n    param_groups = [\n        {'params': list(student.parameters()), 'lr': cfg.distill_lr}\n    ]\n\n    if cfg.hidden_align_weight > 0:\n        param_groups.append({'params': list(projector.parameters()), 'lr': cfg.distill_lr})\n\n    if temp_module is not None:\n        param_groups.append({'params': list(temp_module.parameters()), 'lr': cfg.distill_lr})\n\n    all_params = []\n    for group in param_groups:\n        all_params.extend(group['params'])\n\n    try:\n        optimizer = torch.optim.AdamW(param_groups, weight_decay=0.01, fused=True)\n        print(\"Using fused AdamW\")\n    except TypeError:\n        optimizer = torch.optim.AdamW(param_groups, weight_decay=0.01)\n\n    scheduler = get_cosine_schedule_with_warmup(optimizer, cfg.warmup_steps, cfg.distill_steps)\n    scaler = torch.cuda.amp.GradScaler()\n\n    hw_stats.start()\n    step = 0\n    accum_step = 0\n    current_stage = 1\n\n    accumulation_steps = cfg.accumulation_steps\n    effective_batch = cfg.batch_size * accumulation_steps\n    print(f\"Gradient accumulation: {accumulation_steps} (effective batch = {effective_batch})\")\n    print(f\"Extended training: {cfg.distill_steps} steps\")\n\n    pbar = tqdm(total=cfg.distill_steps, desc='distilling (v13.1 - CTKD+Extended)')\n\n    optimizer.zero_grad(set_to_none=True)\n\n    while step < cfg.distill_steps:\n        for batch in train_loader:\n            if step >= cfg.distill_steps:\n                break\n\n            # Check early stopping\n            if cfg.use_early_stopping and no_improve_steps >= cfg.early_stopping_patience:\n                print(f\"\\n  [Early Stopping] No improvement for {cfg.early_stopping_patience} steps\")\n                print(f\"     Best PPL: {best_ppl:.2f} at step {best_step}\")\n                training_logs['early_stopped'] = True\n                training_logs['early_stop_step'] = step\n                pbar.close()\n                return training_logs\n\n            ids = batch[0].to(device, non_blocking=True)\n\n            # Get lambda for CTKD\n            if cfg.use_ctkd:\n                current_lambda = get_lambda(\n                    step, cfg.distill_steps,\n                    lambda_max=cfg.lambda_max,\n                    warmup_ratio=cfg.lambda_warmup_ratio\n                )\n            else:\n                current_lambda = 0.0\n\n            with torch.cuda.amp.autocast():\n                # Teacher forward\n                with torch.no_grad():\n                    if cfg.hidden_align_weight > 0:\n                        t_out = teacher(ids, output_hidden_states=True)\n                        t_logits = t_out.logits\n                        t_hiddens = t_out.hidden_states\n                    else:\n                        t_logits = teacher(ids).logits\n\n                # Student forward\n                student.train()\n                if cfg.hidden_align_weight > 0:\n                    s_logits, s_hiddens = student(ids, return_hiddens=True)\n                else:\n                    s_logits = student(ids)\n\n                # Get temperature\n                if cfg.use_ctkd and temp_module is not None:\n                    T = temp_module(current_lambda)\n                elif temp_module is not None:\n                    T = temp_module()\n                else:\n                    T = cfg.temperature\n\n                # KL divergence loss with temperature\n                s_log = F.log_softmax(s_logits / T, dim=-1)\n                t_prob = F.softmax(t_logits / T, dim=-1)\n                kl_loss = F.kl_div(\n                    s_log.view(-1, s_logits.size(-1)),\n                    t_prob.view(-1, t_logits.size(-1)),\n                    reduction='batchmean'\n                ) * (T ** 2)\n\n                # Hidden alignment (usually disabled)\n                if cfg.hidden_align_weight > 0:\n                    align_loss = compute_hidden_alignment_loss(\n                        t_hiddens, s_hiddens, projector,\n                        teacher_layers=cfg.teacher_n_layers,\n                        student_layers=cfg.n_layers\n                    )\n                else:\n                    align_loss = torch.tensor(0.0, device=device)\n\n                loss = kl_loss + cfg.hidden_align_weight * align_loss\n                loss = loss / accumulation_steps\n\n            scaler.scale(loss).backward()\n            accum_step += 1\n\n            if accum_step % accumulation_steps == 0:\n                scaler.unscale_(optimizer)\n                gn = torch.nn.utils.clip_grad_norm_(all_params, cfg.max_grad_norm)\n\n                if torch.isfinite(gn):\n                    scaler.step(optimizer)\n                scaler.update()\n                scheduler.step()\n                optimizer.zero_grad(set_to_none=True)\n\n                hw_stats.record_step(ids.size(0) * accumulation_steps, ids.size(1))\n                spike_stats.record(student, step)\n\n                current_lr = optimizer.param_groups[0]['lr']\n                current_temp = temp_module.get_temperature() if temp_module is not None else cfg.temperature\n\n                # Log\n                training_logs['loss_history'].append({'step': step, 'loss': loss.item() * accumulation_steps})\n                training_logs['kl_loss_history'].append({'step': step, 'loss': kl_loss.item()})\n                training_logs['align_loss_history'].append({'step': step, 'loss': align_loss.item() if isinstance(align_loss, torch.Tensor) else align_loss})\n                training_logs['lr_history'].append({'step': step, 'lr': current_lr})\n                training_logs['temp_history'].append({'step': step, 'temperature': current_temp})\n                training_logs['lambda_history'].append({'step': step, 'lambda': current_lambda})\n                training_logs['stage_history'].append({'step': step, 'stage': 1})\n\n                pbar.set_postfix(\n                    loss=f\"{loss.item() * accumulation_steps:.3f}\",\n                    kl=f\"{kl_loss.item():.3f}\",\n                    T=f\"{current_temp:.2f}\",\n                    lam=f\"{current_lambda:.2f}\",\n                    lr=f\"{current_lr:.1e}\"\n                )\n                pbar.update(1)\n                step += 1\n\n                if step % cfg.eval_interval == 0:\n                    val_loss = evaluate(student, val_loader, device)\n                    val_ppl = get_ppl(val_loss)\n                    training_logs['ppl_history'].append({'step': step, 'ppl': val_ppl})\n\n                    amps = student.get_amplitudes()\n                    amp_str = ', '.join([f\"L{i}:{amps[f'layer_{i}']['k']:.2f}\" for i in range(min(4, cfg.n_layers))])\n\n                    # Early stopping check\n                    if val_ppl < best_ppl - cfg.min_ppl_delta:\n                        best_ppl = val_ppl\n                        best_step = step\n                        no_improve_steps = 0\n                        save_dict = {\n                            'student': student.state_dict(),\n                            'projector': projector.state_dict(),\n                            'step': step,\n                            'ppl': val_ppl,\n                        }\n                        if temp_module is not None:\n                            save_dict['temp_module'] = temp_module.state_dict()\n                        torch.save(save_dict, f'{OUTPUT_DIR}/checkpoints/v13_best.pt')\n                        improve_str = \" [NEW BEST]\"\n                    else:\n                        no_improve_steps += cfg.eval_interval\n                        improve_str = f\" (no improve: {no_improve_steps}/{cfg.early_stopping_patience})\"\n\n                    lambda_str = f\", \u03bb={current_lambda:.2f}\" if cfg.use_ctkd else \"\"\n                    print(f\"\\n  step {step}: ppl={val_ppl:.1f}, T={current_temp:.2f}{lambda_str}, amps=[{amp_str}...]{improve_str}\")\n\n    pbar.close()\n    return training_logs\n\nprint(\"distillation function defined (v13.1 - CTKD + Extended Training)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n# cell 18: run distillation (v13.1 - CTKD + Extended Training)\n# =============================================================================\nprint(\"=\"*60)\nprint(\"v13.1: CTKD + Extended Training\")\nprint(\"=\"*60)\nprint(f\"  Architecture: {config.d_model}d x {config.n_layers}L (~22M params)\")\nprint(f\"  Target: PPL < 420 (improve on v12.1's 445.61)\")\nprint(f\"\")\nprint(f\"v13.1 Configuration:\")\nprint(f\"  CTKD: {config.use_ctkd} (proven from v12.1)\")\nprint(f\"  Temperature bounds: [{config.tau_min}, {config.tau_max}]\")\nprint(f\"  Extended training: {config.distill_steps} steps\")\nprint(f\"  Early stopping: patience={config.early_stopping_patience}\")\nprint(f\"  POCL: {config.use_pocl} (disabled - caused regression)\")\nprint(\"\")\n\n# Instantiate collectors\nhw_stats = HardwareStatsCollector()\nspike_stats = SpikeStatsCollector(config.n_layers)\nprint(\"Initialized HardwareStatsCollector and SpikeStatsCollector\")\n\n# Run distillation (CTKD + Extended Training)\nprint(f\"\\nStarting distillation...\")\n\ndistill_logs = distill_v13(\n    teacher, student, projector,\n    train_loader, val_loader,\n    config, DEVICE,\n    hw_stats, spike_stats\n)\n\n# Report results\nprint(f\"\\n\\n\" + \"=\"*60)\nprint(\"v13.1 Distillation Complete!\")\nprint(\"=\"*60)\n\nif distill_logs['ppl_history']:\n    final_ppl = distill_logs['ppl_history'][-1]['ppl']\n    best_ppl_entry = min(distill_logs['ppl_history'], key=lambda x: x['ppl'])\n    print(f\"\\nFinal PPL: {final_ppl:.2f}\")\n    print(f\"Best PPL: {best_ppl_entry['ppl']:.2f} at step {best_ppl_entry['step']}\")\n\nif distill_logs['early_stopped']:\n    print(f\"\\nEarly stopped at step {distill_logs['early_stop_step']}\")\nelse:\n    print(f\"\\nCompleted all {config.distill_steps} steps\")\n\nif distill_logs['temp_history']:\n    temps = [h['temperature'] for h in distill_logs['temp_history']]\n    print(f\"\\nTemperature evolution:\")\n    print(f\"  Start: {temps[0]:.2f}\")\n    print(f\"  End: {temps[-1]:.2f}\")\n\nif distill_logs['lambda_history']:\n    lambdas = [h['lambda'] for h in distill_logs['lambda_history']]\n    print(f\"\\nLambda evolution:\")\n    print(f\"  Start: {lambdas[0]:.2f}\")\n    print(f\"  End: {lambdas[-1]:.2f}\")\n\nprint(f\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# cell 20: lora implementation (same as v9)\n",
    "# =============================================================================\n",
    "class LoRALinear(nn.Module):\n",
    "    \"\"\"lora adapter for linear layers.\"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, rank=8, alpha=16.0):\n",
    "        super().__init__()\n",
    "        self.scaling = alpha / rank\n",
    "        self.lora_A = nn.Parameter(torch.zeros(rank, in_features))\n",
    "        self.lora_B = nn.Parameter(torch.zeros(out_features, rank))\n",
    "        nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return (x @ self.lora_A.T @ self.lora_B.T) * self.scaling\n",
    "\n",
    "\n",
    "def apply_lora(model, rank=8, alpha=16.0, targets=['key_proj', 'value_proj']):\n",
    "    \"\"\"apply lora adapters to specified modules.\"\"\"\n",
    "    lora_modules = {}\n",
    "    for name, module in model.named_modules():\n",
    "        if any(t in name for t in targets) and isinstance(module, nn.Linear):\n",
    "            lora = LoRALinear(module.in_features, module.out_features, rank, alpha).to(next(module.parameters()).device)\n",
    "            lora_modules[name] = lora\n",
    "            orig_forward = module.forward\n",
    "            def make_forward(orig, lora_mod):\n",
    "                def forward(x):\n",
    "                    return orig(x) + lora_mod(x)\n",
    "                return forward\n",
    "            module.forward = make_forward(orig_forward, lora)\n",
    "    print(f\"lora: {len(lora_modules)} modules, rank={rank}\")\n",
    "    return lora_modules\n",
    "\n",
    "print(\"lora defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# cell 21: ttt with lora (same as v9)\n",
    "# =============================================================================\n",
    "print(\"=\"*60)\n",
    "print(\"phase 2: test-time training with lora\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for p in student.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "lora_modules = apply_lora(student, config.lora_rank, config.lora_alpha)\n",
    "lora_params = sum(p.numel() for m in lora_modules.values() for p in m.parameters())\n",
    "\n",
    "pre_ttt_loss = evaluate(student, val_loader, DEVICE)\n",
    "pre_ttt_ppl = get_ppl(pre_ttt_loss)\n",
    "print(f\"\\npre-ttt ppl: {pre_ttt_ppl:.2f}\")\n",
    "\n",
    "lora_opt = torch.optim.AdamW([p for m in lora_modules.values() for p in m.parameters()], lr=config.ttt_lr)\n",
    "ttt_logs = {'loss_history': []}\n",
    "student.train()\n",
    "\n",
    "for step, batch in enumerate(val_loader):\n",
    "    if step >= config.ttt_steps:\n",
    "        break\n",
    "    ids = batch[0].to(DEVICE)\n",
    "    with torch.cuda.amp.autocast():\n",
    "        loss = F.cross_entropy(student(ids)[:, :-1].reshape(-1, config.vocab_size), ids[:, 1:].reshape(-1))\n",
    "    lora_opt.zero_grad()\n",
    "    loss.backward()\n",
    "    lora_opt.step()\n",
    "    ttt_logs['loss_history'].append({'step': step, 'loss': loss.item()})\n",
    "    if step % 20 == 0:\n",
    "        print(f\"  ttt {step}: loss={loss.item():.4f}\")\n",
    "\n",
    "post_ttt_loss = evaluate(student, val_loader, DEVICE)\n",
    "post_ttt_ppl = get_ppl(post_ttt_loss)\n",
    "print(f\"\\npost-ttt ppl: {post_ttt_ppl:.2f}\")\n",
    "print(f\"ttt improvement: {pre_ttt_ppl - post_ttt_ppl:.1f} ppl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n# cell 22: final evaluation (v13 - Proper CTKD with GRL)\n# =============================================================================\nprint(\"=\"*60)\nprint(\"final evaluation (v13 - CTKD with Gradient Reversal Layer)\")\nprint(\"=\"*60)\n\nteacher_loss = evaluate(teacher, val_loader, DEVICE, is_gpt2=True)\nteacher_ppl = get_ppl(teacher_loss)\nstudent_loss = evaluate(student, val_loader, DEVICE)\nstudent_ppl = get_ppl(student_loss)\n\n# v13: Get final temperature and lambda from CTKD\nfinal_temp = distill_logs['temp_history'][-1]['temperature'] if distill_logs['temp_history'] else config.tau_init\nfinal_lambda = distill_logs['temp_history'][-1].get('lambda', config.lambda_max) if distill_logs['temp_history'] else config.lambda_max\n\n# VRAM logging\nvram_peak_gb = torch.cuda.max_memory_allocated() / 1e9 if torch.cuda.is_available() else 0\n\nprint(f\"\")\nprint(f\"{'model':<30} {'ppl':>10} {'params':>15}\")\nprint(\"-\" * 55)\nprint(f\"{'gpt-2 (teacher)':<30} {teacher_ppl:>10.2f} {teacher_params:>15,}\")\nprint(f\"{'asnn-goose v13 (student)':<30} {student_ppl:>10.2f} {student_params:>15,}\")\nprint(\"-\" * 55)\nprint(f\"{'compression':<30} {compression_ratio:>10.1f}x\")\nprint(f\"{'ppl gap':<30} {student_ppl - teacher_ppl:>10.2f}\")\nprint(f\"{'spike density':<30} {student.get_avg_spike_density():>10.3f}\")\nprint(f\"{'VRAM peak':<30} {vram_peak_gb:>10.2f}GB\")\nprint(f\"{'final temperature':<30} {final_temp:>10.2f}\")\nprint(f\"{'final lambda (GRL)':<30} {final_lambda:>10.3f}\")\nprint(\"\")\nprint(\"CTKD Implementation:\")\nprint(f\"  tau range: [{config.tau_min:.1f}, {config.tau_max:.1f}]\")\nprint(f\"  lambda warmup ratio: {config.lambda_warmup_ratio:.0%}\")\nprint(f\"  GRL: Gradient Reversal Layer for adversarial min-max\")\nprint(\"\")\nprint(\"version comparison:\")\nprint(f\"  v6: 627.3 PPL (baseline)\")\nprint(f\"  v7: 1655 PPL (regression!)\")\nprint(f\"  v8: 559 PPL (fixed)\")\nprint(f\"  v9: 541.7 PPL (capacity increase)\")\nprint(f\"  v10: 514.5 PPL (320d/5L baseline)\")\nprint(f\"  v11: 512.67 PPL (channel-wise, WITH reg)\")\nprint(f\"  v11.1: 512.04 PPL (channel-wise, NO reg)\")\nprint(f\"  v12: FAILED (temp runaway without GRL)\")\nprint(f\"  v13: {student_ppl:.2f} PPL (POCL, T={final_temp:.2f}, \u03bb={final_lambda:.3f})\")\nif student_ppl < 500:\n    print(f\"  v13 TARGET MET! PPL < 500\")\nelif student_ppl < 512.04:\n    print(f\"  v13 beats v11.1 by {512.04 - student_ppl:.1f} PPL\")\nelse:\n    print(f\"  WARNING: v13 did not improve over v11.1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n# cell 23: visualization\n# =============================================================================\nfig, axes = plt.subplots(2, 3, figsize=(15, 10))\n\n# distillation loss\nd_steps = [l['step'] for l in distill_logs['loss_history']]\nd_losses = [l['loss'] for l in distill_logs['loss_history']]\nkl_losses = [l['loss'] for l in distill_logs['kl_loss_history']]\naxes[0,0].plot(d_steps, d_losses, label='total', alpha=0.8)\naxes[0,0].plot(d_steps, kl_losses, label='kl', alpha=0.7)\naxes[0,0].set_xlabel('step')\naxes[0,0].set_ylabel('loss')\naxes[0,0].set_title('distillation loss (v11)')\naxes[0,0].legend()\n\n# validation ppl\np_steps = [l['step'] for l in distill_logs['ppl_history']]\np_ppls = [l['ppl'] for l in distill_logs['ppl_history']]\naxes[0,1].plot(p_steps, p_ppls, 'orange', marker='o')\naxes[0,1].axhline(y=teacher_ppl, color='green', linestyle='--', label=f'teacher ({teacher_ppl:.1f})')\naxes[0,1].axhline(y=627.3, color='blue', linestyle=':', label='v6 (627.3)')\naxes[0,1].axhline(y=541.7, color='purple', linestyle=':', label='v9 (541.7)')\naxes[0,1].axhline(y=480, color='red', linestyle='--', label='v11 target (480)')\naxes[0,1].set_xlabel('step')\naxes[0,1].set_ylabel('ppl')\naxes[0,1].set_title('validation ppl')\naxes[0,1].legend()\n\n# lr schedule\nlr_steps = [l['step'] for l in distill_logs['lr_history']]\nlr_vals = [l['lr'] for l in distill_logs['lr_history']]\naxes[0,2].plot(lr_steps, lr_vals, 'purple')\naxes[0,2].axvline(x=config.warmup_steps, color='gray', linestyle='--', label=f'warmup ({config.warmup_steps})')\naxes[0,2].set_xlabel('step')\naxes[0,2].set_ylabel('lr')\naxes[0,2].set_title('learning rate')\naxes[0,2].legend()\n\n# spike density + amplitudes (first 4 layers)\nspike_summary = spike_stats.get_summary()\nlayers = [f'layer_{i}' for i in range(min(4, config.n_layers))]\nk_dens = [spike_summary['per_layer'][l]['k_final'] for l in layers]\nv_dens = [spike_summary['per_layer'][l]['v_final'] for l in layers]\nk_amps = [spike_summary['per_layer'][l]['k_amp_final'] for l in layers]\nv_amps = [spike_summary['per_layer'][l]['v_amp_final'] for l in layers]\n\nx = np.arange(len(layers))\naxes[1,0].bar(x - 0.2, k_dens, 0.4, label='k density')\naxes[1,0].bar(x + 0.2, v_dens, 0.4, label='v density')\nax2 = axes[1,0].twinx()\nax2.plot(x, k_amps, 'r-o', label='k amp')\nax2.plot(x, v_amps, 'b-s', label='v amp')\naxes[1,0].set_xlabel('layer')\naxes[1,0].set_ylabel('density')\nax2.set_ylabel('amplitude')\naxes[1,0].set_title(f'spike density & amps (first 4/{config.n_layers} layers)')\naxes[1,0].legend(loc='upper left')\nax2.legend(loc='upper right')\n\n# ttt loss\nt_steps = [l['step'] for l in ttt_logs['loss_history']]\nt_losses = [l['loss'] for l in ttt_logs['loss_history']]\naxes[1,1].plot(t_steps, t_losses, 'red')\naxes[1,1].set_xlabel('step')\naxes[1,1].set_ylabel('ce loss')\naxes[1,1].set_title('ttt with lora')\n\n# version comparison\nversions = ['v6', 'v7', 'v8', 'v9', 'v11']\nt_ppls = [44.6, 44.6, 44.6, 44.6, teacher_ppl]\ns_ppls = [627.3, 1655, 559, 541.7, student_ppl]\nx = np.arange(len(versions))\naxes[1,2].bar(x - 0.2, t_ppls, 0.4, label='teacher', alpha=0.7)\naxes[1,2].bar(x + 0.2, s_ppls, 0.4, label='student', alpha=0.7)\naxes[1,2].axhline(y=480, color='red', linestyle='--', label='v11 target', alpha=0.7)\naxes[1,2].set_xticks(x)\naxes[1,2].set_xticklabels(versions)\naxes[1,2].set_ylabel('ppl')\naxes[1,2].set_title('version comparison')\naxes[1,2].legend()\naxes[1,2].set_yscale('log')\n\nplt.tight_layout()\nfigure_path = f'{OUTPUT_DIR}/figures/v11_training_{RUN_TIMESTAMP}.png'\nplt.savefig(figure_path, dpi=300, bbox_inches='tight')\nplt.show()\nprint(f\"saved: {figure_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n# cell 24: build results dict (v13 - Proper CTKD with GRL)\n# =============================================================================\nprint(\"building results (v13 - CTKD with Gradient Reversal Layer)...\")\n\nwith open(figure_path, 'rb') as f:\n    figure_base64 = base64.b64encode(f.read()).decode('utf-8')\n\n# v13: Extract final lambda\nfinal_lambda = distill_logs['temp_history'][-1].get('lambda', config.lambda_max) if distill_logs['temp_history'] else config.lambda_max\n\nresults = {\n    'version': 'v13',\n    'timestamp': datetime.now().isoformat(),\n    'run_id': RUN_TIMESTAMP,\n    'platform': PLATFORM,\n    'description': 'CTKD with Gradient Reversal Layer - adversarial temperature learning',\n\n    'v13_design': {\n        'principle': 'CTKD: Adversarial min-max optimization via GRL',\n        'innovation': 'Gradient Reversal Layer makes temperature MAXIMIZE KL while student MINIMIZES',\n        'rationale': 'Proper CTKD (ArXiv 2211.16231) requires adversarial training, not simple regularization',\n        'why_v12_failed': 'v12 used simple regularization - optimizer pushed T to max for easy KL',\n        'techniques': {\n            'ctkd_with_grl': 'ENABLED - Gradient Reversal Layer for adversarial min-max (v13 KEY)',\n            'lambda_scheduling': f'Cosine warmup 0->{config.lambda_max} with {config.lambda_warmup_ratio:.0%} warmup',\n            'sigmoid_bounding': f'T bounded to [{config.tau_min}, {config.tau_max}] via sigmoid (smooth gradients)',\n            'no_manual_reg': 'GRL eliminates need for manual temperature regularization',\n            'progressive_stages': 'DISABLED',\n            'channel_wise_spikes': 'DISABLED (structural symmetry issue)',\n        },\n        'grl_mechanism': {\n            'forward_pass': 'Identity: GRL(x) = x',\n            'backward_pass': 'Negation: dGRL/dx = -lambda',\n            'effect': 'Temperature gradients reversed -> T maximizes KL loss',\n        },\n        'temperature_config': {\n            'tau_min': config.tau_min,\n            'tau_max': config.tau_max,\n            'tau_init': config.tau_init,\n            'lambda_max': config.lambda_max,\n            'lambda_warmup_ratio': config.lambda_warmup_ratio,\n        },\n        'architecture': {\n            'd_model': 320,\n            'n_layers': 5,\n            'params': '~22M',\n        },\n        'speedups': {\n            'gradient_checkpointing': USE_GRADIENT_CHECKPOINTING,\n            'torch_compile': compile_success,\n            'fused_optimizer': True,\n            'accumulation_steps': config.accumulation_steps,\n        },\n        'unchanged': [\n            'hidden_align_weight: 0.0',\n            'warmup_steps: 50',\n            'distill_steps: 3000',\n        ],\n    },\n\n    'architecture': {\n        'teacher': {'name': 'gpt2', 'params': teacher_params},\n        'student': {\n            'name': 'asnn-goose-v13',\n            'd_model': config.d_model,\n            'n_layers': config.n_layers,\n            'params': student_params,\n        },\n        'projector_params': projector_params,\n        'compression_ratio': compression_ratio,\n        'vram_peak_gb': vram_peak_gb,\n    },\n\n    'training_config': {\n        'distill_steps': config.distill_steps,\n        'tau_min': config.tau_min,\n        'tau_max': config.tau_max,\n        'tau_init': config.tau_init,\n        'final_temperature': final_temp,\n        'lambda_max': config.lambda_max,\n        'lambda_warmup_ratio': config.lambda_warmup_ratio,\n        'final_lambda': final_lambda,\n        'hidden_align_weight': config.hidden_align_weight,\n        'warmup_steps': config.warmup_steps,\n        'batch_size': config.batch_size,\n        'accumulation_steps': config.accumulation_steps,\n        'effective_batch': config.batch_size * config.accumulation_steps,\n        'distill_lr': config.distill_lr,\n        'max_grad_norm': config.max_grad_norm,\n    },\n\n    'results': {\n        'teacher_ppl': teacher_ppl,\n        'student_ppl': student_ppl,\n        'ppl_gap': student_ppl - teacher_ppl,\n        'spike_density': student.get_avg_spike_density(),\n        'amplitudes': student.get_amplitudes(),\n        'final_temperature': final_temp,\n        'final_lambda': final_lambda,\n        'target_met': student_ppl < 500,\n    },\n\n    'training_curves': {\n        'loss_history': distill_logs['loss_history'],\n        'kl_loss_history': distill_logs['kl_loss_history'],\n        'align_loss_history': distill_logs['align_loss_history'],\n        'ppl_history': distill_logs['ppl_history'],\n        'lr_history': distill_logs['lr_history'],\n        'temp_history': distill_logs['temp_history'],  # v13: includes temperature AND lambda\n    },\n\n    'hardware_stats': hw_stats.get_summary(),\n    'spike_analysis': spike_stats.get_summary(),\n\n    'ttt': {\n        'lora_params': lora_params,\n        'pre_ppl': pre_ttt_ppl,\n        'post_ppl': post_ttt_ppl,\n        'improvement': pre_ttt_ppl - post_ttt_ppl,\n        'loss_history': ttt_logs['loss_history'],\n    },\n\n    'comparison': {\n        'v6': {'student_ppl': 627.3, 'note': 'baseline'},\n        'v7': {'student_ppl': 1655, 'note': 'regression (align=1.0, T=4)'},\n        'v8': {'student_ppl': 559, 'note': 'fixed defaults (align=0, T=2)'},\n        'v9': {'student_ppl': 541.7, 'note': 'capacity increase (320d, 5L)'},\n        'v10': {'student_ppl': 514.5, 'note': '320d/5L baseline'},\n        'v11': {'student_ppl': 512.67, 'note': 'channel-wise WITH reg (bug)'},\n        'v11.1': {'student_ppl': 512.04, 'note': 'channel-wise NO reg (symmetry issue)'},\n        'v12': {'student_ppl': 'FAILED', 'note': 'temp runaway without GRL'},\n        'v13': {'student_ppl': student_ppl, 'note': f'POCL (T={final_temp:.2f}, L={final_lambda:.3f})'},\n    },\n\n    'figures': {\n        'training_plot': {\n            'filename': f'v13_training_{RUN_TIMESTAMP}.png',\n            'base64': figure_base64,\n        }\n    },\n\n    # validation_tests will be added in cell 26\n}\n\nprint(\"results dict built (validation_tests pending)\")\nprint(f\"  version: v13 (CTKD with GRL)\")\nprint(f\"  final_temperature: {final_temp:.2f}\")\nprint(f\"  final_lambda: {final_lambda:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n# cell 25: validation tests (v13 - 12 tests with POCL)\n# =============================================================================\n# These tests validate correct v13 implementation\n\nprint(\"=\"*60)\nprint(\"v13 Validation Test Suite\")\nprint(\"=\"*60)\n\ntests = []\n\n# =============================================================================\n# Test 1: PPL Target (<420)\n# =============================================================================\nif distill_logs['ppl_history']:\n    best_ppl_entry = min(distill_logs['ppl_history'], key=lambda x: x['ppl'])\n    best_ppl = best_ppl_entry['ppl']\n    target_ppl = 420  # v13 target\n    ppl_pass = best_ppl < target_ppl\n    tests.append(('PPL < 420', ppl_pass, f\"best_ppl={best_ppl:.2f}, target={target_ppl}\"))\nelse:\n    tests.append(('PPL < 420', False, \"No PPL history found\"))\n\n# =============================================================================\n# Test 2: PPL Improvement over v13 (445.61)\n# =============================================================================\nif distill_logs['ppl_history']:\n    best_ppl_entry = min(distill_logs['ppl_history'], key=lambda x: x['ppl'])\n    v12_1_ppl = 445.61\n    improvement = v12_1_ppl - best_ppl_entry['ppl']\n    improve_pass = improvement > 0\n    tests.append(('Improved over v12.1', improve_pass, f\"improvement={improvement:.2f} PPL\"))\nelse:\n    tests.append(('Improved over v12.1', False, \"No PPL history\"))\n\n# =============================================================================\n# Test 3: Spike Density in Valid Range [0.1, 0.9]\n# =============================================================================\nif spike_stats.density_history:\n    final_density = spike_stats.density_history[-1]['density']\n    density_pass = 0.1 <= final_density <= 0.9\n    tests.append(('Spike density [0.1, 0.9]', density_pass, f\"density={final_density:.3f}\"))\nelse:\n    tests.append(('Spike density [0.1, 0.9]', False, \"No spike history\"))\n\n# =============================================================================\n# Test 4: Amplitudes in Healthy Range [0.3, 3.0]\n# =============================================================================\namps = student.get_amplitudes()\namp_values = []\nfor layer_name, layer_amps in amps.items():\n    amp_values.extend([layer_amps['k'], layer_amps['v']])\namp_min, amp_max = min(amp_values), max(amp_values)\namp_pass = 0.3 <= amp_min and amp_max <= 3.0\ntests.append(('Amplitudes [0.3, 3.0]', amp_pass, f\"range=[{amp_min:.2f}, {amp_max:.2f}]\"))\n\n# =============================================================================\n# Test 5: Training Completed (all steps or early stopped)\n# =============================================================================\nif distill_logs['early_stopped']:\n    training_pass = distill_logs['early_stop_step'] > config.distill_steps * 0.3  # At least 30% done\n    tests.append(('Training completed', training_pass, f\"Early stopped at {distill_logs['early_stop_step']} steps\"))\nelse:\n    training_pass = len(distill_logs['loss_history']) >= config.distill_steps * 0.95\n    tests.append(('Training completed', training_pass, f\"Completed {len(distill_logs['loss_history'])}/{config.distill_steps} steps\"))\n\n# =============================================================================\n# Test 6: No NaN/Inf in Loss\n# =============================================================================\nnan_inf_found = False\nfor h in distill_logs['loss_history']:\n    if h['loss'] != h['loss'] or h['loss'] == float('inf') or h['loss'] == float('-inf'):\n        nan_inf_found = True\n        break\nnan_pass = not nan_inf_found\ntests.append(('No NaN/Inf loss', nan_pass, \"All losses finite\" if nan_pass else \"Found NaN/Inf\"))\n\n# =============================================================================\n# Test 7: VRAM Usage Reasonable (<8GB)\n# =============================================================================\nif hasattr(hw_stats, 'get_summary'):\n    hw_summary = hw_stats.get_summary()\n    vram_gb = hw_summary.get('gpu_memory_peak_gb', 0)\n    vram_pass = vram_gb < 8.0\n    tests.append(('VRAM < 8GB', vram_pass, f\"peak={vram_gb:.2f}GB\"))\nelse:\n    tests.append(('VRAM < 8GB', True, \"hw_stats not available\"))\n\n# =============================================================================\n# Test 8: POCL Stages Executed (v13)\n# =============================================================================\nif config.use_pocl:\n    stage_transitions = len(distill_logs['stage_transitions'])\n    expected_transitions = config.pocl_stages - 1  # e.g., 3 stages = 2 transitions\n    pocl_pass = stage_transitions == expected_transitions\n    tests.append(('POCL stages executed', pocl_pass, f\"transitions={stage_transitions}, expected={expected_transitions}\"))\nelse:\n    tests.append(('POCL stages executed', True, \"POCL disabled\"))\n\n# =============================================================================\n# Test 9: Temperature Followed Schedule (v13)\n# =============================================================================\nif config.use_pocl and distill_logs['temp_history']:\n    temps = [h['temperature'] for h in distill_logs['temp_history']]\n    start_temp = temps[0]\n    end_temp = temps[-1]\n    # Rising schedule: should start at 1.0, end at 2.0\n    temp_schedule_pass = (start_temp == config.pocl_temp_schedule[0] and\n                         end_temp == config.pocl_temp_schedule[-1])\n    tests.append(('Temperature schedule', temp_schedule_pass, f\"start={start_temp:.1f}, end={end_temp:.1f}\"))\nelse:\n    tests.append(('Temperature schedule', True, \"POCL disabled or no temp history\"))\n\n# =============================================================================\n# Test 10: Early Stopping Working (if triggered)\n# =============================================================================\nif config.use_early_stopping:\n    if distill_logs['early_stopped']:\n        # Should have stopped at reasonable point\n        es_step = distill_logs['early_stop_step']\n        es_pass = config.distill_steps * 0.3 < es_step < config.distill_steps\n        tests.append(('Early stopping working', es_pass, f\"stopped at {es_step}\"))\n    else:\n        # Completed training, check if patience never exceeded\n        if distill_logs['ppl_history']:\n            best_ppl_entry = min(distill_logs['ppl_history'], key=lambda x: x['ppl'])\n            last_improvement_step = best_ppl_entry['step']\n            final_step = distill_logs['ppl_history'][-1]['step']\n            gap = final_step - last_improvement_step\n            es_pass = gap <= config.early_stopping_patience + config.eval_interval\n            tests.append(('Early stopping working', es_pass, f\"last improvement at step {last_improvement_step}\"))\n        else:\n            tests.append(('Early stopping working', True, \"No PPL history\"))\nelse:\n    tests.append(('Early stopping working', True, \"Early stopping disabled\"))\n\n# =============================================================================\n# Test 11: CTKD Disabled (v13)\n# =============================================================================\nctkd_disabled = not config.use_ctkd\ntests.append(('CTKD disabled for v13', ctkd_disabled, f\"use_ctkd={config.use_ctkd}\"))\n\n# =============================================================================\n# Test 12: Extended Training (5000 steps)\n# =============================================================================\nextended_pass = config.distill_steps >= 5000\ntests.append(('Extended training (5000+)', extended_pass, f\"distill_steps={config.distill_steps}\"))\n\n# =============================================================================\n# Report Results\n# =============================================================================\nprint(\"\\n\" + \"-\"*60)\nprint(\"TEST RESULTS\")\nprint(\"-\"*60)\n\npassed = 0\nfailed = 0\nfor name, result, details in tests:\n    status = \"PASS\" if result else \"FAIL\"\n    symbol = \"V\" if result else \"X\"\n    print(f\"[{symbol}] {name}: {details}\")\n    if result:\n        passed += 1\n    else:\n        failed += 1\n\nprint(\"-\"*60)\nprint(f\"SUMMARY: {passed}/{len(tests)} tests passed\")\nif failed > 0:\n    print(f\"WARNING: {failed} tests failed!\")\nelse:\n    print(\"ALL TESTS PASSED! v13 implementation validated.\")\nprint(\"=\"*60)\n\n# Store results\nvalidation_results = {\n    'tests': tests,\n    'passed': passed,\n    'failed': failed,\n    'total': len(tests)\n}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# cell 26: FINAL save with validation_tests (v11 bug fix)\n",
    "# =============================================================================\n",
    "print(\"=\"*60)\n",
    "print(\"FINAL SAVE (includes validation_tests)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Add validation_tests to results\n",
    "results['validation_tests'] = test_results\n",
    "\n",
    "# Save final results.json with ALL data\n",
    "results_path = f'{OUTPUT_DIR}/results/results_{RUN_TIMESTAMP}.json'\n",
    "with open(results_path, 'w') as f:\n",
    "    json.dump(results, f, indent=2, default=str)\n",
    "\n",
    "print(f\"saved: {results_path}\")\n",
    "print(f\"size: {os.path.getsize(results_path) / 1024:.1f} KB\")\n",
    "print(f\"\")\n",
    "print(f\"validation_tests included: {list(test_results.keys())}\")\n",
    "\n",
    "# v11: auto-download AFTER final save\n",
    "print(\"\")\n",
    "print(\"auto-download\")\n",
    "if IS_COLAB:\n",
    "    try:\n",
    "        from google.colab import files\n",
    "        files.download(results_path)\n",
    "        files.download(figure_path)\n",
    "        print(\"downloads started!\")\n",
    "    except Exception as e:\n",
    "        print(f\"download failed: {e}\")\n",
    "elif IS_KAGGLE:\n",
    "    print(f\"kaggle: {results_path}\")\n",
    "else:\n",
    "    print(f\"local: {results_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}