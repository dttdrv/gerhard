{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# cell 1: environment setup (v14.1 - Hyperparameter Tuning)\n# =============================================================================\nimport os\nos.environ['TOKENIZERS_PARALLELISM'] = 'false'\n\nimport sys\nimport time\nimport math\nimport json\nimport base64\nfrom pathlib import Path\nfrom datetime import datetime\nfrom dataclasses import dataclass, asdict, field\nfrom typing import Dict, List, Optional, Tuple, Any\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# v14: torch.compile flag (disable if causing issues)\nUSE_TORCH_COMPILE = True\nUSE_GRADIENT_CHECKPOINTING = True\n\n# generate timestamp for this run\nRUN_TIMESTAMP = datetime.now().strftime('%Y-%m-%d_%H%M%S')\nprint(f\"run timestamp: {RUN_TIMESTAMP}\")\n\n# detect platform\nIS_KAGGLE = 'KAGGLE_KERNEL_RUN_TYPE' in os.environ\nIS_COLAB = 'COLAB_GPU' in os.environ or 'google.colab' in sys.modules\nPLATFORM = 'kaggle' if IS_KAGGLE else 'colab' if IS_COLAB else 'local'\nOUTPUT_DIR = '/kaggle/working/outputs' if IS_KAGGLE else 'outputs'\n\nfor subdir in ['figures', 'checkpoints', 'logs', 'results']:\n    os.makedirs(f'{OUTPUT_DIR}/{subdir}', exist_ok=True)\n\nprint(f\"platform: {PLATFORM}\")\nprint(f\"output directory: {OUTPUT_DIR}\")\nprint(f\"torch.compile: {'enabled' if USE_TORCH_COMPILE else 'disabled'}\")\nprint(f\"gradient checkpointing: {'enabled' if USE_GRADIENT_CHECKPOINTING else 'disabled'}\")\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# cell 2: pytorch and hardware setup (v14.1)\n# =============================================================================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom torch.utils.checkpoint import checkpoint\nimport numpy as np\n\nimport matplotlib\nmatplotlib.use('Agg')\nimport matplotlib.pyplot as plt\n\nfrom tqdm.auto import tqdm\n\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nSEED = 42\n\ntorch.manual_seed(SEED)\nnp.random.seed(SEED)\n\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(SEED)\n    torch.backends.cuda.matmul.allow_tf32 = True\n    torch.backends.cudnn.allow_tf32 = True\n    torch.backends.cudnn.benchmark = True\n\n    gpu_name = torch.cuda.get_device_name(0)\n    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n    print(f\"gpu: {gpu_name}\")\n    print(f\"memory: {gpu_memory:.1f} gb\")\n\n# v14: set float32 matmul precision for torch.compile\nif USE_TORCH_COMPILE and hasattr(torch, 'set_float32_matmul_precision'):\n    torch.set_float32_matmul_precision('high')\n    print(\"float32 matmul precision: high (for torch.compile)\")\n\nprint(f\"device: {DEVICE}\")\nprint(f\"pytorch: {torch.__version__}\")\n\n# check torch.compile availability\nTORCH_COMPILE_AVAILABLE = hasattr(torch, 'compile') and torch.__version__ >= '2.0'\nprint(f\"torch.compile available: {TORCH_COMPILE_AVAILABLE}\")\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# cell 4: configuration (v14.1 - Hyperparameter Tuning per External LLM)\n# =============================================================================\n@dataclass\nclass Config:\n    # Version for dynamic labeling (NEVER hardcode versions elsewhere!)\n    VERSION: str = 'v14.3'\n    VERSION_DESC: str = 'd_model=768, adjusted training for scaling'\n    \n    # gpt-2 teacher (frozen, pre-trained)\n    teacher_name: str = \"gpt2\"\n\n    # student model architecture - v14.1: capacity increase (512d, 5L, ~56M)\n    d_model: int = 768      # v14.3: safer scaling (512->768 instead of 512->1024)\n    n_layers: int = 5       # v10 value (DO NOT reduce)\n    vocab_size: int = 50257\n    max_seq_len: int = 256\n\n    # ==========================================================================\n    # v14.1: Feature Dynamics Distillation (FDD) with CKA Loss\n    # ==========================================================================\n    # FDD aligns layer-wise dynamics (\u0394h) between student and teacher\n    # Uses CKA (Centered Kernel Alignment) - projector-free, dimension-agnostic\n    use_fdd: bool = True\n    fdd_weight: float = 0.1           # v14.1: 100x increase (CKA bounded [0,1], safe)\n    fdd_warmup_steps: int = 500       # Don't enable until step 500\n    fdd_loss_type: str = \"cka\"        # Options: \"cka\" (recommended), \"mse\"\n    fdd_kill_threshold: float = 0.10  # Disable if PPL increases >10%\n\n    # ==========================================================================\n    # v14.1: Hard Distillation (CE with ground truth)\n    # ==========================================================================\n    # Anchors student to correct tokens, not just teacher's soft distribution\n    ce_hard_weight: float = 0.5       # Ground truth CE loss weight\n    \n    # Layer mapping: student_layer -> teacher_layer\n    # With 5 student layers and 12 teacher layers:\n    # We align early/middle/late semantic representations\n    # Default: {0: 2, 2: 6, 4: 10}\n    fdd_n_align_layers: int = 3       # Number of layer pairs to align\n\n    # ==========================================================================\n    # v14.1: Extended Training (same as v13.1)\n    # ==========================================================================\n    distill_steps: int = 7000       # v14.3: more steps for larger model\n    distill_lr: float = 2e-4       # v14.3: reduced for larger model\n    warmup_steps: int = 100\n    min_lr: float = 1e-6\n\n    # v14.1: gradient accumulation\n    accumulation_steps: int = 2       # effective batch = 8 * 2 = 16\n\n    # ==========================================================================\n    # v14.1: Early Stopping (same as v13.1)\n    # ==========================================================================\n    use_early_stopping: bool = True\n    early_stopping_patience: int = 800  # v14.3: more patience for larger model\n    min_ppl_delta: float = 1.0\n\n    # ==========================================================================\n    # v14.1: POCL DISABLED (failed in v13)\n    # ==========================================================================\n    use_pocl: bool = False\n    pocl_stages: int = 3\n    pocl_temp_schedule: tuple = (1.0, 1.5, 2.0)\n    pocl_pretrain_steps: int = 100\n\n    # ==========================================================================\n    # v14.1: CTKD ENABLED (proven in v12.1, v13.1)\n    # ==========================================================================\n    use_ctkd: bool = True\n    tau_min: float = 1.0\n    tau_max: float = 5.0\n    tau_init: float = 2.0\n    lambda_max: float = 1.0\n    lambda_warmup_ratio: float = 0.25  # v14.3: slower CTKD ramp-up\n\n    # Legacy flags (all disabled for v14.1)\n    use_learnable_temperature: bool = False\n    use_channel_wise_spikes: bool = False\n    use_progressive_stages: bool = False\n    temperature: float = 2.0\n\n    # Hidden alignment DISABLED (using FDD instead)\n    hidden_align_weight: float = 0.0\n    teacher_d_model: int = 768\n    teacher_n_layers: int = 12\n    temperature_lr: float = 0.001\n\n    # lora for ttt\n    lora_rank: int = 8\n    lora_alpha: float = 16.0\n    ttt_lr: float = 1e-4\n    ttt_steps: int = 100\n\n    # spiking parameters\n    spike_alpha: float = 1.0\n\n    # general training\n    batch_size: int = 8\n    max_grad_norm: float = 1.0\n    eval_interval: int = 300\n\nconfig = Config()\n\nprint(f\"configuration (v14.1 - Hyperparameter Tuning per External LLM):\")\nprint(f\"  teacher: {config.teacher_name} (124m params)\")\nprint(f\"  student: d={config.d_model}, layers={config.n_layers} (~56M params)\")\nprint(f\"\")\nprint(f\"{config.VERSION} CHANGES (based on external LLM diagnosis):\")\nprint(f\"  d_model: 320 -> 512 (capacity increase for ternary compensation)\")\nprint(f\"  fdd_weight: 0.001 -> 0.1 (enable alignment gradient signal)\")\nprint(f\"  ce_hard_weight: {config.ce_hard_weight} (NEW - ground truth anchoring)\")\nprint(f\"\")\nprint(f\"{config.VERSION} INNOVATION - Feature Dynamics Distillation (FDD):\")\nprint(f\"  use_fdd: {config.use_fdd}\")\nprint(f\"  fdd_weight: {config.fdd_weight} (100x increase, CKA bounded [0,1], safe)\")\nprint(f\"  fdd_warmup_steps: {config.fdd_warmup_steps}\")\nprint(f\"  fdd_loss_type: {config.fdd_loss_type}\")\nprint(f\"  fdd_n_align_layers: {config.fdd_n_align_layers}\")\nprint(f\"  fdd_kill_threshold: {config.fdd_kill_threshold} (10% PPL increase triggers disable)\")\nprint(f\"\")\nprint(f\"  FDD Strategy:\")\nprint(f\"    - Align layer DYNAMICS (\u0394h), not just hidden states\")\nprint(f\"    - Use CKA loss (projector-free, dimension-agnostic)\")\nprint(f\"    - 100x weight increase (was too weak before)\")\nprint(f\"    - Safety kill-switch if PPL regresses\")\nprint(f\"\")\nprint(f\"{config.VERSION}: Hard Distillation:\")\nprint(f\"  ce_hard_weight: {config.ce_hard_weight}\")\nprint(f\"  Formula: L = KL + 0.5*CE + 0.1*FDD\")\nprint(f\"\")\nprint(f\"{config.VERSION}: CTKD (proven technique):\")\nprint(f\"  use_ctkd: {config.use_ctkd}\")\nprint(f\"  Temperature bounds: [{config.tau_min}, {config.tau_max}]\")\nprint(f\"  Lambda warmup: {config.lambda_warmup_ratio*100:.0f}%\")\nprint(f\"\")\nprint(f\"{config.VERSION}: Extended Training:\")\nprint(f\"  distill_steps: {config.distill_steps}\")\nprint(f\"  warmup_steps: {config.warmup_steps}\")\nprint(f\"  min_lr: {config.min_lr}\")\nprint(f\"\")\nprint(f\"{config.VERSION}: Early Stopping:\")\nprint(f\"  use_early_stopping: {config.use_early_stopping}\")\nprint(f\"  patience: {config.early_stopping_patience} steps\")\nprint(f\"  min_delta: {config.min_ppl_delta} PPL\")\nprint(f\"\")\nprint(f\"disabled features:\")\nprint(f\"  POCL: {config.use_pocl} (failed in v13)\")\nprint(f\"  channel-wise spikes: {config.use_channel_wise_spikes}\")\nprint(f\"  old hidden alignment: {config.hidden_align_weight}\")\nprint(f\"\")\nprint(f\"training:\")\nprint(f\"  accumulation: {config.accumulation_steps} (effective batch = {config.batch_size * config.accumulation_steps})\")\nprint(f\"\")\nprint(f\"targets:\")\nprint(f\"  PPL: <400 (improve on v14 424.81)\")\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# cell 3: PRE-TRAINING VALIDATION (run before training to catch issues)\n# =============================================================================\nprint(\"=\" * 70)\nprint(\"PRE-TRAINING VALIDATION\")\nprint(\"=\" * 70)\n\nvalidation_errors = []\nvalidation_warnings = []\n\n# 1. Config Sanity Checks\nprint(\"\")\nprint(\"[1] CONFIG SANITY CHECKS\")\n\nif config.d_model < 256:\n    validation_errors.append(f\"d_model={config.d_model} too small (min 256)\")\nelif config.d_model > 2048:\n    validation_warnings.append(f\"d_model={config.d_model} very large - check VRAM\")\nprint(f\"  d_model: {config.d_model}\")\n\nif config.n_layers < 3:\n    validation_errors.append(f\"n_layers={config.n_layers} too few\")\nprint(f\"  n_layers: {config.n_layers}\")\n\nprint(f\"  fdd_weight: {config.fdd_weight}\")\nprint(f\"  ce_hard_weight: {config.ce_hard_weight}\")\nprint(f\"  VERSION: {config.VERSION}\")\nprint(f\"  VERSION_DESC: {config.VERSION_DESC}\")\n\n# 2. Memory Estimation\nprint(\"\")\nprint(\"[2] MEMORY ESTIMATION\")\nembed_params = config.vocab_size * config.d_model * 2\nlayer_params = config.n_layers * config.d_model * config.d_model * 8\ntotal_params_est = embed_params + layer_params\nprint(f\"  Estimated params: ~{total_params_est/1e6:.1f}M\")\n\nvram_est_gb = (total_params_est * 4 * 3) / 1e9\nprint(f\"  Estimated VRAM: ~{vram_est_gb:.1f}GB\")\n\nif vram_est_gb > 14:\n    validation_warnings.append(f\"VRAM estimate {vram_est_gb:.1f}GB may exceed 16GB limit\")\n    print(f\"  WARNING: May exceed 16GB VRAM limit!\")\n\n# 3. Training Config\nprint(\"\")\nprint(\"[3] TRAINING CONFIG\")\nprint(f\"  distill_steps: {config.distill_steps}\")\nprint(f\"  distill_lr: {config.distill_lr}\")\nprint(f\"  batch_size: {config.batch_size}\")\nif hasattr(config, 'accumulation_steps'):\n    eff_batch = config.batch_size * config.accumulation_steps\n    print(f\"  effective_batch: {eff_batch}\")\nprint(f\"  early_stopping: patience={config.early_stopping_patience}\")\n\n# 4. Feature Flags\nprint(\"\")\nprint(\"[4] FEATURE FLAGS\")\nprint(f\"  use_fdd: {config.use_fdd}\")\nprint(f\"  use_ctkd: {config.use_ctkd}\")\nprint(f\"  use_pocl: {config.use_pocl}\")\n\n# Summary\nprint(\"\")\nprint(\"=\" * 70)\nif validation_errors:\n    print(f\"VALIDATION FAILED - {len(validation_errors)} errors:\")\n    for e in validation_errors:\n        print(f\"   - {e}\")\n    raise RuntimeError(\"Fix validation errors before training!\")\nelif validation_warnings:\n    print(f\"VALIDATION PASSED WITH {len(validation_warnings)} WARNINGS:\")\n    for w in validation_warnings:\n        print(f\"   - {w}\")\nelse:\n    print(\"ALL VALIDATIONS PASSED\")\nprint(\"=\" * 70)\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# cell 6: v13 PROPER CTKD Implementation\n",
    "# =============================================================================\n",
    "# References:\n",
    "# - CTKD Paper: https://arxiv.org/abs/2211.16231\n",
    "# - GRL Origin: Ganin & Lempitsky (2015) https://arxiv.org/abs/1409.7495\n",
    "# - torch-gradient-reversal: https://pypi.org/project/torch-gradient-reversal/\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# GradientReversalFunction (Custom Autograd)\n",
    "# -----------------------------------------------------------------------------\n",
    "class GradientReversalFunction(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    Gradient Reversal Layer for adversarial training.\n",
    "    \n",
    "    Forward: Identity mapping f(x) = x\n",
    "    Backward: Negates gradient \u2202f/\u2202x = -\u03bb * grad\n",
    "    \n",
    "    This enables min-max optimization in a single backward pass:\n",
    "    - Student minimizes loss (normal gradients)\n",
    "    - Temperature maximizes loss (reversed gradients via GRL)\n",
    "    \n",
    "    Reference: Ganin & Lempitsky, \"Unsupervised Domain Adaptation by Backpropagation\"\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, x, lambda_):\n",
    "        # Save lambda for backward pass\n",
    "        ctx.lambda_ = lambda_\n",
    "        # Forward is identity (must clone to avoid in-place issues)\n",
    "        return x.clone()\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # Backward negates and scales gradient\n",
    "        # Returns: (grad for x, grad for lambda_)\n",
    "        # lambda_ is a hyperparameter, doesn't need gradient\n",
    "        return -ctx.lambda_ * grad_output, None\n",
    "\n",
    "\n",
    "class GradientReversalLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Module wrapper for GradientReversalFunction.\n",
    "    \n",
    "    Usage:\n",
    "        grl = GradientReversalLayer()\n",
    "        grl.set_lambda(0.5)  # Set adversarial strength\n",
    "        y = grl(x)  # Forward: y = x, Backward: grad_x = -0.5 * grad_y\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lambda_ = 1.0\n",
    "    \n",
    "    def set_lambda(self, lambda_: float):\n",
    "        \"\"\"Set the adversarial strength (0 = no reversal, 1 = full reversal).\"\"\"\n",
    "        self.lambda_ = lambda_\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return GradientReversalFunction.apply(x, self.lambda_)\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Lambda Scheduler (Cosine with Warmup)\n",
    "# -----------------------------------------------------------------------------\n",
    "def get_lambda(step: int, total_steps: int, lambda_max: float = 1.0, \n",
    "               warmup_ratio: float = 0.2) -> float:\n",
    "    \"\"\"\n",
    "    Cosine schedule for adversarial strength \u03bb.\n",
    "    \n",
    "    - During warmup (first warmup_ratio of training): \u03bb = 0\n",
    "      Temperature learns freely to find reasonable range\n",
    "    - After warmup: \u03bb increases from 0 to lambda_max via cosine\n",
    "      Gradually increases adversarial pressure\n",
    "    \n",
    "    Args:\n",
    "        step: Current training step\n",
    "        total_steps: Total number of training steps\n",
    "        lambda_max: Maximum \u03bb value (default 1.0 = full reversal)\n",
    "        warmup_ratio: Fraction of training for warmup (default 0.2 = 20%)\n",
    "    \n",
    "    Returns:\n",
    "        Current \u03bb value in [0, lambda_max]\n",
    "    \"\"\"\n",
    "    warmup_steps = int(total_steps * warmup_ratio)\n",
    "    \n",
    "    if step < warmup_steps:\n",
    "        return 0.0\n",
    "    \n",
    "    # Progress after warmup [0, 1]\n",
    "    progress = (step - warmup_steps) / max(total_steps - warmup_steps, 1)\n",
    "    # Cosine increase from 0 to lambda_max\n",
    "    lambda_ = lambda_max * (1 - math.cos(math.pi * progress)) / 2\n",
    "    return lambda_\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# CTKDTemperature (Proper Implementation with GRL)\n",
    "# -----------------------------------------------------------------------------\n",
    "class CTKDTemperature(nn.Module):\n",
    "    \"\"\"\n",
    "    Curriculum Temperature for Knowledge Distillation (CTKD).\n",
    "    \n",
    "    Key features:\n",
    "    1. Adversarial learning via Gradient Reversal Layer\n",
    "    2. Sigmoid bounding for smooth gradients at boundaries\n",
    "    3. Proper initialization via logit transform\n",
    "    \n",
    "    The temperature module tries to MAXIMIZE the KL loss (via GRL),\n",
    "    finding the \"hardest\" temperature for the student.\n",
    "    The student tries to MINIMIZE the KL loss.\n",
    "    This adversarial game leads to optimal curriculum difficulty.\n",
    "    \n",
    "    Reference: Li et al., \"Curriculum Temperature for Knowledge Distillation\", AAAI 2023\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, tau_min: float = 1.0, tau_max: float = 5.0, init: float = 2.0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tau_min: Minimum temperature (default 1.0)\n",
    "            tau_max: Maximum temperature (default 5.0, conservative for LLMs)\n",
    "            init: Initial temperature (default 2.0)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.tau_min = tau_min\n",
    "        self.tau_range = tau_max - tau_min\n",
    "        \n",
    "        # Initialize raw parameter so sigmoid outputs init value\n",
    "        # sigmoid(raw) = (init - tau_min) / tau_range\n",
    "        # raw = logit((init - tau_min) / tau_range)\n",
    "        init_normalized = (init - tau_min) / self.tau_range\n",
    "        init_normalized = max(0.01, min(0.99, init_normalized))  # Clamp for numerical stability\n",
    "        init_raw = math.log(init_normalized / (1 - init_normalized))  # logit function\n",
    "        \n",
    "        self.raw_temp = nn.Parameter(torch.tensor(init_raw, dtype=torch.float32))\n",
    "        self.grl = GradientReversalLayer()\n",
    "        \n",
    "        # Store config for logging\n",
    "        self.tau_min_val = tau_min\n",
    "        self.tau_max_val = tau_max\n",
    "        self.init_val = init\n",
    "    \n",
    "    def forward(self, lambda_: float) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute temperature with GRL applied.\n",
    "        \n",
    "        Args:\n",
    "            lambda_: Current adversarial strength from scheduler\n",
    "        \n",
    "        Returns:\n",
    "            Temperature \u03c4 \u2208 [tau_min, tau_max]\n",
    "        \"\"\"\n",
    "        # Set GRL strength\n",
    "        self.grl.set_lambda(lambda_)\n",
    "        \n",
    "        # Apply GRL to raw parameter (this is where gradient reversal happens!)\n",
    "        raw_reversed = self.grl(self.raw_temp)\n",
    "        \n",
    "        # Sigmoid bounding (smooth, differentiable at boundaries)\n",
    "        tau = self.tau_min + self.tau_range * torch.sigmoid(raw_reversed)\n",
    "        \n",
    "        return tau\n",
    "    \n",
    "    def get_temperature(self) -> float:\n",
    "        \"\"\"Get current temperature without GRL (for logging/display).\"\"\"\n",
    "        with torch.no_grad():\n",
    "            tau = self.tau_min + self.tau_range * torch.sigmoid(self.raw_temp)\n",
    "            return tau.item()\n",
    "    \n",
    "    def get_raw_value(self) -> float:\n",
    "        \"\"\"Get raw (unbounded) parameter value (for debugging).\"\"\"\n",
    "        return self.raw_temp.item()\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Legacy Classes (kept for backward compatibility)\n",
    "# -----------------------------------------------------------------------------\n",
    "class LearnableTemperature(nn.Module):\n",
    "    \"\"\"\n",
    "    DEPRECATED: Simple learnable temperature WITHOUT GRL.\n",
    "    Kept for backward compatibility. Use CTKDTemperature instead.\n",
    "    \n",
    "    WARNING: This class caused temperature runaway in v12!\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, init: float = 2.0):\n",
    "        super().__init__()\n",
    "        self.log_temp = nn.Parameter(torch.log(torch.tensor(init)))\n",
    "    \n",
    "    def forward(self) -> torch.Tensor:\n",
    "        return torch.exp(self.log_temp).clamp(1.0, 10.0)\n",
    "    \n",
    "    def get_temperature(self) -> float:\n",
    "        return self.forward().item()\n",
    "\n",
    "\n",
    "class ChannelWiseTernarySpike(nn.Module):\n",
    "    \"\"\"\n",
    "    Per-channel learnable alpha and amplitude for ternary spikes.\n",
    "    DISABLED in v13 due to structural symmetry issue with RWKV.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, alpha_init: float = 1.0):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.alpha = nn.Parameter(torch.ones(d_model) * alpha_init)\n",
    "        self.amplitude = nn.Parameter(torch.ones(d_model))\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x_abs_mean = x.abs().mean(dim=(0, 1), keepdim=True)\n",
    "        threshold = self.alpha * x_abs_mean\n",
    "        threshold = threshold.clamp(min=0.01, max=10.0)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            pos_mask = (x > threshold).float()\n",
    "            neg_mask = (x < -threshold).float()\n",
    "            spike_signs = pos_mask - neg_mask\n",
    "        \n",
    "        spikes = self.amplitude * spike_signs\n",
    "        return spikes + (x - x.detach())\n",
    "    \n",
    "    def get_amplitude(self) -> float:\n",
    "        return self.amplitude.mean().item()\n",
    "    \n",
    "    def get_stats(self) -> dict:\n",
    "        return {\n",
    "            'alpha_mean': self.alpha.mean().item(),\n",
    "            'alpha_std': self.alpha.std().item(),\n",
    "            'amplitude_mean': self.amplitude.mean().item(),\n",
    "            'amplitude_std': self.amplitude.std().item(),\n",
    "        }\n",
    "\n",
    "    def get_amplitude_stats(self) -> dict:\n",
    "        return {\n",
    "            'mean': self.amplitude.mean().item(),\n",
    "            'std': self.amplitude.std().item(),\n",
    "            'min': self.amplitude.min().item(),\n",
    "            'max': self.amplitude.max().item(),\n",
    "        }\n",
    "\n",
    "\n",
    "class TrainableTernarySpike(nn.Module):\n",
    "    \"\"\"Original trainable ternary spike with scalar amplitude (from v8).\"\"\"\n",
    "\n",
    "    def __init__(self, alpha: float = 1.0):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.amplitude = nn.Parameter(torch.ones(1))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        threshold = self.alpha * x.abs().mean(dim=-1, keepdim=True)\n",
    "        threshold = threshold.clamp(min=0.01, max=10.0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pos_mask = (x > threshold).float()\n",
    "            neg_mask = (x < -threshold).float()\n",
    "            spike_signs = pos_mask - neg_mask\n",
    "\n",
    "        spikes = self.amplitude * spike_signs\n",
    "        return spikes + (x - x.detach())\n",
    "\n",
    "    def get_amplitude(self) -> float:\n",
    "        return self.amplitude.item()\n",
    "\n",
    "\n",
    "def get_stage_params(step: int, total_steps: int = 3000) -> dict:\n",
    "    \"\"\"Progressive training stages (POCL) - kept for infrastructure.\"\"\"\n",
    "    if step < total_steps * 0.4:\n",
    "        return {'stage': 1, 'temp_target': 1.0, 'align_mult': 0.0, 'alpha': 0.9}\n",
    "    elif step < total_steps * 0.7:\n",
    "        return {'stage': 2, 'temp_target': 1.5, 'align_mult': 0.5, 'alpha': 0.7}\n",
    "    else:\n",
    "        return {'stage': 3, 'temp_target': 2.0, 'align_mult': 1.0, 'alpha': 0.5}\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Unit Tests for CTKD Components\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"=\"*60)\n",
    "print(\"v13 CTKD Component Tests\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Test 1: GRL Gradient Reversal\n",
    "print(\"\\n[1] GRL Gradient Reversal Test\")\n",
    "grl = GradientReversalLayer()\n",
    "grl.set_lambda(1.0)\n",
    "x_test = torch.tensor([2.0], requires_grad=True)\n",
    "y_test = grl(x_test)\n",
    "loss_test = y_test.sum()\n",
    "loss_test.backward()\n",
    "expected_grad = -1.0  # GRL should negate: 1 * -1.0 = -1.0\n",
    "actual_grad = x_test.grad.item()\n",
    "grl_pass = abs(actual_grad - expected_grad) < 1e-6\n",
    "print(f\"  Input grad without GRL would be: +1.0\")\n",
    "print(f\"  Input grad with GRL (\u03bb=1.0): {actual_grad:.4f}\")\n",
    "print(f\"  Expected: {expected_grad:.4f}\")\n",
    "print(f\"  {'PASS' if grl_pass else 'FAIL'}\")\n",
    "del x_test, y_test, loss_test\n",
    "\n",
    "# Test 2: Lambda Schedule\n",
    "print(\"\\n[2] Lambda Schedule Test\")\n",
    "total = 3000\n",
    "warmup = 0.2\n",
    "# During warmup\n",
    "lambda_0 = get_lambda(0, total, warmup_ratio=warmup)\n",
    "lambda_500 = get_lambda(500, total, warmup_ratio=warmup)\n",
    "# After warmup\n",
    "lambda_1500 = get_lambda(1500, total, warmup_ratio=warmup)\n",
    "lambda_2999 = get_lambda(2999, total, warmup_ratio=warmup)\n",
    "\n",
    "warmup_pass = lambda_0 == 0.0 and lambda_500 == 0.0\n",
    "increase_pass = 0 < lambda_1500 < lambda_2999 <= 1.0\n",
    "lambda_pass = warmup_pass and increase_pass\n",
    "print(f\"  \u03bb(0) = {lambda_0:.4f} (should be 0.0)\")\n",
    "print(f\"  \u03bb(500) = {lambda_500:.4f} (should be 0.0, still in warmup)\")\n",
    "print(f\"  \u03bb(1500) = {lambda_1500:.4f} (should be > 0)\")\n",
    "print(f\"  \u03bb(2999) = {lambda_2999:.4f} (should be \u2248 1.0)\")\n",
    "print(f\"  {'PASS' if lambda_pass else 'FAIL'}\")\n",
    "\n",
    "# Test 3: Temperature Bounds\n",
    "print(\"\\n[3] Temperature Bounds Test\")\n",
    "temp_module = CTKDTemperature(tau_min=1.0, tau_max=5.0, init=2.0).to(DEVICE)\n",
    "init_temp = temp_module.get_temperature()\n",
    "\n",
    "# Force extreme raw values\n",
    "with torch.no_grad():\n",
    "    temp_module.raw_temp.fill_(-100)\n",
    "    tau_low = temp_module.get_temperature()\n",
    "    \n",
    "    temp_module.raw_temp.fill_(100)\n",
    "    tau_high = temp_module.get_temperature()\n",
    "    \n",
    "    # Reset to init\n",
    "    init_normalized = (2.0 - 1.0) / 4.0\n",
    "    init_raw = math.log(init_normalized / (1 - init_normalized))\n",
    "    temp_module.raw_temp.fill_(init_raw)\n",
    "\n",
    "bounds_pass = (1.0 <= tau_low <= 1.01) and (4.99 <= tau_high <= 5.0) and (1.9 <= init_temp <= 2.1)\n",
    "print(f\"  Initial temp: {init_temp:.4f} (should be \u2248 2.0)\")\n",
    "print(f\"  Min bound test: {tau_low:.4f} (should be \u2248 1.0)\")\n",
    "print(f\"  Max bound test: {tau_high:.4f} (should be \u2248 5.0)\")\n",
    "print(f\"  {'PASS' if bounds_pass else 'FAIL'}\")\n",
    "\n",
    "# Test 4: End-to-End Gradient Flow\n",
    "print(\"\\n[4] End-to-End Gradient Flow Test\")\n",
    "temp_module_test = CTKDTemperature(tau_min=1.0, tau_max=5.0, init=2.0).to(DEVICE)\n",
    "lambda_test = 0.5\n",
    "\n",
    "# Simulate forward pass\n",
    "T = temp_module_test(lambda_test)\n",
    "fake_kl_loss = T * 2.0  # Gradient \u2202L/\u2202T = 2.0\n",
    "\n",
    "# Without GRL: optimizer would DECREASE T to minimize loss\n",
    "# With GRL: optimizer should INCREASE T (because grad is reversed)\n",
    "fake_kl_loss.backward()\n",
    "\n",
    "raw_grad = temp_module_test.raw_temp.grad.item()\n",
    "# The gradient through sigmoid and GRL should be negative (reversed)\n",
    "# Original: \u2202L/\u2202raw > 0 would decrease raw\n",
    "# With GRL: \u2202L/\u2202raw < 0 (negated), so optimizer increases raw\n",
    "grad_flow_pass = raw_grad < 0  # Should be negative due to GRL\n",
    "print(f\"  Loss = T * 2.0, so \u2202L/\u2202T = 2.0 (positive)\")\n",
    "print(f\"  Without GRL: raw_grad would be positive (decrease T)\")\n",
    "print(f\"  With GRL (\u03bb=0.5): raw_grad = {raw_grad:.4f} (should be negative)\")\n",
    "print(f\"  {'PASS' if grad_flow_pass else 'FAIL'}\")\n",
    "del temp_module_test\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "all_pass = grl_pass and lambda_pass and bounds_pass and grad_flow_pass\n",
    "print(f\"CTKD Component Tests: {'ALL PASS' if all_pass else 'SOME FAILED'}\")\n",
    "if not all_pass:\n",
    "    print(\"WARNING: Fix failing tests before running training!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# cell 6.5: v14.1 FDD (Feature Dynamics Distillation) with CKA Loss\n# =============================================================================\n# References:\n# - CKA: Kornblith et al., \"Similarity of Neural Network Representations Revisited\"\n# - FDD: Feature Dynamics Distillation (view transformer as ODE)\n# - v7 lesson: Hidden alignment with weight=1.0 caused PPL regression to 1655!\n\n# -----------------------------------------------------------------------------\n# Centered Kernel Alignment (CKA) Loss\n# -----------------------------------------------------------------------------\ndef cka_loss(X: torch.Tensor, Y: torch.Tensor, eps: float = 1e-8) -> torch.Tensor:\n    \"\"\"\n    Centered Kernel Alignment (CKA) loss for representation alignment.\n\n    CKA is a similarity measure between feature representations that is:\n    - Invariant to orthogonal transformations\n    - Invariant to isotropic scaling\n    - Does NOT require dimension matching (projector-free!)\n\n    Args:\n        X: Student features [n_samples, dim_x]\n        Y: Teacher features [n_samples, dim_y]\n        eps: Small constant for numerical stability\n\n    Returns:\n        Loss = 1 - CKA (minimize to maximize alignment)\n        CKA = 1 means perfect alignment, CKA = 0 means no alignment\n\n    Note: n_samples must match, but dim_x and dim_y can differ!\n\n    CRITICAL: Uses float32 to prevent overflow in mixed precision training.\n    With n=2048 samples, Gram matrix sums can exceed float16 max (~65504).\n    This is training-time only - does NOT affect student's ternary activations.\n    \"\"\"\n    # CRITICAL: Force float32 to prevent overflow in mixed precision\n    # Under torch.cuda.amp.autocast(), tensors are float16 by default\n    # Gram matrix sums: 4M elements squared and summed -> can exceed 65504\n    with torch.cuda.amp.autocast(enabled=False):\n        X = X.float()\n        Y = Y.float()\n\n        # Validate input shapes\n        assert X.dim() == 2 and Y.dim() == 2, f\"Expected 2D tensors, got X:{X.dim()}D, Y:{Y.dim()}D\"\n        assert X.size(0) == Y.size(0), f\"Sample count mismatch: X={X.size(0)}, Y={Y.size(0)}\"\n\n        # Center the features (critical for CKA)\n        X_centered = X - X.mean(dim=0, keepdim=True)\n        Y_centered = Y - Y.mean(dim=0, keepdim=True)\n\n        # Row-normalize for additional numerical stability\n        # This bounds Gram matrix elements to [-1, 1] range\n        X_norm = X_centered / (X_centered.norm(dim=1, keepdim=True) + eps)\n        Y_norm = Y_centered / (Y_centered.norm(dim=1, keepdim=True) + eps)\n\n        # Compute Gram matrices on normalized features\n        # K_X[i,j] = dot(X_norm[i], X_norm[j]) in [-1, 1]\n        K_X = X_norm @ X_norm.T  # [n, n]\n        K_Y = Y_norm @ Y_norm.T  # [n, n]\n\n        # HSIC (Hilbert-Schmidt Independence Criterion)\n        # Now numerically stable: elements in [-1, 1]^2 = [0, 1]\n        hsic_xy = (K_X * K_Y).sum()\n        hsic_xx = (K_X * K_X).sum()\n        hsic_yy = (K_Y * K_Y).sum()\n\n        # CKA = HSIC(X,Y) / sqrt(HSIC(X,X) * HSIC(Y,Y))\n        cka = hsic_xy / (torch.sqrt(hsic_xx * hsic_yy) + eps)\n\n        # Clamp to valid range (numerical safety)\n        cka = cka.clamp(0.0, 1.0)\n\n        # Return loss (1 - CKA): minimize loss = maximize alignment\n        return 1.0 - cka\n\n\n# -----------------------------------------------------------------------------\n# Layer Mapping for FDD\n# -----------------------------------------------------------------------------\ndef get_fdd_layer_mapping(n_student_layers: int, n_teacher_layers: int,\n                          n_align_layers: int = 3) -> Dict[int, int]:\n    \"\"\"\n    Create layer mapping for Feature Dynamics Distillation.\n\n    Maps student layers to teacher layers for hidden state alignment.\n    Uses even spacing to cover early/middle/late representations.\n\n    Args:\n        n_student_layers: Number of student layers (e.g., 5)\n        n_teacher_layers: Number of teacher layers (e.g., 12)\n        n_align_layers: Number of layer pairs to align (default 3)\n\n    Returns:\n        Dict mapping student_layer_idx -> teacher_layer_idx\n\n    Example for 5 student, 12 teacher, 3 alignments:\n        {0: 2, 2: 7, 4: 11}  # Early, middle, late (with +2 offset)\n    \"\"\"\n    if n_align_layers > n_student_layers:\n        n_align_layers = n_student_layers\n\n    layer_map = {}\n\n    # Evenly space the student layers to align\n    student_indices = []\n    for i in range(n_align_layers):\n        # 0, 2, 4 for 3 alignments with 5 layers\n        idx = int(i * (n_student_layers - 1) / max(n_align_layers - 1, 1))\n        student_indices.append(idx)\n\n    # Map each student index to corresponding teacher layer\n    for s_idx in student_indices:\n        # Scale to teacher layers\n        t_idx = int((s_idx / (n_student_layers - 1)) * (n_teacher_layers - 1))\n        # Offset by 1-2 to avoid embedding layer\n        t_idx = max(2, min(t_idx + 2, n_teacher_layers - 1))\n        layer_map[s_idx] = t_idx\n\n    return layer_map\n\n\n# -----------------------------------------------------------------------------\n# Feature Dynamics Distillation Loss\n# -----------------------------------------------------------------------------\ndef compute_fdd_loss(\n    student_hiddens: List[torch.Tensor],\n    teacher_hiddens: List[torch.Tensor],\n    layer_map: Dict[int, int],\n    loss_type: str = \"cka\"\n) -> torch.Tensor:\n    \"\"\"\n    Compute Feature Dynamics Distillation (FDD) loss.\n\n    FDD views the transformer as solving an ODE: dh/dt = f(h, t)\n    where each layer is a discrete time step.\n\n    Instead of matching hidden states directly (which failed in v7),\n    we match the DYNAMICS (layer-to-layer changes): delta_h = h_{l+1} - h_l\n\n    This teaches the student HOW to transform features, not just WHAT features to have.\n\n    Args:\n        student_hiddens: List of student hidden states [h_0, h_1, ..., h_L]\n                        Each has shape [batch, seq, student_dim]\n        teacher_hiddens: List of teacher hidden states (from output_hidden_states=True)\n                        Each has shape [batch, seq, teacher_dim]\n        layer_map: Dict mapping student_layer_idx -> teacher_layer_idx\n        loss_type: \"cka\" (recommended) or \"mse\"\n\n    Returns:\n        FDD loss (scalar tensor)\n\n    Note: student_hiddens[0] is embedding, student_hiddens[1] is after layer 0, etc.\n    \"\"\"\n    total_loss = torch.tensor(0.0, device=student_hiddens[0].device)\n    n_pairs = 0\n\n    for s_layer, t_layer in layer_map.items():\n        # Validate indices\n        # student_hiddens: [embed, after_L0, after_L1, ..., after_L{n-1}]\n        # So layer i output is at index i+1\n        s_idx = s_layer + 1  # +1 because [0] is embedding\n        t_idx = t_layer + 1  # Same for teacher\n\n        # Check bounds\n        if s_idx + 1 >= len(student_hiddens):\n            continue\n        if t_idx + 1 >= len(teacher_hiddens):\n            continue\n\n        # Compute dynamics (velocity): delta_h = h_{l+1} - h_l\n        # Student dynamics: change from layer s_layer to s_layer+1\n        delta_s = student_hiddens[s_idx + 1] - student_hiddens[s_idx]  # [batch, seq, s_dim]\n\n        # Teacher dynamics: change from layer t_layer to t_layer+1\n        delta_t = teacher_hiddens[t_idx + 1] - teacher_hiddens[t_idx]  # [batch, seq, t_dim]\n\n        # Flatten for CKA: [batch, seq, dim] -> [batch*seq, dim]\n        batch_size, seq_len = delta_s.size(0), delta_s.size(1)\n        delta_s_flat = delta_s.reshape(batch_size * seq_len, -1)  # [n, s_dim]\n        delta_t_flat = delta_t.reshape(batch_size * seq_len, -1)  # [n, t_dim]\n\n        # Compute loss\n        if loss_type == \"cka\":\n            pair_loss = cka_loss(delta_s_flat, delta_t_flat)\n        elif loss_type == \"mse\":\n            # MSE requires dimension matching - use projector if needed\n            # For now, skip if dimensions don't match\n            if delta_s_flat.size(1) != delta_t_flat.size(1):\n                continue\n            pair_loss = F.mse_loss(delta_s_flat, delta_t_flat)\n        else:\n            raise ValueError(f\"Unknown loss_type: {loss_type}\")\n\n        total_loss = total_loss + pair_loss\n        n_pairs += 1\n\n    # Average over pairs\n    if n_pairs > 0:\n        total_loss = total_loss / n_pairs\n\n    return total_loss\n\n\n# -----------------------------------------------------------------------------\n# FDD Weight Scheduler (with warmup)\n# -----------------------------------------------------------------------------\ndef get_fdd_weight(step: int, fdd_warmup_steps: int, fdd_weight: float) -> float:\n    \"\"\"\n    Get FDD weight for current step with warmup.\n\n    Args:\n        step: Current training step\n        fdd_warmup_steps: Steps before FDD kicks in\n        fdd_weight: Maximum FDD weight\n\n    Returns:\n        Current FDD weight (0 during warmup, then fdd_weight)\n    \"\"\"\n    if step < fdd_warmup_steps:\n        return 0.0\n    return fdd_weight\n\n\n# -----------------------------------------------------------------------------\n# FDD Unit Tests\n# -----------------------------------------------------------------------------\nprint(\"=\"*60)\nprint(\"v14.1 FDD Component Tests\")\nprint(\"=\"*60)\n\nfdd_tests = []\n\n# Test 1: CKA of identical tensors should return 0 loss (CKA=1)\nprint(\"\\n[1] CKA Identical Tensors Test\")\nX_test = torch.randn(100, 64)\ncka_identical = cka_loss(X_test, X_test)\nidentical_pass = cka_identical.item() < 0.01  # Should be ~0\nprint(f\"  CKA loss of identical tensors: {cka_identical.item():.6f}\")\nprint(f\"  Expected: ~0.0 (CKA=1 means perfect alignment)\")\nprint(f\"  {'PASS' if identical_pass else 'FAIL'}\")\nfdd_tests.append(('CKA identical', identical_pass))\n\n# Test 2: CKA of orthogonal tensors should return high loss\nprint(\"\\n[2] CKA Orthogonal Tensors Test\")\nX_orth = torch.randn(100, 64)\nY_orth = torch.randn(100, 64)  # Different random = nearly orthogonal\ncka_orthogonal = cka_loss(X_orth, Y_orth)\northogonal_pass = 0.5 < cka_orthogonal.item() <= 1.0  # Should be high\nprint(f\"  CKA loss of orthogonal tensors: {cka_orthogonal.item():.4f}\")\nprint(f\"  Expected: 0.5-1.0 (low alignment)\")\nprint(f\"  {'PASS' if orthogonal_pass else 'FAIL'}\")\nfdd_tests.append(('CKA orthogonal', orthogonal_pass))\n\n# Test 3: CKA handles different dimensions\nprint(\"\\n[3] CKA Dimension Agnostic Test\")\nX_small = torch.randn(100, 32)   # 32 dims\nY_large = torch.randn(100, 128)  # 128 dims\ntry:\n    cka_diff_dim = cka_loss(X_small, Y_large)\n    dim_pass = True\n    print(f\"  CKA with dims (32, 128): {cka_diff_dim.item():.4f}\")\nexcept Exception as e:\n    dim_pass = False\n    print(f\"  ERROR: {e}\")\nprint(f\"  {'PASS' if dim_pass else 'FAIL'}\")\nfdd_tests.append(('CKA dimension agnostic', dim_pass))\n\n# Test 4: Layer mapping correctness\nprint(\"\\n[4] Layer Mapping Test\")\nlayer_map = get_fdd_layer_mapping(n_student_layers=5, n_teacher_layers=12, n_align_layers=3)\n# Actual computation: s_idx=0 -> t_idx=0+2=2, s_idx=2 -> t_idx=5+2=7, s_idx=4 -> t_idx=11 (clamped)\nexpected_map = {0: 2, 2: 7, 4: 11}  # Corrected expectation\nmap_pass = layer_map == expected_map\nprint(f\"  Generated map: {layer_map}\")\nprint(f\"  Expected map: {expected_map}\")\nprint(f\"  {'PASS' if map_pass else 'FAIL'}\")\nfdd_tests.append(('Layer mapping', map_pass))\n\n# Test 5: FDD loss computation\nprint(\"\\n[5] FDD Loss Computation Test\")\n# Mock hidden states\nstudent_hiddens_mock = [torch.randn(2, 16, 320) for _ in range(6)]  # embed + 5 layers\nteacher_hiddens_mock = [torch.randn(2, 16, 768) for _ in range(13)]  # embed + 12 layers\ntry:\n    fdd_loss_val = compute_fdd_loss(\n        student_hiddens_mock,\n        teacher_hiddens_mock,\n        layer_map,\n        loss_type=\"cka\"\n    )\n    fdd_pass = 0.0 <= fdd_loss_val.item() <= 1.0\n    print(f\"  FDD loss: {fdd_loss_val.item():.4f}\")\n    print(f\"  Expected: [0, 1]\")\nexcept Exception as e:\n    fdd_pass = False\n    print(f\"  ERROR: {e}\")\nprint(f\"  {'PASS' if fdd_pass else 'FAIL'}\")\nfdd_tests.append(('FDD loss computation', fdd_pass))\n\n# Test 6: FDD weight scheduler\nprint(\"\\n[6] FDD Weight Scheduler Test\")\nw_0 = get_fdd_weight(0, 500, 0.1)\nw_400 = get_fdd_weight(400, 500, 0.1)\nw_500 = get_fdd_weight(500, 500, 0.1)\nw_1000 = get_fdd_weight(1000, 500, 0.1)\nscheduler_pass = (w_0 == 0.0 and w_400 == 0.0 and w_500 == 0.1 and w_1000 == 0.1)\nprint(f\"  weight(0): {w_0} (should be 0)\")\nprint(f\"  weight(400): {w_400} (should be 0)\")\nprint(f\"  weight(500): {w_500} (should be 0.001)\")\nprint(f\"  weight(1000): {w_1000} (should be 0.001)\")\nprint(f\"  {'PASS' if scheduler_pass else 'FAIL'}\")\nfdd_tests.append(('FDD weight scheduler', scheduler_pass))\n\n# Test 7: CKA float32 stability test (simulates mixed precision)\nprint(\"\\n[7] CKA Float32 Stability Test\")\n# Simulate large values that would overflow in float16\nX_large = torch.randn(2048, 320) * 100  # Large values\nY_large = torch.randn(2048, 768) * 100\ntry:\n    cka_large = cka_loss(X_large, Y_large)\n    stability_pass = not (torch.isnan(cka_large) or torch.isinf(cka_large))\n    print(f\"  CKA with large values (n=2048): {cka_large.item():.4f}\")\n    print(f\"  No NaN/Inf: {stability_pass}\")\nexcept Exception as e:\n    stability_pass = False\n    print(f\"  ERROR: {e}\")\nprint(f\"  {'PASS' if stability_pass else 'FAIL'}\")\nfdd_tests.append(('CKA float32 stability', stability_pass))\n\n# Summary\nprint(\"\\n\" + \"=\"*60)\nall_fdd_pass = all(p for _, p in fdd_tests)\nprint(f\"FDD Component Tests: {'ALL PASS' if all_fdd_pass else 'SOME FAILED'}\")\nif not all_fdd_pass:\n    failed = [n for n, p in fdd_tests if not p]\n    print(f\"FAILED: {failed}\")\nprint(\"=\"*60)\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# cell 7: v13 POCL (Progressive Overload Curriculum Learning)\n",
    "# =============================================================================\n",
    "# Reference: \"POCL: Progressive Overload Curriculum Learning\" (2025)\n",
    "# arXiv:2506.05695\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Sample Difficulty Scoring\n",
    "# -----------------------------------------------------------------------------\n",
    "def compute_sample_difficulty(student, teacher, dataloader, device, max_batches=50):\n",
    "    \"\"\"\n",
    "    Compute difficulty scores for each sample using student-teacher divergence.\n",
    "\n",
    "    Difficulty = average (CE loss + KL divergence) per sample.\n",
    "    Higher score = harder sample for the student.\n",
    "\n",
    "    Uses a small pre-trained student to get meaningful gradients.\n",
    "\n",
    "    Args:\n",
    "        student: Student model (should be briefly pre-trained)\n",
    "        teacher: Teacher model (frozen)\n",
    "        dataloader: Training data loader\n",
    "        device: Compute device\n",
    "        max_batches: Limit batches for efficiency\n",
    "\n",
    "    Returns:\n",
    "        Dict with sample indices and difficulty scores\n",
    "    \"\"\"\n",
    "    student.eval()\n",
    "    teacher.eval()\n",
    "\n",
    "    all_difficulties = []\n",
    "    all_indices = []\n",
    "    sample_idx = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(dataloader):\n",
    "            if batch_idx >= max_batches:\n",
    "                break\n",
    "\n",
    "            ids = batch[0].to(device, non_blocking=True)\n",
    "            batch_size = ids.size(0)\n",
    "\n",
    "            # Get logits\n",
    "            s_logits = student(ids)\n",
    "            t_logits = teacher(ids).logits\n",
    "\n",
    "            # Per-sample difficulty (average over sequence)\n",
    "            # 1. Cross-entropy with teacher as target\n",
    "            s_probs = F.softmax(s_logits, dim=-1)\n",
    "            t_probs = F.softmax(t_logits, dim=-1)\n",
    "\n",
    "            # KL divergence per sample\n",
    "            kl_div = F.kl_div(\n",
    "                F.log_softmax(s_logits, dim=-1),\n",
    "                t_probs,\n",
    "                reduction='none'\n",
    "            ).sum(dim=-1).mean(dim=-1)  # [batch_size]\n",
    "\n",
    "            # Cross-entropy per sample (using teacher hard targets)\n",
    "            t_tokens = t_logits.argmax(dim=-1)\n",
    "            ce_loss = F.cross_entropy(\n",
    "                s_logits.view(-1, s_logits.size(-1)),\n",
    "                t_tokens.view(-1),\n",
    "                reduction='none'\n",
    "            ).view(batch_size, -1).mean(dim=-1)  # [batch_size]\n",
    "\n",
    "            # Combined difficulty\n",
    "            difficulty = kl_div + ce_loss  # [batch_size]\n",
    "\n",
    "            all_difficulties.extend(difficulty.cpu().tolist())\n",
    "            all_indices.extend(range(sample_idx, sample_idx + batch_size))\n",
    "            sample_idx += batch_size\n",
    "\n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f\"  Scoring batch {batch_idx+1}/{max_batches}...\")\n",
    "\n",
    "    student.train()\n",
    "\n",
    "    return {\n",
    "        'indices': all_indices,\n",
    "        'difficulties': all_difficulties,\n",
    "        'num_samples': len(all_indices)\n",
    "    }\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Data Partitioning by Difficulty\n",
    "# -----------------------------------------------------------------------------\n",
    "def partition_by_difficulty(difficulties_dict, n_stages=3):\n",
    "    \"\"\"\n",
    "    Partition data into stages by difficulty (easy -> hard).\n",
    "\n",
    "    Stage 1: Easiest 33%\n",
    "    Stage 2: Easiest 66% (includes stage 1)\n",
    "    Stage 3: All 100% (includes stages 1+2)\n",
    "\n",
    "    Args:\n",
    "        difficulties_dict: Output from compute_sample_difficulty()\n",
    "        n_stages: Number of stages (default 3)\n",
    "\n",
    "    Returns:\n",
    "        List of index lists, one per stage (cumulative)\n",
    "    \"\"\"\n",
    "    indices = difficulties_dict['indices']\n",
    "    difficulties = difficulties_dict['difficulties']\n",
    "\n",
    "    # Sort by difficulty (ascending = easy first)\n",
    "    sorted_pairs = sorted(zip(indices, difficulties), key=lambda x: x[1])\n",
    "    sorted_indices = [idx for idx, _ in sorted_pairs]\n",
    "\n",
    "    n = len(sorted_indices)\n",
    "    stage_indices = []\n",
    "\n",
    "    for stage in range(n_stages):\n",
    "        # Cumulative: stage 1 = 33%, stage 2 = 66%, stage 3 = 100%\n",
    "        end_idx = int(n * (stage + 1) / n_stages)\n",
    "        stage_indices.append(sorted_indices[:end_idx])\n",
    "\n",
    "    return stage_indices\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Brief Pre-training for Difficulty Scoring\n",
    "# -----------------------------------------------------------------------------\n",
    "def pretrain_for_difficulty_scoring(student, teacher, train_loader, cfg, device, steps=100):\n",
    "    \"\"\"\n",
    "    Brief pre-training so difficulty scores are meaningful.\n",
    "\n",
    "    Without pre-training, student predictions are random garbage,\n",
    "    making all samples appear equally difficult.\n",
    "\n",
    "    Args:\n",
    "        student: Student model\n",
    "        teacher: Teacher model (frozen)\n",
    "        train_loader: Training data loader\n",
    "        cfg: Config object\n",
    "        device: Compute device\n",
    "        steps: Number of pre-training steps\n",
    "\n",
    "    Returns:\n",
    "        Student model (modified in-place)\n",
    "    \"\"\"\n",
    "    print(f\"Pre-training student for {steps} steps (for difficulty scoring)...\")\n",
    "\n",
    "    optimizer = torch.optim.AdamW(student.parameters(), lr=cfg.distill_lr, weight_decay=0.01)\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    student.train()\n",
    "    teacher.eval()\n",
    "\n",
    "    step = 0\n",
    "    pbar = tqdm(total=steps, desc='Pre-training')\n",
    "\n",
    "    for batch in train_loader:\n",
    "        if step >= steps:\n",
    "            break\n",
    "\n",
    "        ids = batch[0].to(device, non_blocking=True)\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            with torch.no_grad():\n",
    "                t_logits = teacher(ids).logits\n",
    "\n",
    "            s_logits = student(ids)\n",
    "\n",
    "            # Simple KL loss (no temperature complexity)\n",
    "            T = 2.0\n",
    "            s_log = F.log_softmax(s_logits / T, dim=-1)\n",
    "            t_prob = F.softmax(t_logits / T, dim=-1)\n",
    "            loss = F.kl_div(\n",
    "                s_log.view(-1, s_logits.size(-1)),\n",
    "                t_prob.view(-1, t_logits.size(-1)),\n",
    "                reduction='batchmean'\n",
    "            ) * (T ** 2)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        step += 1\n",
    "        pbar.update(1)\n",
    "        if step % 20 == 0:\n",
    "            pbar.set_postfix(loss=f\"{loss.item():.3f}\")\n",
    "\n",
    "    pbar.close()\n",
    "    print(f\"Pre-training complete. Final loss: {loss.item():.3f}\")\n",
    "\n",
    "    return student\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Get Stage Temperature (Fixed Schedule)\n",
    "# -----------------------------------------------------------------------------\n",
    "def get_pocl_temperature(step, total_steps, temp_schedule, n_stages=3):\n",
    "    \"\"\"\n",
    "    Get temperature for current POCL stage.\n",
    "\n",
    "    Args:\n",
    "        step: Current training step\n",
    "        total_steps: Total training steps\n",
    "        temp_schedule: Tuple of temperatures per stage (e.g., (1.0, 1.5, 2.0))\n",
    "        n_stages: Number of stages\n",
    "\n",
    "    Returns:\n",
    "        Temperature for current stage\n",
    "    \"\"\"\n",
    "    current_stage = get_pocl_stage(step, total_steps, n_stages)\n",
    "    return temp_schedule[current_stage]\n",
    "\n",
    "\n",
    "def get_pocl_stage(step, total_steps, n_stages=3):\n",
    "    \"\"\"\n",
    "    Get current POCL stage (0-indexed).\n",
    "\n",
    "    Uses rounded boundaries to ensure even distribution:\n",
    "    - 5000 steps, 3 stages: boundaries at 1667, 3333\n",
    "    - Stage 0: steps 0-1666\n",
    "    - Stage 1: steps 1667-3332\n",
    "    - Stage 2: steps 3333-4999\n",
    "    \"\"\"\n",
    "    for i in range(n_stages - 1):\n",
    "        boundary = round((i + 1) * total_steps / n_stages)\n",
    "        if step < boundary:\n",
    "            return i\n",
    "    return n_stages - 1\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# POCL Unit Tests\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"=\"*60)\n",
    "print(\"v13 POCL Component Tests\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Test 1: Temperature Schedule\n",
    "print(\"\\n[1] Temperature Schedule Test\")\n",
    "temp_schedule = (1.0, 1.5, 2.0)\n",
    "total = 5000\n",
    "\n",
    "t_start = get_pocl_temperature(0, total, temp_schedule)\n",
    "t_stage1_end = get_pocl_temperature(1666, total, temp_schedule)  # End of stage 1\n",
    "t_stage2_start = get_pocl_temperature(1667, total, temp_schedule)  # Start of stage 2\n",
    "t_stage2_end = get_pocl_temperature(3332, total, temp_schedule)\n",
    "t_stage3 = get_pocl_temperature(4000, total, temp_schedule)\n",
    "\n",
    "temp_pass = (t_start == 1.0 and t_stage1_end == 1.0 and t_stage2_start == 1.5 and t_stage3 == 2.0)\n",
    "print(f\"  T(0) = {t_start} (should be 1.0)\")\n",
    "print(f\"  T(1666) = {t_stage1_end} (should be 1.0, end of stage 1)\")\n",
    "print(f\"  T(1667) = {t_stage2_start} (should be 1.5, start of stage 2)\")\n",
    "print(f\"  T(4000) = {t_stage3} (should be 2.0, stage 3)\")\n",
    "print(f\"  {'PASS' if temp_pass else 'FAIL'}\")\n",
    "\n",
    "# Test 2: Stage Boundaries\n",
    "print(\"\\n[2] Stage Boundaries Test\")\n",
    "stages = [get_pocl_stage(s, total) for s in [0, 1666, 1667, 3332, 3333, 4999]]\n",
    "stage_pass = stages == [0, 0, 1, 1, 2, 2]\n",
    "print(f\"  Stages at [0, 1666, 1667, 3332, 3333, 4999]: {stages}\")\n",
    "print(f\"  Expected: [0, 0, 1, 1, 2, 2]\")\n",
    "print(f\"  {'PASS' if stage_pass else 'FAIL'}\")\n",
    "\n",
    "# Test 3: Partition by Difficulty (mock)\n",
    "print(\"\\n[3] Partition by Difficulty Test (mock data)\")\n",
    "mock_difficulties = {\n",
    "    'indices': list(range(9)),\n",
    "    'difficulties': [0.5, 1.5, 0.3, 2.0, 0.8, 1.2, 2.5, 0.1, 1.8],  # Easy: 7,2,0,4 | Med: 5,1 | Hard: 8,3,6\n",
    "    'num_samples': 9\n",
    "}\n",
    "partitions = partition_by_difficulty(mock_difficulties, n_stages=3)\n",
    "# After sorting: [7(0.1), 2(0.3), 0(0.5), 4(0.8), 5(1.2), 1(1.5), 8(1.8), 3(2.0), 6(2.5)]\n",
    "# Stage 1 (33%): indices [7, 2, 0] -> 3 samples\n",
    "# Stage 2 (66%): indices [7, 2, 0, 4, 5, 1] -> 6 samples\n",
    "# Stage 3 (100%): all 9 samples\n",
    "partition_pass = (len(partitions[0]) == 3 and len(partitions[1]) == 6 and len(partitions[2]) == 9)\n",
    "print(f\"  Stage 1 samples: {len(partitions[0])} (should be 3)\")\n",
    "print(f\"  Stage 2 samples: {len(partitions[1])} (should be 6)\")\n",
    "print(f\"  Stage 3 samples: {len(partitions[2])} (should be 9)\")\n",
    "print(f\"  Cumulative check: {partitions[0][0] in partitions[1] and partitions[1][0] in partitions[2]}\")\n",
    "print(f\"  {'PASS' if partition_pass else 'FAIL'}\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "all_pass = temp_pass and stage_pass and partition_pass\n",
    "print(f\"POCL Component Tests: {'ALL PASS' if all_pass else 'SOME FAILED'}\")\n",
    "if not all_pass:\n",
    "    print(\"WARNING: Fix failing tests before running training!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# cell 7: hardware and spike stats collectors (same as v9)\n",
    "# =============================================================================\n",
    "class HardwareStatsCollector:\n",
    "    \"\"\"collect gpu memory, timing, and throughput metrics.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.gpu_memory_history = []\n",
    "        self.step_times = []\n",
    "        self.tokens_processed = 0\n",
    "        self.start_time = None\n",
    "\n",
    "    def start(self):\n",
    "        self.start_time = time.time()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "    def record_step(self, batch_size: int, seq_len: int):\n",
    "        if torch.cuda.is_available():\n",
    "            self.gpu_memory_history.append(torch.cuda.memory_allocated() / 1e9)\n",
    "        self.tokens_processed += batch_size * seq_len\n",
    "        self.step_times.append(time.time())\n",
    "\n",
    "    def get_throughput(self) -> float:\n",
    "        if len(self.step_times) < 2:\n",
    "            return 0.0\n",
    "        elapsed = self.step_times[-1] - self.step_times[0]\n",
    "        return self.tokens_processed / elapsed if elapsed > 0 else 0.0\n",
    "\n",
    "    def get_summary(self) -> Dict[str, Any]:\n",
    "        elapsed = time.time() - self.start_time if self.start_time else 0\n",
    "        return {\n",
    "            'peak_gpu_memory_gb': max(self.gpu_memory_history) if self.gpu_memory_history else 0,\n",
    "            'avg_gpu_memory_gb': float(np.mean(self.gpu_memory_history)) if self.gpu_memory_history else 0,\n",
    "            'total_training_time_s': elapsed,\n",
    "            'total_training_time_min': elapsed / 60,\n",
    "            'tokens_processed': self.tokens_processed,\n",
    "            'throughput_tokens_per_sec': self.get_throughput(),\n",
    "        }\n",
    "\n",
    "\n",
    "class SpikeStatsCollector:\n",
    "    \"\"\"collect per-layer spike density and amplitude evolution.\"\"\"\n",
    "\n",
    "    def __init__(self, n_layers: int):\n",
    "        self.n_layers = n_layers\n",
    "        self.density_history = {i: {'k': [], 'v': []} for i in range(n_layers)}\n",
    "        self.amplitude_history = {i: {'k': [], 'v': []} for i in range(n_layers)}\n",
    "        self.step_densities = []\n",
    "\n",
    "    def record(self, student, step: int):\n",
    "        stats = student.get_spike_stats()\n",
    "        all_densities = []\n",
    "        for i in range(self.n_layers):\n",
    "            layer_key = f'layer_{i}'\n",
    "            if layer_key in stats:\n",
    "                k_density = stats[layer_key].get('k', 0)\n",
    "                v_density = stats[layer_key].get('v', 0)\n",
    "                k_amp = stats[layer_key].get('k_amp', 1.0)\n",
    "                v_amp = stats[layer_key].get('v_amp', 1.0)\n",
    "\n",
    "                self.density_history[i]['k'].append(k_density)\n",
    "                self.density_history[i]['v'].append(v_density)\n",
    "                self.amplitude_history[i]['k'].append(k_amp)\n",
    "                self.amplitude_history[i]['v'].append(v_amp)\n",
    "                all_densities.extend([k_density, v_density])\n",
    "\n",
    "        if all_densities:\n",
    "            self.step_densities.append({'step': step, 'density': float(np.mean(all_densities))})\n",
    "\n",
    "    def get_summary(self) -> Dict[str, Any]:\n",
    "        per_layer = {}\n",
    "        all_k, all_v = [], []\n",
    "        all_k_amp, all_v_amp = [], []\n",
    "\n",
    "        for i in range(self.n_layers):\n",
    "            k_vals = self.density_history[i]['k']\n",
    "            v_vals = self.density_history[i]['v']\n",
    "            k_amps = self.amplitude_history[i]['k']\n",
    "            v_amps = self.amplitude_history[i]['v']\n",
    "\n",
    "            per_layer[f'layer_{i}'] = {\n",
    "                'k_mean': float(np.mean(k_vals)) if k_vals else 0,\n",
    "                'k_std': float(np.std(k_vals)) if k_vals else 0,\n",
    "                'k_final': float(k_vals[-1]) if k_vals else 0,\n",
    "                'v_mean': float(np.mean(v_vals)) if v_vals else 0,\n",
    "                'v_std': float(np.std(v_vals)) if v_vals else 0,\n",
    "                'v_final': float(v_vals[-1]) if v_vals else 0,\n",
    "                'k_amp_final': float(k_amps[-1]) if k_amps else 1.0,\n",
    "                'v_amp_final': float(v_amps[-1]) if v_amps else 1.0,\n",
    "            }\n",
    "            all_k.extend(k_vals)\n",
    "            all_v.extend(v_vals)\n",
    "            if k_amps: all_k_amp.append(k_amps[-1])\n",
    "            if v_amps: all_v_amp.append(v_amps[-1])\n",
    "\n",
    "        return {\n",
    "            'per_layer': per_layer,\n",
    "            'overall_k_density': float(np.mean(all_k)) if all_k else 0,\n",
    "            'overall_v_density': float(np.mean(all_v)) if all_v else 0,\n",
    "            'overall_density': float(np.mean(all_k + all_v)) if (all_k or all_v) else 0,\n",
    "            'amplitudes': {'k': all_k_amp, 'v': all_v_amp},\n",
    "            'density_history': self.step_densities,\n",
    "        }\n",
    "\n",
    "print(\"collectors defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# cell 8: spiking goose model (v14 - channel-wise spikes + gradient checkpointing)\n# =============================================================================\nclass SpikingGooseRecurrentLayer(nn.Module):\n    \"\"\"\n    RWKV-style recurrence with trainable ternary spiking.\n    \n    Supports channel-wise ternary spikes (when use_channel_wise=True)\n    \"\"\"\n\n    def __init__(self, d_model, layer_idx=0, n_layers=4, spike_alpha=1.0, \n                 use_channel_wise: bool = False):\n        super().__init__()\n        self.d_model = d_model\n        self.layer_idx = layer_idx\n        self.use_channel_wise = use_channel_wise\n        self.ln = nn.LayerNorm(d_model)\n\n        ratio = layer_idx / max(n_layers - 1, 1)\n        self.time_mix_k = nn.Parameter(torch.ones(d_model) * (1 - ratio))\n        self.time_mix_v = nn.Parameter(torch.ones(d_model) * (1 - ratio))\n        self.time_mix_r = nn.Parameter(torch.ones(d_model) * (1 - ratio))\n        self.decay_weight = nn.Parameter(torch.zeros(d_model) - 0.5)\n\n        self.key_proj = nn.Linear(d_model, d_model, bias=False)\n        self.value_proj = nn.Linear(d_model, d_model, bias=False)\n        self.receptance_proj = nn.Linear(d_model, d_model, bias=False)\n        self.output_proj = nn.Linear(d_model, d_model, bias=False)\n\n        # v14: Use channel-wise spikes if enabled\n        if use_channel_wise:\n            self.k_spike = ChannelWiseTernarySpike(d_model, alpha_init=spike_alpha)\n            self.v_spike = ChannelWiseTernarySpike(d_model, alpha_init=spike_alpha)\n        else:\n            self.k_spike = TrainableTernarySpike(alpha=spike_alpha)\n            self.v_spike = TrainableTernarySpike(alpha=spike_alpha)\n\n        self.register_buffer('running_k_density', torch.tensor(0.0))\n        self.register_buffer('running_v_density', torch.tensor(0.0))\n        self._init_weights()\n\n    def _init_weights(self):\n        std = 0.1 / math.sqrt(self.d_model)\n        for m in [self.key_proj, self.value_proj, self.receptance_proj, self.output_proj]:\n            nn.init.normal_(m.weight, std=std)\n\n    def forward(self, x):\n        B, T, D = x.shape\n        x_norm = self.ln(x)\n        prev_x = F.pad(x_norm[:, :-1, :], (0, 0, 1, 0))\n\n        xk = x_norm * self.time_mix_k + prev_x * (1 - self.time_mix_k)\n        xv = x_norm * self.time_mix_v + prev_x * (1 - self.time_mix_v)\n        xr = x_norm * self.time_mix_r + prev_x * (1 - self.time_mix_r)\n\n        k_pre = self.key_proj(xk)\n        v_pre = self.value_proj(xv)\n\n        k = self.k_spike(k_pre)\n        v = self.v_spike(v_pre)\n        r = torch.sigmoid(self.receptance_proj(xr))\n\n        kv = k * v\n        decay = torch.sigmoid(self.decay_weight)\n        t_idx = torch.arange(T, device=x.device, dtype=x.dtype)\n        decay_powers = decay.unsqueeze(0) ** t_idx.unsqueeze(1)\n\n        kv_weighted = kv / (decay_powers.unsqueeze(0) + 1e-8)\n        S = torch.cumsum(kv_weighted, dim=1) * decay_powers.unsqueeze(0)\n\n        if self.training:\n            with torch.no_grad():\n                self.running_k_density = 0.99 * self.running_k_density + 0.01 * (k != 0).float().mean()\n                self.running_v_density = 0.99 * self.running_v_density + 0.01 * (v != 0).float().mean()\n\n        return x + r * self.output_proj(S)\n\n    def get_spike_density(self):\n        return {\n            'k': self.running_k_density.item(),\n            'v': self.running_v_density.item(),\n            'k_amp': self.k_spike.get_amplitude(),\n            'v_amp': self.v_spike.get_amplitude(),\n        }\n    \n    def get_channel_wise_stats(self) -> dict:\n        \"\"\"Get channel-wise spike statistics (only available if use_channel_wise=True).\"\"\"\n        if self.use_channel_wise:\n            return {\n                'k': self.k_spike.get_stats(),\n                'v': self.v_spike.get_stats(),\n            }\n        return None\n\n\nclass GooseFFN(nn.Module):\n    def __init__(self, d_model, expand=4):\n        super().__init__()\n        self.ln = nn.LayerNorm(d_model)\n        self.w1 = nn.Linear(d_model, d_model * expand, bias=False)\n        self.w2 = nn.Linear(d_model * expand, d_model, bias=False)\n\n    def forward(self, x):\n        return x + self.w2(F.silu(self.w1(self.ln(x))))\n\n\nclass StudentSpikingGoose(nn.Module):\n    \"\"\"\n    Spiking student model with trainable ternary activations.\n    \n    Supports channel-wise ternary spikes + gradient checkpointing.\n    \"\"\"\n\n    def __init__(self, cfg, use_checkpointing=True):\n        super().__init__()\n        self.cfg = cfg\n        self.use_checkpointing = use_checkpointing and USE_GRADIENT_CHECKPOINTING\n        \n        # v14: Check for channel-wise spikes flag\n        use_channel_wise = getattr(cfg, 'use_channel_wise_spikes', False)\n        \n        self.embed = nn.Embedding(cfg.vocab_size, cfg.d_model)\n        self.pos_embed = nn.Embedding(cfg.max_seq_len, cfg.d_model)\n\n        self.layers = nn.ModuleList([\n            nn.ModuleDict({\n                'rec': SpikingGooseRecurrentLayer(\n                    cfg.d_model, i, cfg.n_layers, cfg.spike_alpha,\n                    use_channel_wise=use_channel_wise\n                ),\n                'ffn': GooseFFN(cfg.d_model),\n            })\n            for i in range(cfg.n_layers)\n        ])\n\n        self.ln_out = nn.LayerNorm(cfg.d_model)\n        self.head = nn.Linear(cfg.d_model, cfg.vocab_size, bias=False)\n        self.head.weight = self.embed.weight\n\n        nn.init.normal_(self.embed.weight, std=0.02)\n        nn.init.normal_(self.pos_embed.weight, std=0.02)\n\n    def _layer_forward(self, layer, x):\n        \"\"\"helper for gradient checkpointing - processes one layer.\"\"\"\n        x = layer['rec'](x)\n        x = layer['ffn'](x)\n        return x\n\n    def forward(self, input_ids, return_hiddens=False):\n        \"\"\"forward pass with optional hidden state return for alignment.\"\"\"\n        B, T = input_ids.shape\n        pos = torch.arange(T, device=input_ids.device).unsqueeze(0)\n        x = self.embed(input_ids) + self.pos_embed(pos)\n\n        hiddens = [x] if return_hiddens else None\n\n        for layer in self.layers:\n            if self.use_checkpointing and self.training:\n                # v10: gradient checkpointing with use_reentrant=False (recommended)\n                x = checkpoint(self._layer_forward, layer, x, use_reentrant=False)\n            else:\n                x = self._layer_forward(layer, x)\n            \n            if return_hiddens:\n                hiddens.append(x)\n\n        logits = self.head(self.ln_out(x))\n\n        if return_hiddens:\n            return logits, hiddens\n        return logits\n\n    def get_spike_stats(self):\n        return {f'layer_{i}': layer['rec'].get_spike_density() for i, layer in enumerate(self.layers)}\n\n    def get_avg_spike_density(self):\n        densities = []\n        for layer in self.layers:\n            d = layer['rec'].get_spike_density()\n            densities.extend([d['k'], d['v']])\n        return float(np.mean(densities)) if densities else 0.0\n\n    def get_amplitudes(self):\n        return {f'layer_{i}': {'k': layer['rec'].k_spike.get_amplitude(), 'v': layer['rec'].v_spike.get_amplitude()}\n                for i, layer in enumerate(self.layers)}\n    \n    def get_channel_amplitude_variance(self) -> float:\n        \"\"\"Get total variance of channel-wise amplitudes (for regularization).\"\"\"\n        total_var = 0.0\n        for layer in self.layers:\n            rec = layer['rec']\n            if hasattr(rec.k_spike, 'amplitude') and rec.k_spike.amplitude.numel() > 1:\n                total_var += rec.k_spike.amplitude.var().item()\n                total_var += rec.v_spike.amplitude.var().item()\n        return total_var\n\nprint(\"student model defined (v14: channel-wise spikes + gradient checkpointing)\")\nprint(f\"  gradient checkpointing: {USE_GRADIENT_CHECKPOINTING}\")\nprint(f\"  channel-wise spikes: {config.use_channel_wise_spikes}\")\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# cell 9: hidden-state projector + v14.1 FDD layer mapping\n# =============================================================================\nclass HiddenStateProjector(nn.Module):\n    \"\"\"\n    Project student hidden states to teacher dimension for alignment.\n\n    student: (B, T, 320) -> (B, T, 768)\n\n    Maps student layers to selected teacher layers.\n\n    NOTE: This is kept for infrastructure but hidden alignment is DISABLED.\n    v14 uses FDD with CKA loss which is projector-free.\n    \"\"\"\n\n    def __init__(self, student_dim: int, teacher_dim: int, n_student_layers: int):\n        super().__init__()\n        self.projectors = nn.ModuleList([\n            nn.Linear(student_dim, teacher_dim, bias=False)\n            for _ in range(n_student_layers)\n        ])\n        for proj in self.projectors:\n            nn.init.normal_(proj.weight, std=0.02)\n\n    def forward(self, student_hidden: torch.Tensor, layer_idx: int) -> torch.Tensor:\n        return self.projectors[layer_idx](student_hidden)\n\n\ndef compute_hidden_alignment_loss(\n    teacher_hiddens: List[torch.Tensor],\n    student_hiddens: List[torch.Tensor],\n    projector: HiddenStateProjector,\n    teacher_layers: int = 12,\n    student_layers: int = 8\n) -> torch.Tensor:\n    \"\"\"\n    Compute MSE loss between projected student and teacher hidden states.\n\n    NOTE: This is DISABLED in v14 (hidden_align_weight=0.0).\n    v14 uses FDD with CKA loss instead.\n    \"\"\"\n    # Map student layers to teacher layers\n    teacher_indices = [1, 2, 4, 5, 7, 8, 10, 11]\n\n    total_loss = 0.0\n    for s_idx, t_idx in enumerate(teacher_indices):\n        if s_idx >= len(student_hiddens) - 1:\n            break\n        if t_idx >= len(teacher_hiddens):\n            break\n\n        s_hidden = student_hiddens[s_idx + 1]\n        t_hidden = teacher_hiddens[t_idx]\n\n        s_proj = projector(s_hidden, s_idx)\n        total_loss = total_loss + F.mse_loss(s_proj, t_hidden)\n\n    return total_loss / len(teacher_indices)\n\n\n# Create projector (even if disabled, keeps infrastructure)\nprojector = HiddenStateProjector(\n    student_dim=config.d_model,\n    teacher_dim=config.teacher_d_model,\n    n_student_layers=config.n_layers\n).to(DEVICE)\n\nprojector_params = sum(p.numel() for p in projector.parameters())\nprint(f\"hidden-state projector: {projector_params:,} params\")\nprint(f\"  student dim: {config.d_model}\")\nprint(f\"  teacher dim: {config.teacher_d_model}\")\nprint(f\"  student layers: {config.n_layers}\")\nprint(f\"  hidden_align_weight: {config.hidden_align_weight}\")\nprint(f\"  STATUS: DISABLED (v14 uses FDD with CKA instead)\")\n\n# =============================================================================\n# v14: Create FDD layer mapping\n# =============================================================================\nfdd_layer_map = get_fdd_layer_mapping(\n    n_student_layers=config.n_layers,\n    n_teacher_layers=config.teacher_n_layers,\n    n_align_layers=config.fdd_n_align_layers\n)\n\nprint(f\"\")\nprint(f\"{config.VERSION} FDD Layer Mapping:\")\nprint(f\"  Layer pairs to align: {config.fdd_n_align_layers}\")\nprint(f\"  Mapping: {fdd_layer_map}\")\nprint(f\"  Strategy: Align early/middle/late semantic layers\")\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# cell 10: cosine lr with warmup (same as v9)\n",
    "# =============================================================================\n",
    "def get_cosine_schedule_with_warmup(\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    warmup_steps: int,\n",
    "    total_steps: int,\n",
    ") -> torch.optim.lr_scheduler.LambdaLR:\n",
    "    \"\"\"\n",
    "    linear warmup then cosine decay to 0.\n",
    "    \"\"\"\n",
    "    def lr_lambda(step: int) -> float:\n",
    "        if step < warmup_steps:\n",
    "            return step / max(warmup_steps, 1)\n",
    "        else:\n",
    "            progress = (step - warmup_steps) / max(total_steps - warmup_steps, 1)\n",
    "            return 0.5 * (1.0 + math.cos(math.pi * progress))\n",
    "\n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "\n",
    "print(f\"cosine lr: {config.warmup_steps} warmup, {config.distill_steps} total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# cell 11: load gpt-2 teacher (same as v9)\n",
    "# =============================================================================\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "print(\"loading gpt-2 teacher...\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "teacher = GPT2LMHeadModel.from_pretrained('gpt2').to(DEVICE)\n",
    "teacher.config.use_cache = False  # Disable KV caching (not needed for distillation)\n",
    "\n",
    "# Compile teacher for faster inference (PyTorch 2.0+)\n",
    "try:\n",
    "    teacher = torch.compile(teacher, mode='reduce-overhead')\n",
    "    print('teacher compiled with torch.compile')\n",
    "except Exception as e:\n",
    "    print(f'torch.compile not available: {e}')\n",
    "teacher.eval()\n",
    "for p in teacher.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "teacher_params = sum(p.numel() for p in teacher.parameters())\n",
    "print(f\"teacher: gpt-2 ({teacher_params:,} params)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# cell 13: data loading (v14 - efficient DataLoader)\n",
    "# =============================================================================\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"loading wikitext-2...\")\n",
    "dataset = load_dataset('wikitext', 'wikitext-2-raw-v1')\n",
    "\n",
    "def pre_tokenize(texts, max_len):\n",
    "    all_tokens = []\n",
    "    for text in tqdm(texts, desc=\"tokenizing\", leave=False):\n",
    "        if text.strip():\n",
    "            all_tokens.extend(tokenizer.encode(text, max_length=max_len*2, truncation=True))\n",
    "    chunks = [all_tokens[i:i+max_len] for i in range(0, len(all_tokens)-max_len+1, max_len//2) if len(all_tokens[i:i+max_len]) == max_len]\n",
    "    print(f\"created {len(chunks)} sequences\")\n",
    "    return torch.tensor(chunks, dtype=torch.long)\n",
    "\n",
    "train_tokens = pre_tokenize(dataset['train']['text'], config.max_seq_len)\n",
    "val_tokens = pre_tokenize(dataset['validation']['text'], config.max_seq_len)\n",
    "\n",
    "# v14: efficient DataLoader with workers and prefetch\n",
    "# Note: num_workers=0 for Kaggle/Colab compatibility, but prefetch still helps\n",
    "dataloader_kwargs = {\n",
    "    'batch_size': config.batch_size,\n",
    "    'pin_memory': True,\n",
    "    'num_workers': 0 if IS_KAGGLE or IS_COLAB else 2,  # workers disabled on cloud platforms\n",
    "    'prefetch_factor': None if IS_KAGGLE or IS_COLAB else 2,\n",
    "    'persistent_workers': False if IS_KAGGLE or IS_COLAB else True,\n",
    "}\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(train_tokens), shuffle=True, **dataloader_kwargs)\n",
    "val_loader = DataLoader(TensorDataset(val_tokens), shuffle=False, **dataloader_kwargs)\n",
    "\n",
    "print(f\"train: {len(train_loader)} batches, val: {len(val_loader)} batches\")\n",
    "print(f\"DataLoader: num_workers={dataloader_kwargs['num_workers']}, pin_memory={dataloader_kwargs['pin_memory']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# cell 14: create student model and projector (v14 - with compile)\n# =============================================================================\nprint(\"creating student model (v14 - v14 baseline + POCL)...\")\n\nstudent = StudentSpikingGoose(config, use_checkpointing=USE_GRADIENT_CHECKPOINTING).to(DEVICE)\nstudent_params = sum(p.numel() for p in student.parameters())\n\n# v14: create projector (even if not used, for infrastructure preservation)\nprojector = HiddenStateProjector(\n    student_dim=config.d_model,\n    teacher_dim=config.teacher_d_model,\n    n_student_layers=config.n_layers\n).to(DEVICE)\nprojector_params = sum(p.numel() for p in projector.parameters())\n\ncompression_ratio = teacher_params / student_params\n\nprint(f\"student: asnn-goose v14 ({student_params:,} params)\")\nprint(f\"projector: ({projector_params:,} params)\")\nprint(f\"compression ratio: {compression_ratio:.1f}x\")\nprint(f\"\")\nprint(f\"{config.VERSION} architecture:\")\nprint(f\"  d_model: {config.d_model}\")\nprint(f\"  n_layers: {config.n_layers}\")\nprint(f\"  params: ~{student_params // 1_000_000}M\")\nprint(f\"\")\n\n# v14: compile model if available and enabled\ncompile_success = False\nif USE_TORCH_COMPILE and TORCH_COMPILE_AVAILABLE:\n    try:\n        print(\"compiling student model with torch.compile...\")\n        # Use the compile() method as recommended by PyTorch docs\n        student = torch.compile(student, mode='reduce-overhead')\n        compile_success = True\n        print(\"compilation successful!\")\n    except Exception as e:\n        print(f\"torch.compile failed: {e}\")\n        print(\"continuing without compilation\")\nelse:\n    print(f\"torch.compile skipped (USE_TORCH_COMPILE={USE_TORCH_COMPILE}, available={TORCH_COMPILE_AVAILABLE})\")\n\nprint(f\"\")\nprint(f\"speedups active:\")\nprint(f\"  gradient checkpointing: {USE_GRADIENT_CHECKPOINTING}\")\nprint(f\"  torch.compile: {compile_success}\")\nprint(f\"  accumulation_steps: {config.accumulation_steps}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# cell 15: evaluation functions (same as v9)\n",
    "# =============================================================================\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, device, is_gpt2=False):\n",
    "    model.eval()\n",
    "    total_loss, total_tokens = 0, 0\n",
    "    with torch.inference_mode():\n",
    "      for batch in loader:\n",
    "        ids = batch[0].to(device, non_blocking=True)\n",
    "        with torch.cuda.amp.autocast():\n",
    "            logits = model(ids).logits if is_gpt2 else model(ids)\n",
    "        loss = F.cross_entropy(logits[:, :-1].reshape(-1, logits.size(-1)), ids[:, 1:].reshape(-1), reduction='sum')\n",
    "        total_loss += loss.item()\n",
    "        total_tokens += ids[:, 1:].numel()\n",
    "    return total_loss / total_tokens\n",
    "\n",
    "def get_ppl(loss):\n",
    "    return math.exp(min(loss, 10))\n",
    "\n",
    "print(\"evaluation functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# cell 17: distillation training loop (v14.1.1 - FDD+CTKD+HardCE)\n# =============================================================================\ndef distill_v14(teacher, student, projector, train_loader, val_loader, cfg, device,\n                hw_stats, spike_stats, fdd_layer_map):\n    \"\"\"\n    v14 distillation with FDD (Feature Dynamics Distillation) + CTKD.\n\n    Key innovations:\n    1. FDD: Align layer dynamics (delta_h) using CKA loss\n    2. CTKD: Adversarial temperature learning (proven in v12.1, v13.1)\n    3. Safety: FDD kill-switch if PPL regresses\n\n    References:\n    - CKA: Kornblith et al., \"Similarity of Neural Network Representations\"\n    - CTKD: https://arxiv.org/abs/2211.16231\n    - FDD: Feature Dynamics Distillation (view transformer as ODE)\n    \"\"\"\n    training_logs = {\n        'loss_history': [],\n        'kl_loss_history': [],\n        'ce_loss_history': [],  # v14.1: hard distillation\n        'fdd_loss_history': [],\n        'align_loss_history': [],\n        'ppl_history': [],\n        'lr_history': [],\n        'temp_history': [],\n        'lambda_history': [],\n        'fdd_weight_history': [],\n        'stage_history': [],\n        'stage_transitions': [],\n        'early_stopped': False,\n        'early_stop_step': None,\n        'fdd_killed': False,\n        'fdd_kill_step': None,\n    }\n\n    # =========================================================================\n    # v14: CTKD Temperature (same as v12.1, v13.1)\n    # =========================================================================\n    if cfg.use_ctkd:\n        temp_module = CTKDTemperature(\n            tau_min=cfg.tau_min,\n            tau_max=cfg.tau_max,\n            init=cfg.tau_init\n        ).to(device)\n        print(f\"{config.VERSION}: CTKD with Gradient Reversal Layer\")\n        print(f\"     Temperature bounds: [{cfg.tau_min}, {cfg.tau_max}]\")\n        print(f\"     Initial temp: {cfg.tau_init}\")\n        print(f\"     Lambda warmup: {cfg.lambda_warmup_ratio*100:.0f}%\")\n    else:\n        temp_module = None\n        print(f\"Using fixed temperature: {cfg.temperature}\")\n\n    # =========================================================================\n    # v14: FDD Setup\n    # =========================================================================\n    fdd_enabled = cfg.use_fdd\n    fdd_killed = False\n    baseline_ppl = None  # Set at fdd_warmup_steps\n\n    if cfg.use_fdd:\n        print(f\"\")\n        print(f\"{config.VERSION}: Feature Dynamics Distillation (FDD)\")\n        print(f\"     Layer mapping: {fdd_layer_map}\")\n        print(f\"     Weight: {cfg.fdd_weight}\")\n        print(f\"     Warmup: {cfg.fdd_warmup_steps} steps\")\n        print(f\"     Loss type: {cfg.fdd_loss_type}\")\n        print(f\"     Kill threshold: {cfg.fdd_kill_threshold*100:.0f}% PPL increase\")\n\n    # =========================================================================\n    # v14: Early Stopping Setup\n    # =========================================================================\n    best_ppl = float('inf')\n    best_step = 0\n    no_improve_steps = 0\n\n    if cfg.use_early_stopping:\n        print(f\"\")\n        print(f\"{config.VERSION}: Early Stopping\")\n        print(f\"     Patience: {cfg.early_stopping_patience} steps\")\n        print(f\"     Min delta: {cfg.min_ppl_delta} PPL\")\n\n    # =========================================================================\n    # Setup optimizer\n    # =========================================================================\n    param_groups = [\n        {'params': list(student.parameters()), 'lr': cfg.distill_lr}\n    ]\n\n    if cfg.hidden_align_weight > 0:\n        param_groups.append({'params': list(projector.parameters()), 'lr': cfg.distill_lr})\n\n    if temp_module is not None:\n        param_groups.append({'params': list(temp_module.parameters()), 'lr': cfg.distill_lr})\n\n    all_params = []\n    for group in param_groups:\n        all_params.extend(group['params'])\n\n    try:\n        optimizer = torch.optim.AdamW(param_groups, weight_decay=0.01, fused=True)\n        print(\"Using fused AdamW\")\n    except TypeError:\n        optimizer = torch.optim.AdamW(param_groups, weight_decay=0.01)\n\n    scheduler = get_cosine_schedule_with_warmup(optimizer, cfg.warmup_steps, cfg.distill_steps)\n    scaler = torch.cuda.amp.GradScaler()\n\n    hw_stats.start()\n    step = 0\n    accum_step = 0\n    current_stage = 1\n\n    accumulation_steps = cfg.accumulation_steps\n    effective_batch = cfg.batch_size * accumulation_steps\n    print(f\"Gradient accumulation: {accumulation_steps} (effective batch = {effective_batch})\")\n    print(f\"Extended training: {cfg.distill_steps} steps\")\n\n    pbar = tqdm(total=cfg.distill_steps, desc='distilling (v14.1 - FDD+CTKD+HardCE)')\n\n    optimizer.zero_grad(set_to_none=True)\n\n    while step < cfg.distill_steps:\n        for batch in train_loader:\n            if step >= cfg.distill_steps:\n                break\n\n            # Check early stopping\n            if cfg.use_early_stopping and no_improve_steps >= cfg.early_stopping_patience:\n                print(f\"\\n  [Early Stopping] No improvement for {cfg.early_stopping_patience} steps\")\n                print(f\"     Best PPL: {best_ppl:.2f} at step {best_step}\")\n                training_logs['early_stopped'] = True\n                training_logs['early_stop_step'] = step\n                pbar.close()\n                return training_logs\n\n            ids = batch[0].to(device, non_blocking=True)\n\n            # Get lambda for CTKD\n            if cfg.use_ctkd:\n                current_lambda = get_lambda(\n                    step, cfg.distill_steps,\n                    lambda_max=cfg.lambda_max,\n                    warmup_ratio=cfg.lambda_warmup_ratio\n                )\n            else:\n                current_lambda = 0.0\n\n            # Get current FDD weight\n            if fdd_enabled and not fdd_killed:\n                current_fdd_weight = get_fdd_weight(step, cfg.fdd_warmup_steps, cfg.fdd_weight)\n            else:\n                current_fdd_weight = 0.0\n\n            with torch.cuda.amp.autocast():\n                # Teacher forward (always get hidden states for FDD)\n                with torch.no_grad():\n                    t_out = teacher(ids, output_hidden_states=True)\n                    t_logits = t_out.logits\n                    t_hiddens = t_out.hidden_states  # tuple of tensors\n\n                # Student forward (always get hidden states for FDD)\n                student.train()\n                s_logits, s_hiddens = student(ids, return_hiddens=True)\n\n                # Get temperature\n                if cfg.use_ctkd and temp_module is not None:\n                    T = temp_module(current_lambda)\n                elif temp_module is not None:\n                    T = temp_module()\n                else:\n                    T = cfg.temperature\n\n                # KL divergence loss with temperature\n                s_log = F.log_softmax(s_logits / T, dim=-1)\n                t_prob = F.softmax(t_logits / T, dim=-1)\n                kl_loss = F.kl_div(\n                    s_log.view(-1, s_logits.size(-1)),\n                    t_prob.view(-1, t_logits.size(-1)),\n                    reduction='batchmean'\n                ) * (T ** 2)\n\n                # FDD loss (v14.1 with 100x weight increase)\n                if current_fdd_weight > 0:\n                    fdd_loss = compute_fdd_loss(\n                        s_hiddens,\n                        list(t_hiddens),  # Convert tuple to list\n                        fdd_layer_map,\n                        loss_type=cfg.fdd_loss_type\n                    )\n                else:\n                    fdd_loss = torch.tensor(0.0, device=device)\n\n                # v14.1: Hard distillation (CE with ground truth)\n                if cfg.ce_hard_weight > 0:\n                    shift_logits = s_logits[:, :-1, :].contiguous()\n                    shift_labels = ids[:, 1:].contiguous()\n                    ce_loss = F.cross_entropy(\n                        shift_logits.view(-1, shift_logits.size(-1)),\n                        shift_labels.view(-1),\n                        ignore_index=-100\n                    )\n                else:\n                    ce_loss = torch.tensor(0.0, device=device)\n\n                # Hidden alignment (usually disabled, kept for infrastructure)\n                if cfg.hidden_align_weight > 0:\n                    align_loss = compute_hidden_alignment_loss(\n                        t_hiddens, s_hiddens, projector,\n                        teacher_layers=cfg.teacher_n_layers,\n                        student_layers=cfg.n_layers\n                    )\n                else:\n                    align_loss = torch.tensor(0.0, device=device)\n\n                # Total loss (v14.1: added ce_hard_weight * ce_loss)\n                loss = kl_loss + cfg.ce_hard_weight * ce_loss + current_fdd_weight * fdd_loss + cfg.hidden_align_weight * align_loss\n                loss = loss / accumulation_steps\n\n            scaler.scale(loss).backward()\n            accum_step += 1\n\n            if accum_step % accumulation_steps == 0:\n                scaler.unscale_(optimizer)\n                gn = torch.nn.utils.clip_grad_norm_(all_params, cfg.max_grad_norm)\n\n                if torch.isfinite(gn):\n                    scaler.step(optimizer)\n                scaler.update()\n                scheduler.step()\n                optimizer.zero_grad(set_to_none=True)\n\n                hw_stats.record_step(ids.size(0) * accumulation_steps, ids.size(1))\n                spike_stats.record(student, step)\n\n                current_lr = optimizer.param_groups[0]['lr']\n                current_temp = temp_module.get_temperature() if temp_module is not None else cfg.temperature\n\n                # Log\n                training_logs['loss_history'].append({'step': step, 'loss': loss.item() * accumulation_steps})\n                training_logs['kl_loss_history'].append({'step': step, 'loss': kl_loss.item()})\n                training_logs['ce_loss_history'].append({'step': step, 'loss': ce_loss.item() if isinstance(ce_loss, torch.Tensor) else ce_loss})  # v14.1\n                training_logs['fdd_loss_history'].append({'step': step, 'loss': fdd_loss.item() if isinstance(fdd_loss, torch.Tensor) else fdd_loss})\n                training_logs['align_loss_history'].append({'step': step, 'loss': align_loss.item() if isinstance(align_loss, torch.Tensor) else align_loss})\n                training_logs['lr_history'].append({'step': step, 'lr': current_lr})\n                training_logs['temp_history'].append({'step': step, 'temperature': current_temp})\n                training_logs['lambda_history'].append({'step': step, 'lambda': current_lambda})\n                training_logs['fdd_weight_history'].append({'step': step, 'fdd_weight': current_fdd_weight})\n                training_logs['stage_history'].append({'step': step, 'stage': 1})\n\n                # Update progress bar (v14.1: added CE loss)\n                fdd_str = f\"fdd={fdd_loss.item():.3f}\" if current_fdd_weight > 0 else \"fdd=of\"\n                ce_str = f\"ce={ce_loss.item():.3f}\" if cfg.ce_hard_weight > 0 else \"ce=of\"\n                pbar.set_postfix(\n                    loss=f\"{loss.item() * accumulation_steps:.3f}\",\n                    kl=f\"{kl_loss.item():.3f}\",\n                    ce=ce_str,\n                    fdd=fdd_str,\n                    T=f\"{current_temp:.2f}\",\n                    lr=f\"{current_lr:.1e}\"\n                )\n                pbar.update(1)\n                step += 1\n\n                if step % cfg.eval_interval == 0:\n                    val_loss = evaluate(student, val_loader, device)\n                    val_ppl = get_ppl(val_loss)\n                    training_logs['ppl_history'].append({'step': step, 'ppl': val_ppl})\n\n                    amps = student.get_amplitudes()\n                    amp_str = ', '.join([f\"L{i}:{amps[f'layer_{i}']['k']:.2f}\" for i in range(min(4, cfg.n_layers))])\n\n                    # =========================================================\n                    # v14.1: FDD Kill Switch\n                    # =========================================================\n                    if step == cfg.fdd_warmup_steps and fdd_enabled:\n                        baseline_ppl = val_ppl\n                        print(f\"\\n  [FDD] Baseline PPL at warmup end: {baseline_ppl:.2f}\")\n\n                    if fdd_enabled and not fdd_killed and baseline_ppl is not None:\n                        ppl_increase = (val_ppl - baseline_ppl) / baseline_ppl\n                        if ppl_increase > cfg.fdd_kill_threshold:\n                            fdd_killed = True\n                            training_logs['fdd_killed'] = True\n                            training_logs['fdd_kill_step'] = step\n                            print(f\"\\n  [FDD KILLED] PPL increased {ppl_increase*100:.1f}% > {cfg.fdd_kill_threshold*100:.0f}%\")\n                            print(f\"     Baseline: {baseline_ppl:.2f}, Current: {val_ppl:.2f}\")\n                            print(f\"     Disabling FDD for remaining training\")\n\n                    # Early stopping check\n                    if val_ppl < best_ppl - cfg.min_ppl_delta:\n                        best_ppl = val_ppl\n                        best_step = step\n                        no_improve_steps = 0\n                        save_dict = {\n                            'student': student.state_dict(),\n                            'projector': projector.state_dict(),\n                            'step': step,\n                            'ppl': val_ppl,\n                        }\n                        if temp_module is not None:\n                            save_dict['temp_module'] = temp_module.state_dict()\n                        torch.save(save_dict, f'{OUTPUT_DIR}/checkpoints/v14.1_best.pt')\n                        improve_str = \" [NEW BEST]\"\n                    else:\n                        no_improve_steps += cfg.eval_interval\n                        improve_str = f\" (no improve: {no_improve_steps}/{cfg.early_stopping_patience})\"\n\n                    lambda_str = f\", lambda={current_lambda:.2f}\" if cfg.use_ctkd else \"\"\n                    fdd_status = \"KILLED\" if fdd_killed else f\"w={current_fdd_weight:.4f}\"\n                    print(f\"\\n  step {step}: ppl={val_ppl:.1f}, T={current_temp:.2f}{lambda_str}, FDD:{fdd_status}, amps=[{amp_str}...]{improve_str}\")\n\n    pbar.close()\n    return training_logs\n\nprint(\"distillation function defined (v14.1.1 - FDD+CTKD+HardCE)\")\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# cell 18: run distillation (v14.1.1 - FDD+CTKD+HardCE)\n# =============================================================================\nprint(\"=\"*60)\nprint(\"v14.1: FDD (Feature Dynamics Distillation) + CTKD\")\nprint(\"=\"*60)\nprint(f\"  Architecture: {config.d_model}d x {config.n_layers}L (~22M params)\")\nprint(f\"  Target: PPL < 400 (improve on v13.1's 434.44)\")\nprint(f\"\")\nprint(f\"{config.VERSION} Configuration:\")\nprint(f\"  FDD: {config.use_fdd}\")\nprint(f\"    Weight: {config.fdd_weight}\")\nprint(f\"    Warmup: {config.fdd_warmup_steps} steps\")\nprint(f\"    Layer map: {fdd_layer_map}\")\nprint(f\"    Loss type: {config.fdd_loss_type}\")\nprint(f\"    Kill threshold: {config.fdd_kill_threshold*100:.0f}%\")\nprint(f\"  CTKD: {config.use_ctkd} (proven from v12.1, v13.1)\")\nprint(f\"  Extended training: {config.distill_steps} steps\")\nprint(f\"  Early stopping: patience={config.early_stopping_patience}\")\nprint(f\"  POCL: {config.use_pocl} (disabled - caused regression)\")\nprint(\"\")\n\n# Instantiate collectors\nhw_stats = HardwareStatsCollector()\nspike_stats = SpikeStatsCollector(config.n_layers)\nprint(\"Initialized HardwareStatsCollector and SpikeStatsCollector\")\n\n# Run distillation (FDD + CTKD)\nprint(f\"\\nStarting distillation...\")\n\ndistill_logs = distill_v14(\n    teacher, student, projector,\n    train_loader, val_loader,\n    config, DEVICE,\n    hw_stats, spike_stats,\n    fdd_layer_map  # v14: pass FDD layer mapping\n)\n\n# Report results\nprint(f\"\\n\\n\" + \"=\"*60)\nprint(\"v14.1 Distillation Complete!\")\nprint(\"=\"*60)\n\nif distill_logs['ppl_history']:\n    final_ppl = distill_logs['ppl_history'][-1]['ppl']\n    best_ppl_entry = min(distill_logs['ppl_history'], key=lambda x: x['ppl'])\n    print(f\"\\nFinal PPL: {final_ppl:.2f}\")\n    print(f\"Best PPL: {best_ppl_entry['ppl']:.2f} at step {best_ppl_entry['step']}\")\n\nif distill_logs['early_stopped']:\n    print(f\"\\nEarly stopped at step {distill_logs['early_stop_step']}\")\nelse:\n    print(f\"\\nCompleted all {config.distill_steps} steps\")\n\nif distill_logs['fdd_killed']:\n    print(f\"\\nFDD was KILLED at step {distill_logs['fdd_kill_step']} (PPL regressed)\")\nelse:\n    print(f\"\\nFDD remained active throughout training\")\n\nif distill_logs['temp_history']:\n    temps = [h['temperature'] for h in distill_logs['temp_history']]\n    print(f\"\\nTemperature evolution:\")\n    print(f\"  Start: {temps[0]:.2f}\")\n    print(f\"  End: {temps[-1]:.2f}\")\n\nif distill_logs['lambda_history']:\n    lambdas = [h['lambda'] for h in distill_logs['lambda_history']]\n    print(f\"\\nLambda evolution:\")\n    print(f\"  Start: {lambdas[0]:.2f}\")\n    print(f\"  End: {lambdas[-1]:.2f}\")\n\nif distill_logs['fdd_loss_history']:\n    fdd_losses = [h['loss'] for h in distill_logs['fdd_loss_history'] if h['loss'] > 0]\n    if fdd_losses:\n        print(f\"\\nFDD loss evolution:\")\n        print(f\"  Start (after warmup): {fdd_losses[0]:.4f}\")\n        print(f\"  End: {fdd_losses[-1]:.4f}\")\n\nprint(f\"\\n\" + \"=\"*60)\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# cell 20: lora implementation (same as v9)\n",
    "# =============================================================================\n",
    "class LoRALinear(nn.Module):\n",
    "    \"\"\"lora adapter for linear layers.\"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, rank=8, alpha=16.0):\n",
    "        super().__init__()\n",
    "        self.scaling = alpha / rank\n",
    "        self.lora_A = nn.Parameter(torch.zeros(rank, in_features))\n",
    "        self.lora_B = nn.Parameter(torch.zeros(out_features, rank))\n",
    "        nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return (x @ self.lora_A.T @ self.lora_B.T) * self.scaling\n",
    "\n",
    "\n",
    "def apply_lora(model, rank=8, alpha=16.0, targets=['key_proj', 'value_proj']):\n",
    "    \"\"\"apply lora adapters to specified modules.\"\"\"\n",
    "    lora_modules = {}\n",
    "    for name, module in model.named_modules():\n",
    "        if any(t in name for t in targets) and isinstance(module, nn.Linear):\n",
    "            lora = LoRALinear(module.in_features, module.out_features, rank, alpha).to(next(module.parameters()).device)\n",
    "            lora_modules[name] = lora\n",
    "            orig_forward = module.forward\n",
    "            def make_forward(orig, lora_mod):\n",
    "                def forward(x):\n",
    "                    return orig(x) + lora_mod(x)\n",
    "                return forward\n",
    "            module.forward = make_forward(orig_forward, lora)\n",
    "    print(f\"lora: {len(lora_modules)} modules, rank={rank}\")\n",
    "    return lora_modules\n",
    "\n",
    "print(\"lora defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# cell 21: ttt with lora (same as v9)\n",
    "# =============================================================================\n",
    "print(\"=\"*60)\n",
    "print(\"phase 2: test-time training with lora\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for p in student.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "lora_modules = apply_lora(student, config.lora_rank, config.lora_alpha)\n",
    "lora_params = sum(p.numel() for m in lora_modules.values() for p in m.parameters())\n",
    "\n",
    "pre_ttt_loss = evaluate(student, val_loader, DEVICE)\n",
    "pre_ttt_ppl = get_ppl(pre_ttt_loss)\n",
    "print(f\"\\npre-ttt ppl: {pre_ttt_ppl:.2f}\")\n",
    "\n",
    "lora_opt = torch.optim.AdamW([p for m in lora_modules.values() for p in m.parameters()], lr=config.ttt_lr)\n",
    "ttt_logs = {'loss_history': []}\n",
    "student.train()\n",
    "\n",
    "for step, batch in enumerate(val_loader):\n",
    "    if step >= config.ttt_steps:\n",
    "        break\n",
    "    ids = batch[0].to(DEVICE)\n",
    "    with torch.cuda.amp.autocast():\n",
    "        loss = F.cross_entropy(student(ids)[:, :-1].reshape(-1, config.vocab_size), ids[:, 1:].reshape(-1))\n",
    "    lora_opt.zero_grad()\n",
    "    loss.backward()\n",
    "    lora_opt.step()\n",
    "    ttt_logs['loss_history'].append({'step': step, 'loss': loss.item()})\n",
    "    if step % 20 == 0:\n",
    "        print(f\"  ttt {step}: loss={loss.item():.4f}\")\n",
    "\n",
    "post_ttt_loss = evaluate(student, val_loader, DEVICE)\n",
    "post_ttt_ppl = get_ppl(post_ttt_loss)\n",
    "print(f\"\\npost-ttt ppl: {post_ttt_ppl:.2f}\")\n",
    "print(f\"ttt improvement: {pre_ttt_ppl - post_ttt_ppl:.1f} ppl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# cell 22: final evaluation (v14.1.1 - FDD+CTKD+HardCE)\n# =============================================================================\nprint(\"=\"*60)\nprint(\"final evaluation (v14.1.1 - FDD+CTKD+HardCE)\")\nprint(\"=\"*60)\n\nteacher_loss = evaluate(teacher, val_loader, DEVICE, is_gpt2=True)\nteacher_ppl = get_ppl(teacher_loss)\nstudent_loss = evaluate(student, val_loader, DEVICE)\nstudent_ppl = get_ppl(student_loss)\n\n# v14: Get final temperature and lambda from CTKD\nfinal_temp = distill_logs['temp_history'][-1]['temperature'] if distill_logs['temp_history'] else config.tau_init\nfinal_lambda = distill_logs['temp_history'][-1].get('lambda', config.lambda_max) if distill_logs['temp_history'] else config.lambda_max\n\n# VRAM logging\nvram_peak_gb = torch.cuda.max_memory_allocated() / 1e9 if torch.cuda.is_available() else 0\n\nprint(f\"\")\nprint(f\"{'model':<30} {'ppl':>10} {'params':>15}\")\nprint(\"-\" * 55)\nprint(f\"{'gpt-2 (teacher)':<30} {teacher_ppl:>10.2f} {teacher_params:>15,}\")\nprint(f\"{'asnn-goose v14.1 (student)':<30} {student_ppl:>10.2f} {student_params:>15,}\")\nprint(\"-\" * 55)\nprint(f\"{'compression':<30} {compression_ratio:>10.1f}x\")\nprint(f\"{'ppl gap':<30} {student_ppl - teacher_ppl:>10.2f}\")\nprint(f\"{'spike density':<30} {student.get_avg_spike_density():>10.3f}\")\nprint(f\"{'VRAM peak':<30} {vram_peak_gb:>10.2f}GB\")\nprint(f\"{'final temperature':<30} {final_temp:>10.2f}\")\nprint(f\"{'final lambda (GRL)':<30} {final_lambda:>10.3f}\")\nprint(\"\")\nprint(\"CTKD Implementation:\")\nprint(f\"  tau range: [{config.tau_min:.1f}, {config.tau_max:.1f}]\")\nprint(f\"  lambda warmup ratio: {config.lambda_warmup_ratio:.0%}\")\nprint(f\"  GRL: Gradient Reversal Layer for adversarial min-max\")\nprint(\"\")\nprint(\"version comparison:\")\nprint(f\"  v6: 627.3 PPL (baseline)\")\nprint(f\"  v7: 1655 PPL (regression!)\")\nprint(f\"  v8: 559 PPL (fixed)\")\nprint(f\"  v9: 541.7 PPL (capacity increase)\")\nprint(f\"  v10: 514.5 PPL (320d/5L baseline)\")\nprint(f\"  v14: 512.67 PPL (channel-wise, WITH reg)\")\nprint(f\"  v14.1: 512.04 PPL (channel-wise, NO reg)\")\nprint(f\"  v12: FAILED (temp runaway without GRL)\")\nprint(f\"  v14: {student_ppl:.2f} PPL (POCL, T={final_temp:.2f}, \u03bb={final_lambda:.3f})\")\nif student_ppl < 500:\n    print(f\"  {config.VERSION} TARGET MET! PPL < 500\")\nelif student_ppl < 512.04:\n    print(f\"  v14.1 beats v14 by {424.81 - student_ppl:.1f} PPL\")\nelse:\n    print(f\"  WARNING: v14.1 did not improve over v14\")\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# cell 23: visualization\n# =============================================================================\nfig, axes = plt.subplots(2, 3, figsize=(15, 10))\n\n# distillation loss\nd_steps = [l['step'] for l in distill_logs['loss_history']]\nd_losses = [l['loss'] for l in distill_logs['loss_history']]\nkl_losses = [l['loss'] for l in distill_logs['kl_loss_history']]\naxes[0,0].plot(d_steps, d_losses, label='total', alpha=0.8)\naxes[0,0].plot(d_steps, kl_losses, label='kl', alpha=0.7)\n\n# CE loss (if available)\nif 'ce_loss_history' in distill_logs and distill_logs['ce_loss_history']:\n    ce_losses = [l['loss'] for l in distill_logs['ce_loss_history']]\n    axes[0,0].plot(d_steps, ce_losses, label='ce', alpha=0.6)\naxes[0,0].set_xlabel('step')\naxes[0,0].set_ylabel('loss')\naxes[0,0].set_title(f'distillation loss ({config.VERSION})')\naxes[0,0].legend()\n\n# validation ppl\np_steps = [l['step'] for l in distill_logs['ppl_history']]\np_ppls = [l['ppl'] for l in distill_logs['ppl_history']]\naxes[0,1].plot(p_steps, p_ppls, 'orange', marker='o')\naxes[0,1].axhline(y=teacher_ppl, color='green', linestyle='--', label=f'teacher ({teacher_ppl:.1f})')\naxes[0,1].axhline(y=627.3, color='blue', linestyle=':', label='v6 (627.3)')\naxes[0,1].axhline(y=541.7, color='purple', linestyle=':', label='v9 (541.7)')\naxes[0,1].axhline(y=480, color='red', linestyle='--', label=f'{config.VERSION} target')\naxes[0,1].set_xlabel('step')\naxes[0,1].set_ylabel('ppl')\naxes[0,1].set_title('validation ppl')\naxes[0,1].legend()\n\n# lr schedule\nlr_steps = [l['step'] for l in distill_logs['lr_history']]\nlr_vals = [l['lr'] for l in distill_logs['lr_history']]\naxes[0,2].plot(lr_steps, lr_vals, 'purple')\naxes[0,2].axvline(x=config.warmup_steps, color='gray', linestyle='--', label=f'warmup ({config.warmup_steps})')\naxes[0,2].set_xlabel('step')\naxes[0,2].set_ylabel('lr')\naxes[0,2].set_title('learning rate')\naxes[0,2].legend()\n\n# spike density + amplitudes (first 4 layers)\nspike_summary = spike_stats.get_summary()\nlayers = [f'layer_{i}' for i in range(min(4, config.n_layers))]\nk_dens = [spike_summary['per_layer'][l]['k_final'] for l in layers]\nv_dens = [spike_summary['per_layer'][l]['v_final'] for l in layers]\nk_amps = [spike_summary['per_layer'][l]['k_amp_final'] for l in layers]\nv_amps = [spike_summary['per_layer'][l]['v_amp_final'] for l in layers]\n\nx = np.arange(len(layers))\naxes[1,0].bar(x - 0.2, k_dens, 0.4, label='k density')\naxes[1,0].bar(x + 0.2, v_dens, 0.4, label='v density')\nax2 = axes[1,0].twinx()\nax2.plot(x, k_amps, 'r-o', label='k amp')\nax2.plot(x, v_amps, 'b-s', label='v amp')\naxes[1,0].set_xlabel('layer')\naxes[1,0].set_ylabel('density')\nax2.set_ylabel('amplitude')\naxes[1,0].set_title(f'spike density & amps (first 4/{config.n_layers} layers)')\naxes[1,0].legend(loc='upper left')\nax2.legend(loc='upper right')\n\n# ttt loss\nt_steps = [l['step'] for l in ttt_logs['loss_history']]\nt_losses = [l['loss'] for l in ttt_logs['loss_history']]\naxes[1,1].plot(t_steps, t_losses, 'red')\naxes[1,1].set_xlabel('step')\naxes[1,1].set_ylabel('ce loss')\naxes[1,1].set_title('ttt with lora')\n\n# version comparison\n# Historical versions for comparison (must match in length!)\nversions = ['v6', 'v9', 'v10', 'v12.1', 'v13.1', 'v14', config.VERSION]\n# Teacher PPL is constant\nt_ppls = [44.6] * len(versions)\n# Student PPL history (from changelog)\ns_ppls = [627.3, 541.7, 514.5, 445.61, 434.44, 424.81, student_ppl]\nassert len(versions) == len(t_ppls) == len(s_ppls), f\"Array length mismatch: {len(versions)}, {len(t_ppls)}, {len(s_ppls)}\"\nx = np.arange(len(versions))\naxes[1,2].bar(x - 0.2, t_ppls, 0.4, label='teacher', alpha=0.7)\naxes[1,2].bar(x + 0.2, s_ppls, 0.4, label='student', alpha=0.7)\naxes[1,2].axhline(y=480, color='red', linestyle='--', label=f'{config.VERSION} target', alpha=0.7)\naxes[1,2].set_xticks(x)\naxes[1,2].set_xticklabels(versions)\naxes[1,2].set_ylabel('ppl')\naxes[1,2].set_title('version comparison')\naxes[1,2].legend()\naxes[1,2].set_yscale('log')\n\nplt.tight_layout()\nfigure_path = f'{OUTPUT_DIR}/figures/v14_training_{RUN_TIMESTAMP}.png'\nplt.savefig(figure_path, dpi=300, bbox_inches='tight')\nplt.show()\nprint(f\"saved: {figure_path}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# cell 24: build results dict (v13 - Proper CTKD with GRL)\n# =============================================================================\nprint(\"building results (v13 - CTKD with Gradient Reversal Layer)...\")\n\nwith open(figure_path, 'rb') as f:\n    figure_base64 = base64.b64encode(f.read()).decode('utf-8')\n\n# v13: Extract final lambda\nfinal_lambda = distill_logs['temp_history'][-1].get('lambda', config.lambda_max) if distill_logs['temp_history'] else config.lambda_max\n\nresults = {\n    'version': config.VERSION,\n    'timestamp': datetime.now().isoformat(),\n    'run_id': RUN_TIMESTAMP,\n    'platform': PLATFORM,\n    'description': 'CTKD with Gradient Reversal Layer - adversarial temperature learning',\n\n    f'{config.VERSION}_design': {\n        'principle': 'CTKD: Adversarial min-max optimization via GRL',\n        'innovation': 'Gradient Reversal Layer makes temperature MAXIMIZE KL while student MINIMIZES',\n        'rationale': 'Proper CTKD (ArXiv 2211.16231) requires adversarial training, not simple regularization',\n        'why_v12_failed': 'v12 used simple regularization - optimizer pushed T to max for easy KL',\n        'techniques': {\n            'ctkd_with_grl': 'ENABLED - Gradient Reversal Layer for adversarial min-max (v13 KEY)',\n            'lambda_scheduling': f'Cosine warmup 0->{config.lambda_max} with {config.lambda_warmup_ratio:.0%} warmup',\n            'sigmoid_bounding': f'T bounded to [{config.tau_min}, {config.tau_max}] via sigmoid (smooth gradients)',\n            'no_manual_reg': 'GRL eliminates need for manual temperature regularization',\n            'progressive_stages': 'DISABLED',\n            'channel_wise_spikes': 'DISABLED (structural symmetry issue)',\n        },\n        'grl_mechanism': {\n            'forward_pass': 'Identity: GRL(x) = x',\n            'backward_pass': 'Negation: dGRL/dx = -lambda',\n            'effect': 'Temperature gradients reversed -> T maximizes KL loss',\n        },\n        'temperature_config': {\n            'tau_min': config.tau_min,\n            'tau_max': config.tau_max,\n            'tau_init': config.tau_init,\n            'lambda_max': config.lambda_max,\n            'lambda_warmup_ratio': config.lambda_warmup_ratio,\n        },\n        'architecture': {\n            'd_model': config.d_model,\n            'n_layers': 5,\n            'params': '~22M',\n        },\n        'speedups': {\n            'gradient_checkpointing': USE_GRADIENT_CHECKPOINTING,\n            'torch_compile': compile_success,\n            'fused_optimizer': True,\n            'accumulation_steps': config.accumulation_steps,\n        },\n        'unchanged': [\n            'hidden_align_weight: 0.0',\n            'warmup_steps: 50',\n            'distill_steps: 3000',\n        ],\n    },\n\n    'architecture': {\n        'teacher': {'name': 'gpt2', 'params': teacher_params},\n        'student': {\n            'name': f'asnn-goose-{config.VERSION}',\n            'd_model': config.d_model,\n            'n_layers': config.n_layers,\n            'params': student_params,\n        },\n        'projector_params': projector_params,\n        'compression_ratio': compression_ratio,\n        'vram_peak_gb': vram_peak_gb,\n    },\n\n    'training_config': {\n        'distill_steps': config.distill_steps,\n        'tau_min': config.tau_min,\n        'tau_max': config.tau_max,\n        'tau_init': config.tau_init,\n        'final_temperature': final_temp,\n        'lambda_max': config.lambda_max,\n        'lambda_warmup_ratio': config.lambda_warmup_ratio,\n        'final_lambda': final_lambda,\n        'hidden_align_weight': config.hidden_align_weight,\n        'warmup_steps': config.warmup_steps,\n        'batch_size': config.batch_size,\n        'accumulation_steps': config.accumulation_steps,\n        'effective_batch': config.batch_size * config.accumulation_steps,\n        'distill_lr': config.distill_lr,\n        'max_grad_norm': config.max_grad_norm,\n    },\n\n    'results': {\n        'teacher_ppl': teacher_ppl,\n        'student_ppl': student_ppl,\n        'ppl_gap': student_ppl - teacher_ppl,\n        'spike_density': student.get_avg_spike_density(),\n        'amplitudes': student.get_amplitudes(),\n        'final_temperature': final_temp,\n        'final_lambda': final_lambda,\n        'target_met': student_ppl < 500,\n    },\n\n    'training_curves': {\n        'loss_history': distill_logs['loss_history'],\n        'kl_loss_history': distill_logs['kl_loss_history'],\n        'align_loss_history': distill_logs['align_loss_history'],\n        'ppl_history': distill_logs['ppl_history'],\n        'lr_history': distill_logs['lr_history'],\n        'temp_history': distill_logs['temp_history'],  # v13: includes temperature AND lambda\n    },\n\n    'hardware_stats': hw_stats.get_summary(),\n    'spike_analysis': spike_stats.get_summary(),\n\n    'ttt': {\n        'lora_params': lora_params,\n        'pre_ppl': pre_ttt_ppl,\n        'post_ppl': post_ttt_ppl,\n        'improvement': pre_ttt_ppl - post_ttt_ppl,\n        'loss_history': ttt_logs['loss_history'],\n    },\n\n    'comparison': {\n        'v6': {'student_ppl': 627.3, 'note': 'baseline'},\n        'v7': {'student_ppl': 1655, 'note': 'regression (align=1.0, T=4)'},\n        'v8': {'student_ppl': 559, 'note': 'fixed defaults (align=0, T=2)'},\n        'v9': {'student_ppl': 541.7, 'note': 'capacity increase (320d, 5L)'},\n        'v10': {'student_ppl': 514.5, 'note': '320d/5L baseline'},\n        'v14': {'student_ppl': 512.67, 'note': 'channel-wise WITH reg (bug)'},\n        'v14.1': {'student_ppl': 512.04, 'note': 'channel-wise NO reg (symmetry issue)'},\n        'v12': {'student_ppl': 'FAILED', 'note': 'temp runaway without GRL'},\n        'v13': {'student_ppl': student_ppl, 'note': f'POCL (T={final_temp:.2f}, L={final_lambda:.3f})'},\n    },\n\n    'figures': {\n        'training_plot': {\n            'filename': f'{config.VERSION}_training_{RUN_TIMESTAMP}.png',\n            'base64': figure_base64,\n        }\n    },\n\n    # validation_tests will be added in cell 26\n}\n\nprint(\"results dict built (validation_tests pending)\")\nprint(f\"  version: {config.VERSION} ({config.VERSION_DESC})\")\nprint(f\"  final_temperature: {final_temp:.2f}\")\nprint(f\"  final_lambda: {final_lambda:.3f}\")\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# cell 25: validation tests (v14.1 - 12 tests with FDD)\n# =============================================================================\n# These tests validate correct v14.1 implementation\n\nprint(\"=\"*60)\nprint(f\"{config.VERSION} Validation Test Suite\")\nprint(\"=\"*60)\n\ntests = []\n\n# =============================================================================\n# Test 1: PPL Target (<400)\n# =============================================================================\nif distill_logs['ppl_history']:\n    best_ppl_entry = min(distill_logs['ppl_history'], key=lambda x: x['ppl'])\n    best_ppl = best_ppl_entry['ppl']\n    target_ppl = 400  # target for this version\n    ppl_pass = best_ppl < target_ppl\n    tests.append(('PPL < 400', ppl_pass, f\"best_ppl={best_ppl:.2f}, target={target_ppl}\"))\nelse:\n    tests.append(('PPL < 400', False, \"No PPL history found\"))\n\n# =============================================================================\n# Test 2: PPL Improvement over v13.1 (434.44)\n# =============================================================================\nif distill_logs['ppl_history']:\n    best_ppl_entry = min(distill_logs['ppl_history'], key=lambda x: x['ppl'])\n    v13_1_ppl = 434.44\n    improvement = v13_1_ppl - best_ppl_entry['ppl']\n    improve_pass = improvement > 0\n    tests.append(('Improved over v13.1', improve_pass, f\"improvement={improvement:.2f} PPL\"))\nelse:\n    tests.append(('Improved over v13.1', False, \"No PPL history\"))\n\n# =============================================================================\n# Test 3: Spike Density in Valid Range [0.1, 0.9]\n# =============================================================================\nif spike_stats.step_densities:\n    final_density = spike_stats.step_densities[-1]['density']\n    density_pass = 0.1 <= final_density <= 0.9\n    tests.append(('Spike density [0.1, 0.9]', density_pass, f\"density={final_density:.3f}\"))\nelse:\n    tests.append(('Spike density [0.1, 0.9]', False, \"No spike history\"))\n\n# =============================================================================\n# Test 4: Amplitudes in Healthy Range [0.3, 3.0]\n# =============================================================================\namps = student.get_amplitudes()\namp_values = []\nfor layer_name, layer_amps in amps.items():\n    amp_values.extend([layer_amps['k'], layer_amps['v']])\namp_min, amp_max = min(amp_values), max(amp_values)\namp_pass = 0.3 <= amp_min and amp_max <= 3.0\ntests.append(('Amplitudes [0.3, 3.0]', amp_pass, f\"range=[{amp_min:.2f}, {amp_max:.2f}]\"))\n\n# =============================================================================\n# Test 5: Training Completed (all steps or early stopped)\n# =============================================================================\nif distill_logs['early_stopped']:\n    training_pass = distill_logs['early_stop_step'] > config.distill_steps * 0.3\n    tests.append(('Training completed', training_pass, f\"Early stopped at {distill_logs['early_stop_step']} steps\"))\nelse:\n    training_pass = len(distill_logs['loss_history']) >= config.distill_steps * 0.95\n    tests.append(('Training completed', training_pass, f\"Completed {len(distill_logs['loss_history'])}/{config.distill_steps} steps\"))\n\n# =============================================================================\n# Test 6: No NaN/Inf in Loss\n# =============================================================================\nnan_inf_found = False\nfor h in distill_logs['loss_history']:\n    if h['loss'] != h['loss'] or h['loss'] == float('inf') or h['loss'] == float('-inf'):\n        nan_inf_found = True\n        break\nnan_pass = not nan_inf_found\ntests.append(('No NaN/Inf loss', nan_pass, \"All losses finite\" if nan_pass else \"Found NaN/Inf\"))\n\n# =============================================================================\n# Test 7: VRAM Usage Reasonable (<8GB)\n# =============================================================================\nif hasattr(hw_stats, 'get_summary'):\n    hw_summary = hw_stats.get_summary()\n    vram_gb = hw_summary.get('peak_gpu_memory_gb', 0)\n    vram_pass = vram_gb < 8.0\n    tests.append(('VRAM < 8GB', vram_pass, f\"peak={vram_gb:.2f}GB\"))\nelse:\n    tests.append(('VRAM < 8GB', True, \"hw_stats not available\"))\n\n# =============================================================================\n# Test 8: FDD Was Active (v14.1)\n# =============================================================================\nif config.use_fdd:\n    # FDD should have been active at some point\n    fdd_losses = [h['loss'] for h in distill_logs['fdd_loss_history'] if h.get('loss') is not None and h['loss'] != 0]\n    fdd_active_pass = len(fdd_losses) > 0\n    if distill_logs['fdd_killed']:\n        status = f\"Active then KILLED at step {distill_logs['fdd_kill_step']}\"\n    else:\n        status = f\"Active for {len(fdd_losses)} steps\"\n    tests.append(('FDD was active', fdd_active_pass, status))\nelse:\n    tests.append(('FDD was active', True, \"FDD disabled in config\"))\n\n# =============================================================================\n# Test 9: CTKD Temperature Evolved (v14.1)\n# =============================================================================\nif config.use_ctkd and distill_logs['temp_history']:\n    temps = [h['temperature'] for h in distill_logs['temp_history']]\n    start_temp = temps[0]\n    end_temp = temps[-1]\n    temp_evolved = abs(end_temp - start_temp) > 0.1  # Should have moved\n    tests.append(('Temperature evolved', temp_evolved, f\"start={start_temp:.2f}, end={end_temp:.2f}\"))\nelse:\n    tests.append(('Temperature evolved', True, \"CTKD disabled or no temp history\"))\n\n# =============================================================================\n# Test 10: Early Stopping Working (if triggered)\n# =============================================================================\nif config.use_early_stopping:\n    if distill_logs['early_stopped']:\n        es_step = distill_logs['early_stop_step']\n        es_pass = config.distill_steps * 0.3 < es_step < config.distill_steps\n        tests.append(('Early stopping working', es_pass, f\"stopped at {es_step}\"))\n    else:\n        if distill_logs['ppl_history']:\n            best_ppl_entry = min(distill_logs['ppl_history'], key=lambda x: x['ppl'])\n            last_improvement_step = best_ppl_entry['step']\n            final_step = distill_logs['ppl_history'][-1]['step']\n            gap = final_step - last_improvement_step\n            es_pass = gap <= config.early_stopping_patience + config.eval_interval\n            tests.append(('Early stopping working', es_pass, f\"last improvement at step {last_improvement_step}\"))\n        else:\n            tests.append(('Early stopping working', True, \"No PPL history\"))\nelse:\n    tests.append(('Early stopping working', True, \"Early stopping disabled\"))\n\n# =============================================================================\n# Test 11: FDD Loss Decreased (v14.1)\n# =============================================================================\nif config.use_fdd and distill_logs['fdd_loss_history']:\n    fdd_losses = [h['loss'] for h in distill_logs['fdd_loss_history'] if h.get('loss') is not None and h['loss'] != 0]\n    if len(fdd_losses) >= 10:\n        start_fdd = sum(fdd_losses[:5]) / 5\n        end_fdd = sum(fdd_losses[-5:]) / 5\n        fdd_decreased = end_fdd < start_fdd\n        tests.append(('FDD loss decreased', fdd_decreased, f\"start={start_fdd:.4f}, end={end_fdd:.4f}\"))\n    else:\n        tests.append(('FDD loss decreased', True, \"Not enough FDD data points\"))\nelse:\n    tests.append(('FDD loss decreased', True, \"FDD disabled or no history\"))\n\n# =============================================================================\n# Test 12: Extended Training (5000 steps)\n# =============================================================================\nextended_pass = config.distill_steps >= 5000\ntests.append(('Extended training (5000+)', extended_pass, f\"distill_steps={config.distill_steps}\"))\n\n# =============================================================================\n# Report Results\n# =============================================================================\nprint(\"\\n\" + \"-\"*60)\nprint(\"TEST RESULTS\")\nprint(\"-\"*60)\n\npassed = 0\nfailed = 0\nfor name, result, details in tests:\n    status = \"PASS\" if result else \"FAIL\"\n    symbol = \"V\" if result else \"X\"\n    print(f\"[{symbol}] {name}: {details}\")\n    if result:\n        passed += 1\n    else:\n        failed += 1\n\nprint(\"-\"*60)\nprint(f\"SUMMARY: {passed}/{len(tests)} tests passed\")\nif failed > 0:\n    print(f\"WARNING: {failed} tests failed!\")\nelse:\n    print(f\"ALL TESTS PASSED! {config.VERSION} implementation validated.\")\nprint(\"=\"*60)\n\n# Store results\nvalidation_results = {\n    'tests': tests,\n    'passed': passed,\n    'failed': failed,\n    'total': len(tests)\n}\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# cell 26: FINAL save with validation_tests (v14)\n",
    "# =============================================================================\n",
    "print(\"=\"*60)\n",
    "print(\"FINAL SAVE (includes validation_tests)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Add validation_tests to results\n",
    "results['validation_tests'] = validation_results\n",
    "\n",
    "# Save final results.json with ALL data\n",
    "results_path = f'{OUTPUT_DIR}/results/results_{RUN_TIMESTAMP}.json'\n",
    "with open(results_path, 'w') as f:\n",
    "    json.dump(results, f, indent=2, default=str)\n",
    "\n",
    "print(f\"saved: {results_path}\")\n",
    "print(f\"size: {os.path.getsize(results_path) / 1024:.1f} KB\")\n",
    "print(f\"\")\n",
    "print(f\"validation_tests included: {list(validation_results.keys())}\")\n",
    "\n",
    "# v14: auto-download AFTER final save\n",
    "print(\"\")\n",
    "print(\"auto-download\")\n",
    "if IS_COLAB:\n",
    "    try:\n",
    "        from google.colab import files\n",
    "        files.download(results_path)\n",
    "        files.download(figure_path)\n",
    "        print(\"downloads started!\")\n",
    "    except Exception as e:\n",
    "        print(f\"download failed: {e}\")\n",
    "elif IS_KAGGLE:\n",
    "    print(f\"kaggle: {results_path}\")\n",
    "else:\n",
    "    print(f\"local: {results_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# cell 27: V15 SpikingBrain - Information Encoding Validation\n",
    "# =============================================================================\n",
    "# Validate that spike patterns encode meaningful semantic information\n",
    "# Prerequisite for v16 (sparse ops)\n",
    "\n",
    "import numpy as np\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "\n",
    "# =============================================================================\n",
    "# INLINE: SpikingBrain Validation Classes\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass\n",
    "class SpikeHealthMetrics:\n",
    "    \"\"\"Container for spike health metrics.\"\"\"\n",
    "    dead_neuron_pct: float\n",
    "    dead_neuron_indices: Dict[str, np.ndarray]\n",
    "    saturated_neuron_pct: float\n",
    "    saturated_neuron_indices: Dict[str, np.ndarray]\n",
    "    firing_rate_mean: float\n",
    "    firing_rate_std: float\n",
    "    per_channel_rates: Dict[str, np.ndarray]\n",
    "    health_pass: bool\n",
    "    alerts: List[str]\n",
    "\n",
    "@dataclass\n",
    "class SpikingBrainValidation:\n",
    "    \"\"\"Complete validation results.\"\"\"\n",
    "    health: SpikeHealthMetrics\n",
    "    mutual_information: Dict[str, float]\n",
    "    cka: Dict[str, float]\n",
    "    overall_pass: bool\n",
    "    summary: str\n",
    "\n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        return {\n",
    "            'health': {\n",
    "                'dead_neuron_pct': self.health.dead_neuron_pct,\n",
    "                'saturated_neuron_pct': self.health.saturated_neuron_pct,\n",
    "                'firing_rate_mean': self.health.firing_rate_mean,\n",
    "                'firing_rate_std': self.health.firing_rate_std,\n",
    "                'health_pass': self.health.health_pass,\n",
    "                'alerts': self.health.alerts,\n",
    "            },\n",
    "            'mutual_information': self.mutual_information,\n",
    "            'cka': self.cka,\n",
    "            'overall_pass': self.overall_pass,\n",
    "        }\n",
    "\n",
    "\n",
    "class MutualInformationEstimator:\n",
    "    \"\"\"Estimate MI between spikes and teacher hiddens using binning.\"\"\"\n",
    "\n",
    "    def __init__(self, n_dims: int = 8, n_bins: int = 32):\n",
    "        self.n_dims = n_dims\n",
    "        self.n_bins = n_bins\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def estimate(\n",
    "        self,\n",
    "        spikes: torch.Tensor,\n",
    "        teacher_hidden: torch.Tensor,\n",
    "    ) -> float:\n",
    "        \"\"\"Binning-based MI estimation.\"\"\"\n",
    "        # Flatten to 2D\n",
    "        spikes_flat = spikes.reshape(-1, spikes.shape[-1])[:, :self.n_dims]\n",
    "        teacher_flat = teacher_hidden.reshape(-1, teacher_hidden.shape[-1])[:, :self.n_dims]\n",
    "\n",
    "        n_samples = min(spikes_flat.shape[0], 10000)\n",
    "        spikes_flat = spikes_flat[:n_samples].cpu().numpy()\n",
    "        teacher_flat = teacher_flat[:n_samples].cpu().numpy()\n",
    "\n",
    "        # Bin teacher values\n",
    "        teacher_binned = np.zeros_like(teacher_flat, dtype=np.int32)\n",
    "        for d in range(self.n_dims):\n",
    "            col = teacher_flat[:, d]\n",
    "            bins = np.linspace(col.min() - 1e-10, col.max() + 1e-10, self.n_bins + 1)\n",
    "            teacher_binned[:, d] = np.digitize(col, bins) - 1\n",
    "\n",
    "        # Spikes are already discrete {-1, 0, +1} -> map to {0, 1, 2}\n",
    "        spikes_discrete = (spikes_flat + 1).astype(np.int32)\n",
    "\n",
    "        # Compute MI per dimension and average\n",
    "        mi_per_dim = []\n",
    "        for d in range(self.n_dims):\n",
    "            # Joint histogram\n",
    "            joint = np.zeros((3, self.n_bins))\n",
    "            for i in range(n_samples):\n",
    "                s_idx = spikes_discrete[i, d]\n",
    "                t_idx = min(teacher_binned[i, d], self.n_bins - 1)\n",
    "                joint[s_idx, t_idx] += 1\n",
    "            joint = joint / n_samples + 1e-10\n",
    "\n",
    "            # Marginals\n",
    "            p_s = joint.sum(axis=1, keepdims=True)\n",
    "            p_t = joint.sum(axis=0, keepdims=True)\n",
    "\n",
    "            # MI\n",
    "            mi = np.sum(joint * np.log2(joint / (p_s * p_t + 1e-10)))\n",
    "            mi_per_dim.append(max(0, mi))\n",
    "\n",
    "        return float(np.mean(mi_per_dim))\n",
    "\n",
    "\n",
    "class RepresentationAnalyzer:\n",
    "    \"\"\"Compute CKA between spike patterns and teacher representations.\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def linear_cka(X: np.ndarray, Y: np.ndarray) -> float:\n",
    "        \"\"\"Compute linear CKA similarity.\"\"\"\n",
    "        X = X - X.mean(axis=0)\n",
    "        Y = Y - Y.mean(axis=0)\n",
    "\n",
    "        hsic_xy = np.linalg.norm(X.T @ Y, 'fro') ** 2\n",
    "        hsic_xx = np.linalg.norm(X.T @ X, 'fro') ** 2\n",
    "        hsic_yy = np.linalg.norm(Y.T @ Y, 'fro') ** 2\n",
    "\n",
    "        return float(hsic_xy / (np.sqrt(hsic_xx * hsic_yy) + 1e-10))\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def compute_cka(\n",
    "        self,\n",
    "        spikes: torch.Tensor,\n",
    "        teacher_hidden: torch.Tensor,\n",
    "        max_samples: int = 5000,\n",
    "    ) -> float:\n",
    "        \"\"\"Compute CKA between spikes and teacher hidden states.\"\"\"\n",
    "        spikes_flat = spikes.reshape(-1, spikes.shape[-1])\n",
    "        teacher_flat = teacher_hidden.reshape(-1, teacher_hidden.shape[-1])\n",
    "\n",
    "        n_samples = min(spikes_flat.shape[0], max_samples)\n",
    "        X = spikes_flat[:n_samples].float().cpu().numpy()\n",
    "        Y = teacher_flat[:n_samples].float().cpu().numpy()\n",
    "\n",
    "        return self.linear_cka(X, Y)\n",
    "\n",
    "\n",
    "class SpikingBrainValidator:\n",
    "    \"\"\"Main validator for V15 SpikingBrain validation.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        device: torch.device,\n",
    "        layer_map: Dict[int, int],\n",
    "        dead_threshold: float = 0.05,\n",
    "        saturated_threshold: float = 0.10,\n",
    "        mi_threshold: float = 0.1,\n",
    "        cka_threshold: float = 0.3,\n",
    "        firing_rate_range: Tuple[float, float] = (0.2, 0.6),\n",
    "    ):\n",
    "        self.device = device\n",
    "        self.layer_map = layer_map\n",
    "        self.dead_threshold = dead_threshold\n",
    "        self.saturated_threshold = saturated_threshold\n",
    "        self.mi_threshold = mi_threshold\n",
    "        self.cka_threshold = cka_threshold\n",
    "        self.firing_rate_range = firing_rate_range\n",
    "\n",
    "        self.mi_estimator = MutualInformationEstimator()\n",
    "        self.cka_analyzer = RepresentationAnalyzer()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def validate(\n",
    "        self,\n",
    "        student: torch.nn.Module,\n",
    "        teacher: torch.nn.Module,\n",
    "        dataloader,\n",
    "        max_batches: int = 20,\n",
    "    ) -> SpikingBrainValidation:\n",
    "        \"\"\"Run complete SpikingBrain validation.\"\"\"\n",
    "        student.eval()\n",
    "        teacher.eval()\n",
    "\n",
    "        # Collect spikes and teacher hiddens\n",
    "        all_spikes = {}  # layer_idx -> list of tensors\n",
    "        all_teacher_hiddens = {}  # teacher_layer -> list of tensors\n",
    "\n",
    "        print(\"Collecting spikes and teacher representations...\")\n",
    "        for batch_idx, batch in enumerate(dataloader):\n",
    "            if batch_idx >= max_batches:\n",
    "                break\n",
    "\n",
    "            input_ids = batch['input_ids'].to(self.device)\n",
    "\n",
    "            # Student forward with spikes\n",
    "            _, _, aux = student(input_ids, return_spike_info=True)\n",
    "            spike_info = aux.get('spike_info', {})\n",
    "\n",
    "            for layer_idx, layer_spikes in spike_info.items():\n",
    "                if layer_idx not in all_spikes:\n",
    "                    all_spikes[layer_idx] = []\n",
    "                for s in layer_spikes:\n",
    "                    all_spikes[layer_idx].append(s['k_spikes'].cpu())\n",
    "\n",
    "            # Teacher forward (get hidden states from mapped layers)\n",
    "            with torch.no_grad():\n",
    "                teacher_out = teacher(input_ids, output_hidden_states=True)\n",
    "                for student_layer, teacher_layer in self.layer_map.items():\n",
    "                    if teacher_layer not in all_teacher_hiddens:\n",
    "                        all_teacher_hiddens[teacher_layer] = []\n",
    "                    h = teacher_out.hidden_states[teacher_layer + 1].cpu()\n",
    "                    all_teacher_hiddens[teacher_layer].append(h)\n",
    "\n",
    "        # 1. Compute health metrics\n",
    "        print(\"Computing health metrics...\")\n",
    "        health = self._compute_health(all_spikes)\n",
    "\n",
    "        # 2. Compute MI\n",
    "        print(\"Estimating mutual information...\")\n",
    "        mi_results = self._compute_mi(all_spikes, all_teacher_hiddens)\n",
    "\n",
    "        # 3. Compute CKA\n",
    "        print(\"Computing CKA similarity...\")\n",
    "        cka_results = self._compute_cka(all_spikes, all_teacher_hiddens)\n",
    "\n",
    "        # 4. Overall pass check\n",
    "        overall_pass = (\n",
    "            health.health_pass and\n",
    "            mi_results.get('mutual_information', 0) >= self.mi_threshold and\n",
    "            cka_results.get('cka_mean', 0) >= self.cka_threshold\n",
    "        )\n",
    "\n",
    "        # 5. Generate summary\n",
    "        summary = self._generate_summary(health, mi_results, cka_results, overall_pass)\n",
    "\n",
    "        return SpikingBrainValidation(\n",
    "            health=health,\n",
    "            mutual_information=mi_results,\n",
    "            cka=cka_results,\n",
    "            overall_pass=overall_pass,\n",
    "            summary=summary,\n",
    "        )\n",
    "\n",
    "    def _compute_health(self, all_spikes: Dict[int, List[torch.Tensor]]) -> SpikeHealthMetrics:\n",
    "        \"\"\"Compute spike health metrics.\"\"\"\n",
    "        dead_indices = {}\n",
    "        saturated_indices = {}\n",
    "        per_channel_rates = {}\n",
    "        all_rates = []\n",
    "\n",
    "        total_dead = 0\n",
    "        total_saturated = 0\n",
    "        total_channels = 0\n",
    "\n",
    "        for layer_idx, spikes_list in all_spikes.items():\n",
    "            # Stack all spikes for this layer\n",
    "            stacked = torch.cat([s.view(-1, s.shape[-1]) for s in spikes_list], dim=0)\n",
    "            d_model = stacked.shape[-1]\n",
    "\n",
    "            # Per-channel firing rates\n",
    "            rates = (stacked != 0).float().mean(dim=0).numpy()\n",
    "            per_channel_rates[f'layer_{layer_idx}'] = rates\n",
    "            all_rates.append(rates)\n",
    "\n",
    "            # Dead neurons (firing rate < 0.001)\n",
    "            dead_mask = rates < 0.001\n",
    "            dead_indices[f'layer_{layer_idx}'] = np.where(dead_mask)[0]\n",
    "            total_dead += dead_mask.sum()\n",
    "\n",
    "            # Saturated neurons (always fire)\n",
    "            always_active = (stacked != 0).all(dim=0).numpy()\n",
    "            saturated_indices[f'layer_{layer_idx}'] = np.where(always_active)[0]\n",
    "            total_saturated += always_active.sum()\n",
    "\n",
    "            total_channels += d_model\n",
    "\n",
    "        all_rates_flat = np.concatenate(all_rates)\n",
    "        dead_pct = total_dead / total_channels if total_channels > 0 else 0\n",
    "        saturated_pct = total_saturated / total_channels if total_channels > 0 else 0\n",
    "\n",
    "        # Check health\n",
    "        alerts = []\n",
    "        if dead_pct > self.dead_threshold:\n",
    "            alerts.append(f\"Dead neurons: {dead_pct*100:.1f}% > {self.dead_threshold*100:.0f}%\")\n",
    "        if saturated_pct > self.saturated_threshold:\n",
    "            alerts.append(f\"Saturated neurons: {saturated_pct*100:.1f}% > {self.saturated_threshold*100:.0f}%\")\n",
    "\n",
    "        fr_mean = float(np.mean(all_rates_flat))\n",
    "        if not (self.firing_rate_range[0] <= fr_mean <= self.firing_rate_range[1]):\n",
    "            alerts.append(f\"Firing rate {fr_mean:.3f} outside range {self.firing_rate_range}\")\n",
    "\n",
    "        health_pass = len(alerts) == 0\n",
    "\n",
    "        return SpikeHealthMetrics(\n",
    "            dead_neuron_pct=float(dead_pct),\n",
    "            dead_neuron_indices=dead_indices,\n",
    "            saturated_neuron_pct=float(saturated_pct),\n",
    "            saturated_neuron_indices=saturated_indices,\n",
    "            firing_rate_mean=fr_mean,\n",
    "            firing_rate_std=float(np.std(all_rates_flat)),\n",
    "            per_channel_rates=per_channel_rates,\n",
    "            health_pass=health_pass,\n",
    "            alerts=alerts,\n",
    "        )\n",
    "\n",
    "    def _compute_mi(\n",
    "        self,\n",
    "        all_spikes: Dict[int, List[torch.Tensor]],\n",
    "        all_teacher_hiddens: Dict[int, List[torch.Tensor]],\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"Compute mutual information.\"\"\"\n",
    "        mi_per_layer = {}\n",
    "\n",
    "        for student_layer, teacher_layer in self.layer_map.items():\n",
    "            if student_layer not in all_spikes or teacher_layer not in all_teacher_hiddens:\n",
    "                continue\n",
    "\n",
    "            spikes = torch.cat(all_spikes[student_layer], dim=0)\n",
    "            hiddens = torch.cat(all_teacher_hiddens[teacher_layer], dim=0)\n",
    "\n",
    "            mi = self.mi_estimator.estimate(spikes, hiddens)\n",
    "            mi_per_layer[f'layer_{student_layer}_to_{teacher_layer}'] = mi\n",
    "\n",
    "        mi_mean = np.mean(list(mi_per_layer.values())) if mi_per_layer else 0.0\n",
    "\n",
    "        return {\n",
    "            **mi_per_layer,\n",
    "            'mutual_information': float(mi_mean),\n",
    "        }\n",
    "\n",
    "    def _compute_cka(\n",
    "        self,\n",
    "        all_spikes: Dict[int, List[torch.Tensor]],\n",
    "        all_teacher_hiddens: Dict[int, List[torch.Tensor]],\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"Compute CKA similarity.\"\"\"\n",
    "        cka_per_layer = {}\n",
    "\n",
    "        for student_layer, teacher_layer in self.layer_map.items():\n",
    "            if student_layer not in all_spikes or teacher_layer not in all_teacher_hiddens:\n",
    "                continue\n",
    "\n",
    "            spikes = torch.cat(all_spikes[student_layer], dim=0)\n",
    "            hiddens = torch.cat(all_teacher_hiddens[teacher_layer], dim=0)\n",
    "\n",
    "            cka = self.cka_analyzer.compute_cka(spikes, hiddens)\n",
    "            cka_per_layer[f'layer_{student_layer}_to_{teacher_layer}'] = cka\n",
    "\n",
    "        cka_mean = np.mean(list(cka_per_layer.values())) if cka_per_layer else 0.0\n",
    "\n",
    "        return {\n",
    "            **cka_per_layer,\n",
    "            'cka_mean': float(cka_mean),\n",
    "        }\n",
    "\n",
    "    def _generate_summary(\n",
    "        self,\n",
    "        health: SpikeHealthMetrics,\n",
    "        mi_results: Dict[str, float],\n",
    "        cka_results: Dict[str, float],\n",
    "        overall_pass: bool,\n",
    "    ) -> str:\n",
    "        \"\"\"Generate validation summary.\"\"\"\n",
    "        lines = [\n",
    "            \"=\" * 60,\n",
    "            \"SPIKINGBRAIN VALIDATION SUMMARY\",\n",
    "            \"=\" * 60,\n",
    "            \"\",\n",
    "            \"[HEALTH]\",\n",
    "            f\"  Dead neurons: {health.dead_neuron_pct*100:.1f}% {'PASS' if health.dead_neuron_pct < self.dead_threshold else 'FAIL'}\",\n",
    "            f\"  Saturated neurons: {health.saturated_neuron_pct*100:.1f}% {'PASS' if health.saturated_neuron_pct < self.saturated_threshold else 'FAIL'}\",\n",
    "            f\"  Firing rate: {health.firing_rate_mean:.3f} +/- {health.firing_rate_std:.3f}\",\n",
    "            \"\",\n",
    "            \"[INFORMATION]\",\n",
    "            f\"  Mutual Information: {mi_results.get('mutual_information', 0):.4f} {'PASS' if mi_results.get('mutual_information', 0) >= self.mi_threshold else 'FAIL'}\",\n",
    "            \"\",\n",
    "            \"[REPRESENTATION]\",\n",
    "            f\"  CKA (mean): {cka_results.get('cka_mean', 0):.4f} {'PASS' if cka_results.get('cka_mean', 0) >= self.cka_threshold else 'FAIL'}\",\n",
    "            \"\",\n",
    "            \"=\" * 60,\n",
    "            f\"OVERALL: {'PASS - Ready for v16 (sparse ops)' if overall_pass else 'NEEDS ATTENTION'}\",\n",
    "            \"=\" * 60,\n",
    "        ]\n",
    "        return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# RUN VALIDATION\n",
    "# =============================================================================\n",
    "\n",
    "print('='*60)\n",
    "print('V15: SPIKINGBRAIN INFORMATION ENCODING VALIDATION')\n",
    "print('='*60)\n",
    "\n",
    "# Initialize validator with v14 layer mapping\n",
    "validator = SpikingBrainValidator(\n",
    "    device=device,\n",
    "    layer_map={0: 2, 2: 7, 4: 11},  # Student -> Teacher layer mapping\n",
    "    dead_threshold=0.05,      # Alert if >5% dead neurons\n",
    "    saturated_threshold=0.10,  # Alert if >10% saturated neurons\n",
    "    mi_threshold=0.1,         # Minimum acceptable MI\n",
    "    cka_threshold=0.3,        # Minimum acceptable CKA\n",
    "    firing_rate_range=(0.2, 0.6),  # Healthy firing rate range\n",
    ")\n",
    "\n",
    "# Run validation\n",
    "v15_results = validator.validate(\n",
    "    student=student,\n",
    "    teacher=teacher,\n",
    "    dataloader=val_dataloader,\n",
    "    max_batches=20,\n",
    ")\n",
    "\n",
    "# Print summary\n",
    "print(v15_results.summary)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# cell 28: V15 SpikingBrain Visualizations\n",
    "# =============================================================================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_firing_rate_histogram(\n",
    "    firing_rates: np.ndarray,\n",
    "    target_rate: float = 0.38,\n",
    "    title: str = 'Firing Rate Distribution',\n",
    "    show: bool = True,\n",
    "):\n",
    "    \"\"\"Plot histogram of per-channel firing rates.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    ax.hist(firing_rates, bins=50, alpha=0.7, color='steelblue', edgecolor='black')\n",
    "    ax.axvline(target_rate, color='red', linestyle='--', linewidth=2, label=f'Target: {target_rate}')\n",
    "    ax.axvline(firing_rates.mean(), color='orange', linestyle='-', linewidth=2, label=f'Mean: {firing_rates.mean():.3f}')\n",
    "\n",
    "    # Healthy range shading\n",
    "    ax.axvspan(0.2, 0.6, alpha=0.1, color='green', label='Healthy range [0.2, 0.6]')\n",
    "\n",
    "    ax.set_xlabel('Firing Rate')\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{OUTPUT_DIR}/figures/v15_firing_rate_dist.png', dpi=150, bbox_inches='tight')\n",
    "    if show:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_cka_by_layer(\n",
    "    cka_values: Dict[str, float],\n",
    "    threshold: float = 0.3,\n",
    "    title: str = 'CKA Similarity by Layer',\n",
    "    show: bool = True,\n",
    "):\n",
    "    \"\"\"Plot CKA similarity as a bar chart.\"\"\"\n",
    "    # Filter to per-layer values only\n",
    "    layer_cka = {k: v for k, v in cka_values.items() if 'layer_' in k and 'mean' not in k}\n",
    "\n",
    "    if not layer_cka:\n",
    "        print(\"No per-layer CKA values to plot\")\n",
    "        return\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    names = list(layer_cka.keys())\n",
    "    values = [layer_cka[n] for n in names]\n",
    "    colors = ['green' if v >= threshold else 'red' for v in values]\n",
    "\n",
    "    bars = ax.bar(range(len(names)), values, color=colors, alpha=0.7, edgecolor='black')\n",
    "    ax.axhline(threshold, color='orange', linestyle='--', linewidth=2, label=f'Threshold: {threshold}')\n",
    "    ax.axhline(cka_values.get('cka_mean', 0), color='blue', linestyle='-', linewidth=2, label=f'Mean: {cka_values.get(\"cka_mean\", 0):.3f}')\n",
    "\n",
    "    ax.set_xticks(range(len(names)))\n",
    "    ax.set_xticklabels([n.replace('layer_', 'L').replace('_to_', '->') for n in names], rotation=45, ha='right')\n",
    "    ax.set_ylabel('CKA Similarity')\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{OUTPUT_DIR}/figures/v15_cka_by_layer.png', dpi=150, bbox_inches='tight')\n",
    "    if show:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# Collect all firing rates from health metrics\n",
    "all_rates = []\n",
    "for key, rates in v15_results.health.per_channel_rates.items():\n",
    "    if len(rates) > 0:\n",
    "        all_rates.append(rates)\n",
    "\n",
    "if all_rates:\n",
    "    combined_rates = np.concatenate(all_rates)\n",
    "\n",
    "    # Firing rate histogram\n",
    "    plot_firing_rate_histogram(\n",
    "        firing_rates=combined_rates,\n",
    "        target_rate=0.38,\n",
    "        title=f'V15 Firing Rate Distribution (mean={combined_rates.mean():.3f})',\n",
    "        show=True,\n",
    "    )\n",
    "\n",
    "# CKA by layer\n",
    "if v15_results.cka:\n",
    "    plot_cka_by_layer(\n",
    "        cka_values=v15_results.cka,\n",
    "        threshold=0.3,\n",
    "        title='V15 CKA Similarity: Spikes vs Teacher',\n",
    "        show=True,\n",
    "    )\n",
    "\n",
    "print(f'\\nVisualizations saved to {OUTPUT_DIR}/figures/')\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# cell 29: V15 Success Criteria Check\n",
    "# =============================================================================\n",
    "\n",
    "print('='*60)\n",
    "print('V15 SUCCESS CRITERIA')\n",
    "print('='*60)\n",
    "\n",
    "tests = []\n",
    "\n",
    "# Test 1: Dead neurons < 5%\n",
    "dead_pct = v15_results.health.dead_neuron_pct\n",
    "test1 = dead_pct < 0.05\n",
    "tests.append(('Dead neurons < 5%', test1, f'{dead_pct*100:.1f}%'))\n",
    "\n",
    "# Test 2: Saturated neurons < 10%\n",
    "sat_pct = v15_results.health.saturated_neuron_pct\n",
    "test2 = sat_pct < 0.10\n",
    "tests.append(('Saturated neurons < 10%', test2, f'{sat_pct*100:.1f}%'))\n",
    "\n",
    "# Test 3: MI > 0.1\n",
    "mi_val = v15_results.mutual_information.get('mutual_information', 0)\n",
    "test3 = mi_val > 0.1\n",
    "tests.append(('MI > 0.1', test3, f'{mi_val:.4f}'))\n",
    "\n",
    "# Test 4: CKA mean > 0.3\n",
    "cka_mean = v15_results.cka.get('cka_mean', 0)\n",
    "test4 = cka_mean > 0.3\n",
    "tests.append(('CKA mean > 0.3', test4, f'{cka_mean:.4f}'))\n",
    "\n",
    "# Test 5: Firing rate in healthy range [0.2, 0.6]\n",
    "fr_mean = v15_results.health.firing_rate_mean\n",
    "test5 = 0.2 <= fr_mean <= 0.6\n",
    "tests.append(('Firing rate [0.2, 0.6]', test5, f'{fr_mean:.3f}'))\n",
    "\n",
    "# Print results\n",
    "for name, passed, value in tests:\n",
    "    status = 'PASS' if passed else 'FAIL'\n",
    "    print(f'  [{status}] {name}: {value}')\n",
    "\n",
    "all_pass = all(t[1] for t in tests)\n",
    "print(f'\\nOverall: {\"ALL PASS - Ready for v16 (sparse ops)\" if all_pass else \"NEEDS ATTENTION\"}')\n",
    "print('='*60)\n",
    "\n",
    "# Store in results\n",
    "results['v15_spiking_brain'] = {\n",
    "    'validation': v15_results.to_dict(),\n",
    "    'tests': {name: {'passed': passed, 'value': value} for name, passed, value in tests},\n",
    "    'all_pass': all_pass,\n",
    "}\n",
    "\n",
    "print('\\nV15 results added to results dict')\n",
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}